# Repository structure

A Restack backend application should be structured as follows:

- src/
    - client.ts
    - functions/
        - index.ts
        - function.ts
    - workflows/
        - index.ts
        - workflow.ts
    - services.ts
    - scheduleWorkflow.ts
    - package.json
    - .env.example
    - README.md
    - Dockerfile
    - restackUp.ts

All these files are mandatory.

### File: ./encryption/scheduleWorkflow.ts ###
import { client } from "./src/client";

export type InputSchedule = {
  name: string;
};

async function scheduleWorkflow(input: InputSchedule) {
  try {
    const workflowId = `${Date.now()}-helloWorkflow`;
    const runId = await client.scheduleWorkflow({
      workflowName: "helloWorkflow",
      workflowId,
      input,
    });

    const result = await client.getWorkflowResult({ workflowId, runId });

    console.log("Workflow result:", result);

    process.exit(0); // Exit the process successfully
  } catch (error) {
    console.error("Error scheduling workflow:", error);
    process.exit(1); // Exit the process with an error code
  }
}

scheduleWorkflow({
  name: "test",
});


### File: ./encryption/src/data-converter.ts ###
import { DataConverter } from '@temporalio/common';
import { EncryptionCodec } from './encryption-codec';

let dataConverterPromise: Promise<DataConverter>;

export async function getDataConverter(): Promise<DataConverter> {
  if (!dataConverterPromise) {
    dataConverterPromise = createDataConverter();
  }
  return await dataConverterPromise;
}

async function createDataConverter(): Promise<DataConverter> {
  return {
    payloadCodecs: [await EncryptionCodec.create('test-key-id')],
  };
}


### File: ./encryption/src/codec-server.ts ###
import express from 'express';
import cors from 'cors';
import * as proto from '@temporalio/proto';
import { EncryptionCodec } from './encryption-codec';
import yargs from 'yargs/yargs';

type Payload = proto.temporal.api.common.v1.IPayload;

interface JSONPayload {
  metadata?: Record<string, string> | null;
  data?: string | null;
}

interface Body {
  payloads: JSONPayload[];
}

/**
 * Helper function to convert a valid proto JSON to a payload object.
 *
 * This method will be part of the SDK when it supports proto JSON serialization.
 */
function fromJSON({ metadata, data }: JSONPayload): Payload {
  return {
    metadata:
      metadata &&
      Object.fromEntries(Object.entries(metadata).map(([k, v]): [string, Uint8Array] => [k, Buffer.from(v, 'base64')])),
    data: data ? Buffer.from(data, 'base64') : undefined,
  };
}

/**
 * Helper function to convert a payload object to a valid proto JSON.
 *
 * This method will be part of the SDK when it supports proto JSON serialization.
 */
function toJSON({ metadata, data }: proto.temporal.api.common.v1.IPayload): JSONPayload {
  return {
    metadata:
      metadata &&
      Object.fromEntries(
        Object.entries(metadata).map(([k, v]): [string, string] => [k, Buffer.from(v).toString('base64')])
      ),
    data: data ? Buffer.from(data).toString('base64') : undefined,
  };
}

async function main({ port = 8888 }: any) {
  const codec = await EncryptionCodec.create('test-key-id');

  const app = express();
  app.use(cors({ allowedHeaders: ['x-namespace', 'content-type'] }));
  app.use(express.json());

  app.post('/decode', async (req, res) => {
    try {
      const { payloads: raw } = req.body as Body;
      const encoded = raw.map(fromJSON);
      const decoded = await codec.decode(encoded);
      const payloads = decoded.map(toJSON);
      res.json({ payloads }).end();
    } catch (err) {
      console.error('Error in /decode', err);
      res.status(500).end('Internal server error');
    }
  });

  app.post('/encode', async (req, res) => {
    try {
      const { payloads: raw } = req.body as Body;
      const decoded = raw.map(fromJSON);
      const encoded = await codec.encode(decoded);
      const payloads = encoded.map(toJSON);
      res.json({ payloads }).end();
    } catch (err) {
      console.error('Error in /encode', err);
      res.status(500).end('Internal server error');
    }
  });

  await new Promise<void>((resolve, reject) => {
    app.listen(port, resolve);
    app.on('error', reject);
  });

  console.log(`Codec server listening on port ${port}`);
}

const argv = yargs(process.argv.slice(2)).argv;

main(argv).catch((err) => {
  console.error(err);
  process.exit(1);
});


### File: ./encryption/src/workflows/index.ts ###
export * from "./hello";


### File: ./encryption/src/workflows/hello.ts ###
import { log, step } from "@restackio/ai/workflow";
import * as functions from "../functions";
import { z } from "zod";
import zodToJsonSchema from "zod-to-json-schema";

interface Input {
  name: string;
}

export async function helloWorkflow({ name }: Input) {
  const userContent = `Greet this person: ${name}. In 4 words or less.`;

  const MessageSchema = z.object({
    message: z.string().describe("The greeting message."),
  });

  const jsonSchema = {
    name: "greet",
    schema: zodToJsonSchema(MessageSchema),
  };

  // Step 1 create greeting message with openai

  const openaiOutput = await step<typeof functions>({
    taskQueue: "openai",
  }).openaiChatCompletionsBase({
    userContent,
    jsonSchema,
  });

  const greetMessage = openaiOutput.result.choices[0].message.content ?? "";
  const greetCost = openaiOutput.cost;

  log.info("greeted", { greetMessage });

  // Step 2 create goodbye message with simple function

  const { message: goodbyeMessage } = await step<typeof functions>({}).goodbye({
    name,
  });

  log.info("goodbye", { goodbyeMessage });

  return {
    messages: [greetMessage, goodbyeMessage],
    cost: greetCost,
  };
}


### File: ./encryption/src/crypto.ts ###
import { webcrypto as crypto } from 'node:crypto';

const CIPHER = 'AES-GCM';
const IV_LENGTH_BYTES = 12;
const TAG_LENGTH_BYTES = 16;

export async function encrypt(data: Uint8Array, key: crypto.CryptoKey): Promise<Uint8Array> {
  const iv = crypto.getRandomValues(new Uint8Array(IV_LENGTH_BYTES));
  const encrypted = await crypto.subtle.encrypt(
    {
      name: CIPHER,
      iv,
      tagLength: TAG_LENGTH_BYTES * 8,
    },
    key,
    data
  );

  return Buffer.concat([iv, new Uint8Array(encrypted)]);
}

export async function decrypt(encryptedData: Uint8Array, key: crypto.CryptoKey): Promise<Uint8Array> {
  const iv = encryptedData.subarray(0, IV_LENGTH_BYTES);
  const ciphertext = encryptedData.subarray(IV_LENGTH_BYTES);
  const decrypted = await crypto.subtle.decrypt(
    {
      name: CIPHER,
      iv,
      tagLength: TAG_LENGTH_BYTES * 8,
    },
    key,
    ciphertext
  );

  return new Uint8Array(decrypted);
}


### File: ./encryption/src/client.ts ###
import Restack from "@restackio/ai";

import "dotenv/config";

export const connectionOptions = {
  engineId: process.env.RESTACK_ENGINE_ID!,
  address: process.env.RESTACK_ENGINE_ADDRESS!,
  apiKey: process.env.RESTACK_ENGINE_API_KEY!,
};

export const client = new Restack(
  process.env.RESTACK_ENGINE_API_KEY ? connectionOptions : undefined
);


### File: ./encryption/src/encryption-codec.ts ###
// @@@SNIPSTART typescript-encryption-codec
import { webcrypto as crypto } from 'node:crypto';
import { METADATA_ENCODING_KEY, Payload, PayloadCodec, ValueError } from '@temporalio/common';
import { temporal } from '@temporalio/proto';
import { decode, encode } from '@temporalio/common/lib/encoding';
import { decrypt, encrypt } from './crypto';

const ENCODING = 'binary/encrypted';
const METADATA_ENCRYPTION_KEY_ID = 'encryption-key-id';

export class EncryptionCodec implements PayloadCodec {
  constructor(protected readonly keys: Map<string, crypto.CryptoKey>, protected readonly defaultKeyId: string) {}

  static async create(keyId: string): Promise<EncryptionCodec> {
    const keys = new Map<string, crypto.CryptoKey>();
    keys.set(keyId, await fetchKey(keyId));
    return new this(keys, keyId);
  }

  async encode(payloads: Payload[]): Promise<Payload[]> {
    return Promise.all(
      payloads.map(async (payload) => ({
        metadata: {
          [METADATA_ENCODING_KEY]: encode(ENCODING),
          [METADATA_ENCRYPTION_KEY_ID]: encode(this.defaultKeyId),
        },
        // Encrypt entire payload, preserving metadata
        data: await encrypt(
          temporal.api.common.v1.Payload.encode(payload).finish(),
          this.keys.get(this.defaultKeyId)! // eslint-disable-line @typescript-eslint/no-non-null-assertion
        ),
      }))
    );
  }

  async decode(payloads: Payload[]): Promise<Payload[]> {
    return Promise.all(
      payloads.map(async (payload) => {
        if (!payload.metadata || decode(payload.metadata[METADATA_ENCODING_KEY]) !== ENCODING) {
          return payload;
        }
        if (!payload.data) {
          throw new ValueError('Payload data is missing');
        }

        const keyIdBytes = payload.metadata[METADATA_ENCRYPTION_KEY_ID];
        if (!keyIdBytes) {
          throw new ValueError('Unable to decrypt Payload without encryption key id');
        }

        const keyId = decode(keyIdBytes);
        let key = this.keys.get(keyId);
        if (!key) {
          key = await fetchKey(keyId);
          this.keys.set(keyId, key);
        }
        const decryptedPayloadBytes = await decrypt(payload.data, key);
        console.log('Decrypting payload.data:', payload.data);
        return temporal.api.common.v1.Payload.decode(decryptedPayloadBytes);
      })
    );
  }
}

async function fetchKey(_keyId: string): Promise<crypto.CryptoKey> {
  // In production, fetch key from a key management system (KMS). You may want to memoize requests if you'll be decoding
  // Payloads that were encrypted using keys other than defaultKeyId.
  const key = Buffer.from('test-key-test-key-test-key-test!');
  const cryptoKey = await crypto.subtle.importKey(
    'raw',
    key,
    {
      name: 'AES-GCM',
    },
    true,
    ['encrypt', 'decrypt']
  );

  return cryptoKey;
}
// @@@SNIPEND


### File: ./encryption/src/functions/goodbye.ts ###
interface Input {
  name: string;
}

interface Output {
  message: string;
}

export async function goodbye(input: Input): Promise<Output> {
  return { message: `Goodbye, ${input.name}!` };
}


### File: ./encryption/src/functions/index.ts ###
export * from "./goodbye";
export * from "./openai";

### File: ./encryption/src/functions/openai/types/events.ts ###
import OpenAI from "openai/index";

export type StreamEvent = {
  chunkId?: string;
  response: string;
  assistantName?: string;
  isLast: boolean;
};

export type ToolCallEvent =
  OpenAI.Chat.Completions.ChatCompletionChunk.Choice.Delta.ToolCall & {
    function: {
      name: string;
      input: JSON;
    };
    assistantName?: string;
  };


### File: ./encryption/src/functions/openai/types/index.ts ###
export * from "./events";


### File: ./encryption/src/functions/openai/chat/completionsBase.ts ###
import { FunctionFailure, log } from "@restackio/ai/function";
import OpenAI from "openai/index";
import { ChatCompletionCreateParamsNonStreaming } from "openai/resources/chat/completions";
import { openaiClient } from "../utils/client";
import { openaiCost, Price } from "../utils/cost";
import { ChatCompletion, ChatModel } from "openai/resources/index";

export type UsageOutput = { tokens: number; cost: number };

export type OpenAIChatInput = {
  userContent: string;
  systemContent?: string;
  model?: ChatModel;
  jsonSchema?: {
    name: string;
    schema: Record<string, unknown>;
  };
  price?: Price;
  apiKey?: string;
  params?: ChatCompletionCreateParamsNonStreaming;
  tools?: OpenAI.Chat.Completions.ChatCompletionTool[];
  toolChoice?: OpenAI.Chat.Completions.ChatCompletionToolChoiceOption;
};

export const openaiChatCompletionsBase = async ({
  userContent,
  systemContent = "",
  model = "gpt-4o-mini",
  jsonSchema,
  price,
  apiKey,
  params,
  tools,
  toolChoice,
}: OpenAIChatInput): Promise<{ result: ChatCompletion; cost?: number }> => {
  try {
    const openai = openaiClient({ apiKey });

    const isO1Model = model.startsWith("o1-");

    const o1ModelParams = {
      temperature: 1,
      top_p: 1,
      frequency_penalty: 0,
      presence_penalty: 0,
    };

    const chatParams: ChatCompletionCreateParamsNonStreaming = {
      messages: [
        ...(systemContent ? [{ role: "system" as const, content: systemContent }] : []),
        { role: "user" as const, content: userContent },
        ...(params?.messages ?? []),
      ],
      ...(jsonSchema && {
        response_format: {
          type: "json_schema",
          json_schema: {
            name: jsonSchema.name,
            strict: true,
            schema: jsonSchema.schema,
          },
        },
      }),
      model,
      ...(tools && { tools }),
      ...(toolChoice && { tool_choice: toolChoice }),
      ...params,
      ...(isO1Model && o1ModelParams),
    };

    log.debug("OpenAI chat completion params", {
      chatParams,
    });

    const completion = await openai.chat.completions.create(chatParams);

    return {
      result: completion,
      cost:
        price &&
        openaiCost({
          price,
          tokensCount: {
            input: completion.usage?.prompt_tokens ?? 0,
            output: completion.usage?.completion_tokens ?? 0,
          },
        }),
    };
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error OpenAI chat: ${error}`);
  }
};

### File: ./encryption/src/functions/openai/chat/completionsStream.ts ###
import OpenAI from "openai/index";
import { ChatCompletionChunk } from "openai/resources/chat/completions";

import Restack from "@restackio/ai";
import { currentWorkflow, log } from "@restackio/ai/function";

import { StreamEvent, ToolCallEvent } from "../types/events";

import { aggregateStreamChunks } from "../utils/aggregateStream";
import { mergeToolCalls } from "../utils/mergeToolCalls";
import { openaiClient } from "../utils/client";
import { openaiCost, Price } from "../utils/cost";
import { SendWorkflowEvent } from "@restackio/ai/event";
import { ChatModel } from "openai/resources/index";

export async function openaiChatCompletionsStream({
  model = "gpt-4o-mini",
  userName,
  newMessage,
  assistantName,
  messages = [],
  tools,
  toolEvent,
  streamAtCharacter,
  streamEvent,
  apiKey,
  price,
}: {
  model?: ChatModel;
  userName?: string;
  newMessage?: string;
  assistantName?: string;
  messages?: OpenAI.Chat.Completions.ChatCompletionMessageParam[];
  tools?: OpenAI.Chat.Completions.ChatCompletionTool[];
  toolEvent?: {
    workflowEventName: string;
    workflow?: SendWorkflowEvent["workflow"];
  };
  streamAtCharacter?: string;
  streamEvent?: {
    workflowEventName: string;
    workflow?: SendWorkflowEvent["workflow"];
  };
  apiKey?: string;
  price?: Price;
}) {
  const restack = new Restack();
  const workflow = currentWorkflow().workflowExecution;

  log.debug("workflow", { workflow });

  if (newMessage) {
    messages.push({
      role: "user",
      name: userName,
      content: newMessage,
    });
  }

  const openai = openaiClient({ apiKey });
  const chatStream = await openai.chat.completions.create({
    model: model,
    messages,
    tools,
    stream: true,
    stream_options: {
      include_usage: true,
    },
  });

  const [stream, streamEnd] = chatStream.tee();
  const readableStream = streamEnd.toReadableStream() as unknown as ReadableStream<any>;
  const aggregatedStream = await aggregateStreamChunks(readableStream);

  let finishReason: ChatCompletionChunk.Choice["finish_reason"];
  let response: ChatCompletionChunk.Choice.Delta["content"] = "";
  let tokensCountInput = 0;
  let tokensCountOutput = 0;

  for await (const chunk of stream) {
    let content = chunk.choices[0]?.delta?.content || "";
    finishReason = chunk.choices[0]?.finish_reason;
    tokensCountInput += chunk.usage?.prompt_tokens ?? 0;
    tokensCountOutput += chunk.usage?.completion_tokens ?? 0;

    if (finishReason === "tool_calls") {
      const { toolCalls } = mergeToolCalls(aggregatedStream);
      await Promise.all(
        toolCalls.map((toolCall) => {
          if (toolEvent) {
            const functionArguments = JSON.parse(
              toolCall.function?.arguments ?? ""
            );

            const input: ToolCallEvent = {
              ...toolCall,
              function: {
                name: toolCall.function?.name ?? "",
                input: functionArguments,
              },
              assistantName,
            };

            if (toolEvent) {
              const workflowEvent = {
                event: {
                  name: toolEvent.workflowEventName,
                  input,
                },
                workflow: {
                  ...workflow,
                  ...toolEvent.workflow,
                },
              };
              log.debug("toolEvent sendWorkflowEvent", { workflowEvent });

              restack.sendWorkflowEvent(workflowEvent);
            }
          }
        })
      );
      return {
        result: {
          messages,
          toolCalls,
        },
        cost:
          price &&
          openaiCost({
            price,
            tokensCount: {
              input: tokensCountInput,
              output: tokensCountOutput,
            },
          }),
      };
    } else {
      response += content;
      if (
        content.trim().slice(-1) === streamAtCharacter ||
        finishReason === "stop"
      ) {
        if (response.length) {
          const input: StreamEvent = {
            chunkId: chunk.id,
            response,
            assistantName,
            isLast: finishReason === "stop",
          };
          if (streamEvent) {
            const workflowEvent = {
              event: {
                name: streamEvent.workflowEventName,
                input,
              },
              workflow: {
                ...workflow,
                ...streamEvent.workflow,
              },
            };
            log.debug("streamEvent sendWorkflowEvent", { workflowEvent });
            restack.sendWorkflowEvent(workflowEvent);
          }
        }
      }

      if (finishReason === "stop") {
        const newMessage: OpenAI.Chat.Completions.ChatCompletionMessageParam = {
          content: response,
          role: "assistant",
          name: assistantName,
        };

        messages.push(newMessage);

        return {
          result: {
            messages,
          },
          cost:
            price &&
            openaiCost({
              price,
              tokensCount: {
                input: tokensCountInput,
                output: tokensCountOutput,
              },
            }),
        };
      }
    }
  }
}


### File: ./encryption/src/functions/openai/chat/index.ts ###
export * from "./completionsBase";
export * from "./completionsStream";


### File: ./encryption/src/functions/openai/utils/cost.ts ###
export type TokensCount = {
  input: number;
  output: number;
};

export type Price = {
  input: number;
  output: number;
};
export const openaiCost = ({
  tokensCount,
  price,
}: {
  tokensCount: TokensCount;
  price: Price;
}): number => {
  let cost = 0;
  const { input: inputTokens, output: outputTokens } = tokensCount;
  const { input: inputPrice, output: outputPrice } = price;

  cost = inputTokens * inputPrice + outputTokens * outputPrice;

  return cost;
};


### File: ./encryption/src/functions/openai/utils/aggregateStream.ts ###
export async function aggregateStreamChunks(stream: ReadableStream) {
  const reader = stream.getReader();
  const chunks: Uint8Array[] = [];

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    if (value) chunks.push(value);
  }

  const aggregated = new Uint8Array(
    chunks.reduce((acc, chunk) => acc + chunk.length, 0)
  );
  let offset = 0;
  for (const chunk of chunks) {
    aggregated.set(chunk, offset);
    offset += chunk.length;
  }

  const textContent = new TextDecoder().decode(aggregated);
  const jsonObjects = textContent
    .split("\n")
    .filter((line) => line.trim())
    .map((line) => JSON.parse(line));
  return jsonObjects;
}


### File: ./encryption/src/functions/openai/utils/client.ts ###
import OpenAI from "openai/index";
import "dotenv/config";

let openaiInstance: OpenAI | null = null;

export const openaiClient = ({
  apiKey = process.env.OPENAI_API_KEY,
}: {
  apiKey?: string;
}): OpenAI => {
  if (!apiKey) {
    throw new Error("API key is required to create OpenAI client.");
  }

  if (!openaiInstance) {
    openaiInstance = new OpenAI({
      apiKey,
    });
  }
  return openaiInstance;
};


### File: ./encryption/src/functions/openai/utils/index.ts ###
export * from "./aggregateStream";
export * from "./client";
export * from "./cost";
export * from "./mergeToolCalls";


### File: ./encryption/src/functions/openai/utils/mergeToolCalls.ts ###
import OpenAI from "openai/index";
import { ChatCompletionChunk } from "openai/resources/chat/completions.mjs";

export function mergeToolCalls(aggregatedStream: ChatCompletionChunk[]) {
  const toolCalls: OpenAI.Chat.Completions.ChatCompletionChunk.Choice.Delta.ToolCall[] =
    [];

  aggregatedStream.forEach((chunk) => {
    chunk.choices.forEach((choice) => {
      if (choice.delta.tool_calls) {
        choice.delta.tool_calls.forEach((toolCall) => {
          const lastToolCall = toolCalls[toolCalls.length - 1];
          if (toolCall.id) {
            toolCalls.push({
              ...toolCall,
              function: { ...toolCall.function, arguments: "" },
            });
          } else if (
            lastToolCall &&
            lastToolCall.function &&
            toolCall.function?.arguments
          ) {
            lastToolCall.function.arguments += toolCall.function.arguments;
          }
        });
      }
    });
  });

  return { toolCalls };
}


### File: ./encryption/src/functions/openai/index.ts ###
export * from "./chat";
export * from "./thread";


### File: ./encryption/src/functions/openai/thread/createMessageOnThread.ts ###
import OpenAI from "openai/index";
import { FunctionFailure } from "@restackio/ai/function";

import { openaiClient } from "../utils/client";

export async function createMessageOnThread({
  apiKey,
  threadId,
  content,
  role,
}: {
  apiKey: string;
  threadId: string;
  content: string;
  role: OpenAI.Beta.Threads.MessageCreateParams["role"];
}) {
  try {
    const openai = openaiClient({ apiKey });
    await openai.beta.threads.messages.create(threadId, {
      role,
      content,
    });
  } catch (error) {
    throw FunctionFailure.nonRetryable(
      `Error creating message thread: ${error}`
    );
  }
}


### File: ./encryption/src/functions/openai/thread/runThread.ts ###
import { FunctionFailure } from "@restackio/ai/function";
import { Stream } from "openai/streaming";
import { AssistantStreamEvent } from "openai/resources/beta/index";
import { Run } from "openai/resources/beta/threads/runs/index";

import { openaiClient } from "../utils/client";

export async function runThread({
  apiKey,
  threadId,
  assistantId,
  stream = false,
}: {
  apiKey: string;
  threadId: string;
  assistantId: string;
  stream: boolean;
}): Promise<Stream<AssistantStreamEvent> | Run> {
  try {
    const openai = openaiClient({ apiKey });

    const run = await openai.beta.threads.runs.create(threadId, {
      assistant_id: assistantId,
      ...(stream && { stream }),
    });

    return run;
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error running thread: ${error}`);
  }
}

### File: ./encryption/src/functions/openai/thread/createAssistant.ts ###
import { ChatModel } from "openai/resources/index";
import { FunctionFailure } from "@restackio/ai/function";
import { Assistant, AssistantTool } from "openai/resources/beta/index";

import { openaiClient } from "../utils/client";

export async function createAssistant({
  apiKey,
  name,
  instructions,
  model = "gpt-4o-mini",
  tools = [],
}: {
  apiKey: string;
  name: string;
  instructions: string;
  tools?: AssistantTool[];
  model: ChatModel;
}): Promise<Assistant> {
  try {
    const openai = openaiClient({ apiKey });

    const assistant = await openai.beta.assistants.create({
      name,
      instructions,
      model,
      tools,
    });

    return assistant;
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error creating assistant: ${error}`);
  }
}


### File: ./encryption/src/functions/openai/thread/index.ts ###
export * from "./createAssistant";
export * from "./createMessageOnThread";
export * from "./createThread";
export * from "./runThread";


### File: ./encryption/src/functions/openai/thread/createThread.ts ###
import { FunctionFailure } from "@restackio/ai/function";
import { Thread } from "openai/resources/beta/index";

import { openaiClient } from "../utils/client";

export async function createThread({
  apiKey,
}: {
  apiKey: string;
}): Promise<Thread> {
  try {
    const openai = openaiClient({ apiKey });
    const thread = await openai.beta.threads.create();

    return thread;
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error creating thread: ${error}`);
  }
}


### File: ./encryption/src/services.ts ###
import { goodbye, openaiChatCompletionsBase, openaiChatCompletionsStream } from "./functions";
import { client } from "./client";

async function services() {
  const workflowsPath = require.resolve("./workflows");
  try {
    await Promise.all([
      // Start service with current workflows and functions
      client.startService({
        workflowsPath,
        functions: { goodbye },
      }),
      // Start the openai service
      client.startService({
        taskQueue: "openai",
        functions: { openaiChatCompletionsBase, openaiChatCompletionsStream },
      }),
    ]);

    console.log("Services running successfully.");
  } catch (e) {
    console.error("Failed to run services", e);
  }
}

services().catch((err) => {
  console.error("Error running services:", err);
});


### File: ./human-loop/scheduleWorkflow.ts ###
import { client } from "./src/client";

import { endEvent, feedbackEvent } from "./src/events";

async function scheduleWorkflow() {
  try {
    const workflowId = `${Date.now()}-HumanLoopWorkflow`;
    const runId = await client.scheduleWorkflow({
      workflowName: "humanLoopWorkflow",
      workflowId,
    });

    const feedback = await client.sendWorkflowEvent({
      workflow: {
        workflowId,
        runId,
      },
      event: {
        name: feedbackEvent.name,
        input: { feedback: "Hello, how are you?" },
      },
    });

    console.log("Feedback:", feedback);

    const end = await client.sendWorkflowEvent({
      workflow: {
        workflowId,
        runId,
      },
      event: { name: endEvent.name, input: { end: true } },
    });

    console.log("End:", end);

    process.exit(0); // Exit the process successfully
  } catch (error) {
    console.error("Error scheduling workflow:", error);
    process.exit(1); // Exit the process with an error code
  }
}

scheduleWorkflow();


### File: ./human-loop/src/workflows/humanLoop.ts ###
import { log, step, condition } from "@restackio/ai/workflow";
import { onEvent } from "@restackio/ai/event";

import { feedbackEvent, FeedbackEvent, endEvent, EndEvent } from "../events";
import * as functions from "../functions";

export async function humanLoopWorkflow() {
  let endWorkflow = false;

  onEvent(feedbackEvent, async (event: FeedbackEvent) => {
    log.info("Received feedback", { feedback: event.feedback });

    const feedback = await step<typeof functions>({}).feedback({
      feedback: event.feedback,
    });
    return feedback;
  });

  onEvent(endEvent, async (event: EndEvent) => {
    log.info("Received end", { end: event.end });
    endWorkflow = event.end;
    return event.end;
  });

  await condition(() => endWorkflow);

  const goodbye = await step<typeof functions>({}).goodbye();

  return goodbye;
}


### File: ./human-loop/src/workflows/index.ts ###
export * from "./humanLoop";


### File: ./human-loop/src/client.ts ###
import Restack from "@restackio/ai";

export const client = new Restack();


### File: ./human-loop/src/functions/goodbye.ts ###
export async function goodbye(): Promise<string> {
  return "Goodbye!";
}


### File: ./human-loop/src/functions/feedback.ts ###
interface Input {
  feedback: string;
}

export async function feedback(input: Input): Promise<string> {
  return `Feedback received: ${input.feedback}`;
}


### File: ./human-loop/src/functions/index.ts ###
export * from "./feedback";
export * from "./goodbye";


### File: ./human-loop/src/events/feedbackEvent.ts ###
import { defineEvent } from "@restackio/ai/event";

export type FeedbackEvent = {
  feedback: string;
};

export const feedbackEvent = defineEvent<string>("feedback");


### File: ./human-loop/src/events/index.ts ###
export * from "./feedbackEvent";
export * from "./endEvent";


### File: ./human-loop/src/events/endEvent.ts ###
import { defineEvent } from "@restackio/ai/event";

export type EndEvent = {
  end: boolean;
};

export const endEvent = defineEvent<boolean>("end");


### File: ./human-loop/src/services.ts ###
import { feedback, goodbye } from "./functions";
import { client } from "./client";

async function services() {
  const workflowsPath = require.resolve("./workflows");
  try {
    await Promise.all([
      // Start service with current workflows and functions
      client.startService({
        workflowsPath,
        functions: { feedback, goodbye },
      }),
    ]);

    console.log("Services running successfully.");
  } catch (e) {
    console.error("Failed to run services", e);
  }
}

services().catch((err) => {
  console.error("Error running services:", err);
});


### File: ./defense_quickstart_news_scraper_summarizer/frontend/tailwind.config.ts ###
import type { Config } from "tailwindcss";

const config: Config = {
  content: [
    "./src/pages/**/*.{js,ts,jsx,tsx,mdx}",
    "./src/components/**/*.{js,ts,jsx,tsx,mdx}",
    "./src/app/**/*.{js,ts,jsx,tsx,mdx}",
  ],
  theme: {
    extend: {
      colors: {
        background: "var(--background)",
        foreground: "var(--foreground)",
      },
    },
  },
  plugins: [],
};
export default config;


### File: ./defense_quickstart_news_scraper_summarizer/frontend/src/app/actions/schedule.ts ###
"use server";
import Restack from "@restackio/ai";
import { Example } from "../components/examplesList";

const connectionOptions = {
  engineId: process.env.RESTACK_ENGINE_ID!,
  address: process.env.RESTACK_ENGINE_ADDRESS!,
  apiKey: process.env.RESTACK_ENGINE_API_KEY!,
};

const client = new Restack(
  process.env.RESTACK_ENGINE_API_KEY ? connectionOptions : undefined
);

export async function scheduleWorkflow(
  workflowName: Example["workflowName"],
  input: Example["input"]
) {
  if (!workflowName || !input) {
    throw new Error("Workflow name and input are required");
  }

  const workflowId = `${Date.now()}-${workflowName.toString()}`;

  const runId = await client.scheduleWorkflow({
    workflowName: workflowName as string,
    workflowId,
    input,
  });

  const result = await client.getWorkflowResult({
    workflowId,
    runId,
  });

  return result;
}


### File: ./defense_quickstart_news_scraper_summarizer/backend/src/workflows/index.ts ###
export * from './rssDigest';

### File: ./defense_quickstart_news_scraper_summarizer/backend/src/workflows/rssDigest.ts ###
import { step } from "@restackio/ai/workflow";
import * as functions from "../functions";


export async function rssDigest({ url = "https://www.pravda.com.ua/rss/", count = 2 }: { url: string, count: number }) {

    // Step 1: Fetch RSS feed
    const rssResponse: functions.RssItem[] = await step<typeof functions>({
    }).rssPull({
        url,
        count,
    });

    // Step 2: Crawl website content
    let websiteContent: { result: string }[] = [];
    await Promise.all(rssResponse.map(async (item) => {
        const response: { result: string } = await step<typeof functions>({
        }).crawlWebsite({
            url: item.link,
        });

        websiteContent.push(response);
    }));

    // Step 3: Very basic character split because OpenBabylon model has a token limit of 4096
    let splittedContent: string[] = [];
    await Promise.all(websiteContent.map(async (item) => {
        const response: { result: string[] } = await step<typeof functions>({
        }).splitContent({
            content: item.result,
        });

        splittedContent.push(...response.result);
    }));

    // // Step 4: LLM translation
    let translatedContent: { result: string }[] = [];
    await Promise.all(splittedContent.map(async (item) => {
        const response: { result: string } = await step<typeof functions>({
            taskQueue: 'llm',
        }).llmChat({
            userContent: `Only return the translated content string, no other text!! Translate the following content to English!!: ${item}.`,
        });

        translatedContent.push(response);
    }));

    // // Step 5: LLM summarization per translated chunk
    let summarizedContent: { result: string }[] = [];
    await Promise.all(translatedContent.map(async (item) => {
        const response: { result: string } = await step<typeof functions>({
            taskQueue: 'llm',
        }).llmChat({
            userContent: `Summarize the following content in maximum 1 sentence: ${item.result}.`,
        });

        summarizedContent.push(response);
    }));

    // Step 6: LLM Create a digest
    return await step<typeof functions>({
        taskQueue: 'llm',
    }).llmChat({
        userContent: `Summarize the following content as a daily digest, prioritize the most important information: ${summarizedContent.map((item) => item.result).join("\n")}`,
    });
}


### File: ./defense_quickstart_news_scraper_summarizer/backend/src/client.ts ###
import Restack from '@restackio/ai';
import dotenv from 'dotenv';

dotenv.config();

export const connectionOptions = {
  engineId: process.env.RESTACK_ENGINE_ID!,
  address: process.env.RESTACK_ENGINE_ADDRESS!,
  apiKey: process.env.RESTACK_ENGINE_API_KEY!,
};

export const client = new Restack(
  process.env.RESTACK_ENGINE_API_KEY ? connectionOptions : undefined
);

### File: ./defense_quickstart_news_scraper_summarizer/backend/src/functions/llm/chat.ts ###
import { FunctionFailure, log } from "@restackio/ai/function";
import { llmClient } from "./client";

export const llmChat = async ({
    userContent
}: { userContent: string }): Promise<{ result: string }> => {
    try {
        const llm = llmClient();

        const response = await llm.chat.completions.create({
            messages: [{ role: "user", content: userContent }],
            model: "orpo-mistral-v0.3-ua-tokV2-focus-10B-low-lr-1epoch-aux-merged-1ep",
        });

        if (!response.choices[0].message.content) {
            throw FunctionFailure.nonRetryable("No response from OpenBabylon");
        }

        return { result: response.choices[0].message.content || "" };
    } catch (error) {
        throw FunctionFailure.nonRetryable(`Error LLM chat: ${error}, llmClient: ${llmClient}`);
    }
};

### File: ./defense_quickstart_news_scraper_summarizer/backend/src/functions/llm/client.ts ###
import OpenAI from "openai/index";
import dotenv from 'dotenv';

dotenv.config();

let openaiInstance: OpenAI | null = null;

export const llmClient = (): OpenAI => {
    if (!process.env.OPENBABYLON_API_URL) {
        throw new Error("OPENBABYLON_API_URL is not set in environment variables.");
    }

    const baseUrl = process.env.OPENBABYLON_API_URL.startsWith('http')
        ? process.env.OPENBABYLON_API_URL
        : `http://${process.env.OPENBABYLON_API_URL}`;

    if (!openaiInstance) {
        openaiInstance = new OpenAI({
            baseURL: baseUrl,
            apiKey: `non-existent`,
        });
    }
    return openaiInstance;
};

### File: ./defense_quickstart_news_scraper_summarizer/backend/src/functions/llm/index.ts ###
export * from './chat';

### File: ./defense_quickstart_news_scraper_summarizer/backend/src/functions/rss/index.ts ###
export * from './pull';

### File: ./defense_quickstart_news_scraper_summarizer/backend/src/functions/rss/pull.ts ###
import { FunctionFailure, log } from "@restackio/ai/function";
import axios from "axios";
import { parseString } from "xml2js";
import { promisify } from "util";
import * as iconv from "iconv-lite";

interface RssInput {
    url: string;
    count?: number;
}

export interface RssItem {
    title: string;
    link: string;
    description: string;
    category?: string;
    creator?: string;
    pub_date?: string;
    content_encoded?: string;
}

export async function rssPull(input: RssInput): Promise<RssItem[]> {
    try {
        // Fetch the RSS feed
        const response = await axios.get(input.url, { responseType: 'arraybuffer' });

        // Determine the encoding from the response headers or assume a default
        const contentType = response.headers['content-type'];
        const encoding = contentType && contentType.includes('charset=')
            ? contentType.split('charset=')[1]
            : 'utf-8'; // Default to utf-8 if not specified

        // Decode the response data using iconv-lite with the detected encoding
        const decodedData = iconv.decode(Buffer.from(response.data), encoding);

        // Parse the RSS feed
        const parseXml = promisify(parseString);
        const result: any = await parseXml(decodedData);

        const items: RssItem[] = result.rss.channel[0].item.map((item: any) => ({
            title: item.title?.[0] || '',
            link: item.link?.[0] || '',
            description: item.description?.[0] || '',
            category: item.category?.[0] || '',
            creator: item["dc:creator"]?.[0] || '',
            pub_date: item.pubDate?.[0] || '',
            content_encoded: item["content:encoded"]?.[0] || ''
        }));

        // Limit the number of items based on input.count
        const maxCount = input.count ?? items.length;
        const limitedItems = items.slice(0, maxCount);

        return limitedItems;

    } catch (error) {
        log.error("rssPull function failed", { error });
        throw FunctionFailure.nonRetryable(`Error fetching RSS feed: ${error}`);
    }
}

### File: ./defense_quickstart_news_scraper_summarizer/backend/src/functions/utils/splitText.ts ###
import { FunctionFailure } from "@restackio/ai/function";

export const splitContent = async ({
    content,
    maxTokens = 4096
}: {
    content: string,
    maxTokens?: number
}): Promise<{ result: string[] }> => {
    try {
        const chunkSize = Math.floor(maxTokens / 3); // Assuming ~3 tokens per character
        const chunks = [];

        for (let i = 0; i < content.length; i += chunkSize) {
            chunks.push(content.slice(i, i + chunkSize));
        }

        return { result: chunks };
    } catch (error) {
        throw FunctionFailure.nonRetryable(`Error split content: ${error}`);
    }
};

### File: ./defense_quickstart_news_scraper_summarizer/backend/src/functions/utils/index.ts ###
export * from "./splitText";

### File: ./defense_quickstart_news_scraper_summarizer/backend/src/functions/crawl/website.ts ###
import { FunctionFailure, log } from "@restackio/ai/function";
import axios from "axios";
import * as iconv from "iconv-lite";
import { JSDOM } from "jsdom";
import { Readability } from "@mozilla/readability";

export const crawlWebsite = async ({
    url
}: { url: string }): Promise<{ result: string }> => {
    try {
        const response = await axios.get(url, { responseType: 'arraybuffer' });
        const contentType = response.headers['content-type'];
        const encoding = contentType && contentType.includes('charset=')
            ? contentType.split('charset=')[1]
            : 'utf-8';
        const decodedData = iconv.decode(response.data, encoding);

        const dom = new JSDOM(decodedData, { url });
        const reader = new Readability(dom.window.document);
        const article = reader.parse();

        if (article) {
            return { result: article.textContent };
        } else {
            throw new Error("Failed to parse the main content");
        }
    } catch (error) {
        throw FunctionFailure.nonRetryable(`Error crawl website: ${error}`);
    }
};

### File: ./defense_quickstart_news_scraper_summarizer/backend/src/functions/crawl/index.ts ###
export * from './website';

### File: ./defense_quickstart_news_scraper_summarizer/backend/src/functions/index.ts ###
export * from './crawl';
export * from './llm';
export * from './rss';
export * from './utils';

### File: ./defense_quickstart_news_scraper_summarizer/backend/src/services.ts ###
// Simple example to start two services in the same file
import { config } from 'dotenv';
import { rssPull, llmChat, crawlWebsite, splitContent } from "./functions";
import { client } from "./client";

config();

export async function services() {
    const workflowsPath = require.resolve("./workflows");
    try {
        await Promise.all([
            // Generic service with current workflows and functions
            client.startService({
                workflowsPath,
                functions: {
                    rssPull,
                    crawlWebsite,
                    splitContent,
                },
            }),
            client.startService({
                taskQueue: 'llm',
                workflowsPath,
                functions: {
                    llmChat
                },
            }),
        ]);

        console.log("Services running successfully.");
    } catch (e) {
        console.error("Failed to run services", e);
    }
}

services().catch((err) => {
    console.error("Error running services:", err);
});


### File: ./discord/scheduleWorkflow.ts ###
import { client } from "./src/client";
import { endConnectionEvent, messageCreatedEvent } from "./src/events";
import { DiscordGatewayClient } from "./src/utils/client";

interface Input {
  channelId: string,
  botId: string
}

interface Author {
  id: string
}

interface Message {
  content: string, 
  id: string
  author: Author
}

async function scheduleWorkflow(input: Input) {
  try {
    console.log(WebSocket);
    const workflowId = `${Date.now()}-HumanLoopWorkflow`;
    const runId = await client.scheduleWorkflow({
      workflowName: "humanLoopDiscordWorkflow",
      workflowId,
      input
    });
    
    let endGateWayConnection = false;
    function handleMessageEvent(message: Message) {
      const botId = process.env.DISCORD_BOT_ID ?? "";
      const botTag = `<@${botId}>`;
      if (message.content.replace(botTag, '').trim() === 'STOP') {
        endGateWayConnection = true;
        client.sendWorkflowEvent({
          workflow: {
            workflowId,
            runId
          },
          event: {
            name: endConnectionEvent.name,
            input: { end: true }
          },
        });
      } else if (message.content.length > 0
        && message.author.id != botId) {
        client.sendWorkflowEvent({
          workflow: {
            workflowId,
            runId,
          },
          event: {
            name: messageCreatedEvent.name,
            input: { 
              content: message.content,
              id: message.id
            }
          },
        });
      }
    }
    const discordGatewayClient = new DiscordGatewayClient(handleMessageEvent);
    await new Promise(f => setTimeout(f, 1000));
    discordGatewayClient.identify(3585, process.env.DISCORD_BOT_TOKEN ?? "");
    while (!endGateWayConnection) {
      await new Promise(f => setTimeout(f, 1000));
      discordGatewayClient.sendHeartbeat();
    }
    process.exit(0); // Exit the process successfully
  } catch (error) {
    console.error("Error scheduling workflow:", error);
    process.exit(1); // Exit the process with an error code
  }
}

scheduleWorkflow({
  channelId: process.env.DISCORD_CHANNEL_ID ?? "",
  botId: process.env.DISCORD_BOT_ID ?? ""
});


### File: ./discord/src/workflows/index.ts ###
export * from "./humanLoopDiscord";


### File: ./discord/src/workflows/humanLoopDiscord.ts ###
import { step, condition } from "@restackio/ai/workflow";
import { onEvent } from "@restackio/ai/event";
import * as functions from "../functions";
import { endConnectionEvent, MessageCreatedEvent, messageCreatedEvent, EndConnectionEvent } from "../events";

interface Input {
  channelId: string,
  botId: string, 
  messageHandler: Function
}

export async function humanLoopDiscordWorkflow(input: Input) {
  let endWorkflow = false;
  const botTag = `<@${input.botId}>`;

  onEvent(endConnectionEvent, async (event: EndConnectionEvent) => {
    endWorkflow = event.end;
    return event.end;
  });

  onEvent(messageCreatedEvent, async (event: MessageCreatedEvent) => {
    return await step<typeof functions>({}).postReplyToMessage({
      messageText: event.content.replace(botTag, '').split('').reverse().join(''),
      channelId: input.channelId,
      messageId: event.id
    });
  });

  await step<typeof functions>({}).postMessageToChannel({
    messageText: `Message this channel and tag ${botTag} for backwards messages. Message "${botTag} STOP" to end the loop`,
    channelId: input.channelId,
  });

  await condition(() => endWorkflow)

  return await step<typeof functions>({}).postMessageToChannel({
    messageText: `Goodbye!`,
    channelId: input.channelId,
  });
}


### File: ./discord/src/utils/client.ts ###
const DISCORD_API_VERSION = '10';
const DISCORD_BASE_URL = `https://discord.com/api/v${DISCORD_API_VERSION}`;
const DISCORD_GATEWAY_URL = 'wss://gateway.discord.gg';

export class DiscordClient {
    private botToken: string;
    constructor(
        botToken: string = process.env.DISCORD_BOT_TOKEN ?? ""
    ){
        this.botToken = botToken;
    }

    public postMessage(message: string, channelId: string) {
        const messagesUrl = `/channels/${channelId}/messages`;
        const url = DISCORD_BASE_URL + messagesUrl;
        const body = {
            'content': message
        }
        const headers = {
            'Content-Type': 'application/json',
            'Authorization': `Bot ${this.botToken}`
        }
        const options = {
            'method': 'POST',
            'body': JSON.stringify(body),
            'headers': headers
        }
        return fetch(url, options)
            .then(response => response.json());
    }

    public postMessageToReply(message: string, 
        channelId: string, messageId: string) {
        const messagesUrl = `/channels/${channelId}/messages`;
        const url = DISCORD_BASE_URL + messagesUrl;
        const body = {
            'content': message,
            'message_reference': {
                'message_id': messageId
            }
        }
        const headers = {
            'Content-Type': 'application/json',
            'Authorization': `Bot ${this.botToken}`
        }
        const options = {
            'method': 'POST',
            'body': JSON.stringify(body),
            'headers': headers
        }
        return fetch(url, options)
            .then(response => response.json());
    }

    public getMessagesAfterId(afterMessageId: string, channelId: string) {
        const messagesUrl = `/channels/${channelId}/messages`;
        const queryParams = new URLSearchParams({
            'after': afterMessageId
        })
        const url = DISCORD_BASE_URL + messagesUrl 
            + '?' + queryParams.toString();
        const headers = {
            'Content-Type': 'application/json',
            'Authorization': `Bot ${this.botToken}`
        }
        const options = {
            'method': 'GET',
            'headers': headers
        }
        return fetch(url, options)
            .then(response => response.json());
    }
}

export class DiscordGatewayClient {
    private webSocket: WebSocket;
    private messageCreationHandler: Function;
    private seq: Number;

    constructor(
        messageCreationHandler: Function
    ){
        this.webSocket = new WebSocket(DISCORD_GATEWAY_URL);
        this.webSocket.addEventListener("message", (event: MessageEvent) => {
            const eventDataJson = JSON.parse(event.data);
            this.seq = eventDataJson.s;
            const eventType = eventDataJson.t
            if (eventType === 'MESSAGE_CREATE') {
                console.log(eventDataJson.d)
                this.messageCreationHandler(eventDataJson.d);
            }
        });
        this.messageCreationHandler = messageCreationHandler;
        this.seq = 0;
    }

    public sendHeartbeat() {
        const heartbeat = {
            "op": 1,
            "d": this.seq
        }
        this.webSocket.send(JSON.stringify(heartbeat));
    }

    public identify(intentNum: number, botToken: string) {
        const body = {
            "op": 2,
            "d": {
                "intents": intentNum,
                "token": botToken,
                "properties": {
                    "os": "macos",
                    "device": "macbook",
                    "browser": "chrome"
                }
            }
        }
        this.webSocket.send(JSON.stringify(body));
    }
}

### File: ./discord/src/client.ts ###
import Restack from "@restackio/ai";

export const client = new Restack();


### File: ./discord/src/functions/postMessage.ts ###
import { log } from "@restackio/ai/function";
import { DiscordClient } from "../utils/client";

export async function postMessageToChannel({
    messageText,
    channelId,
    botToken
}: {
    messageText: string,
    channelId: string,
    botToken?: string
}){
    try {
        const client = new DiscordClient(botToken);
        return client.postMessage(messageText, channelId);
    } catch (error) {
        log.error("Discord integration error", { error });
        throw new Error(`Discord integration error ${error}`);
    }
}

export async function postReplyToMessage({
    messageText,
    channelId,
    messageId,
    botToken
}: {
    messageText: string,
    channelId: string,
    messageId: string
    botToken?: string
}){
    try {
        const client = new DiscordClient(botToken);
        return client.postMessageToReply(messageText, channelId, messageId);
    } catch (error) {
        log.error("Discord integration error", { error });
        throw new Error(`Discord integration error ${error}`);
    }
}

### File: ./discord/src/functions/index.ts ###
export * from "./postMessage";
export * from "./getMessages";


### File: ./discord/src/functions/getMessages.ts ###
import { log } from "@restackio/ai/function";
import { DiscordClient } from "../utils/client";

export async function getMessagesAfterId({
    afterMessageId,
    channelId,
    botToken
}: {
    afterMessageId: string,
    channelId: string,
    botToken?: string
}){
    try {
        const client = new DiscordClient(botToken);
        return client.getMessagesAfterId(afterMessageId, channelId);
    } catch (error) {
        log.error("Discord integration error", { error });
        throw new Error(`Discord integration error ${error}`);
    }
}

### File: ./discord/src/events/messageCreated.ts ###
import { defineEvent } from "@restackio/ai/event";

export type MessageCreatedEvent = {
  content: string;
  id: string;
};

export const messageCreatedEvent = defineEvent<string>("messageCreated");

### File: ./discord/src/events/endConnection.ts ###
import { defineEvent } from "@restackio/ai/event";

export type EndConnectionEvent = {
  end: boolean;
};

export const endConnectionEvent = defineEvent<boolean>("endConnection");

### File: ./discord/src/events/index.ts ###
export * from "./endConnection";
export * from "./messageCreated";

### File: ./discord/src/services.ts ###
import { getMessagesAfterId, postMessageToChannel, postReplyToMessage } from "./functions";
import { client } from "./client";

async function services() {
  const workflowsPath = require.resolve("./workflows");
  try {
    await Promise.all([
      // Start service with current workflows and functions
      client.startService({
        workflowsPath,
        functions: { postMessageToChannel, getMessagesAfterId, postReplyToMessage },
      }),
    ]);

    console.log("Services running successfully.");
  } catch (e) {
    console.error("Failed to run services", e);
  }
}

services().catch((err) => {
  console.error("Error running services:", err);
});


### File: ./stripe-ai/scheduleWorkflow.ts ###
import { client } from './src/client';

import 'dotenv/config';

async function scheduleWorkflow() {
  try {
    const workflowId = `${Date.now()}-sendEmailWorkflow`;
    await client.scheduleWorkflow({
      workflowName: 'createPaymentLinkWorkflow',
      workflowId,
    });

    console.log('Workflow scheduled successfully');

    process.exit(0); // Exit the process successfully
  } catch (error) {
    console.error('Error scheduling workflow:', error);
    process.exit(1); // Exit the process with an error code
  }
}

scheduleWorkflow();


### File: ./stripe-ai/src/workflows/createPaymentLink.ts ###
import { log, step } from "@restackio/ai/workflow";

import * as functions from '../functions';

export async function createPaymentLinkWorkflow() {
  const result = await step<typeof functions>({}).createPaymentLink();

  log.info('Payment link created', { result });
}


### File: ./stripe-ai/src/workflows/index.ts ###
export * from './createPaymentLink';


### File: ./stripe-ai/src/client.ts ###
import Restack from '@restackio/ai';

import 'dotenv/config';

export const connectionOptions = {
  engineId: process.env.RESTACK_ENGINE_ID!,
  address: process.env.RESTACK_ENGINE_ADDRESS!,
  apiKey: process.env.RESTACK_ENGINE_API_KEY!,
};

export const client = new Restack(
  process.env.RESTACK_ENGINE_API_KEY ? connectionOptions : undefined
);


### File: ./stripe-ai/src/functions/createPaymentLink.ts ###
import { StripeAgentToolkit } from '@stripe/agent-toolkit/ai-sdk';
import { openai } from '@ai-sdk/openai';
import { generateText } from 'ai';
import { FunctionFailure } from '@restackio/ai/function';

import 'dotenv/config';

export async function createPaymentLink() {
  if (!process.env.STRIPE_SECRET_KEY) {
    throw FunctionFailure.nonRetryable('STRIPE_SECRET_KEY is not set');
  }

  if (!process.env.OPENAI_API_KEY) {
    throw FunctionFailure.nonRetryable('OPENAI_API_KEY is not set');
  }

  const stripeAgentToolkit = new StripeAgentToolkit({
    secretKey: process.env.STRIPE_SECRET_KEY!,
    configuration: {
      actions: {
        paymentLinks: {
          create: true,
        },
        products: {
          create: true,
        },
        prices: {
          create: true,
        },
      },
    },
  });

  const result = await generateText({
    model: openai('gpt-4o'),
    tools: {
      ...stripeAgentToolkit.getTools(),
    },
    maxSteps: 5,
    prompt: 'Create a payment link for a new product called \"AI-Generated-Product\" with a price of $100.',
  });

  return result.text;
}


### File: ./stripe-ai/src/functions/index.ts ###
export * from './createPaymentLink';


### File: ./stripe-ai/src/services.ts ###
import { client } from './client';
import { createPaymentLink } from './functions';

async function services() {
  const workflowsPath = require.resolve('./workflows');
  try {
    await Promise.all([
      // Start service with current workflows and functions
      client.startService({
        workflowsPath,
        functions: { createPaymentLink },
      }),
    ]);

    console.log('Services running successfully.');
  } catch (e) {
    console.error('Failed to run services', e);
  }
}

services().catch((err) => {
  console.error('Error running services:', err);
});


### File: ./gotohuman/scheduleWorkflow.ts ###
import { client } from "./src/client";

interface TopicInput {
  topic: string;
}

async function scheduleWorkflow(input: TopicInput) {
  try {
    const workflowId = `${Date.now()}-writePostWorkflow`;
    await client.scheduleWorkflow({
      workflowName: "writePostWorkflow",
      workflowId,
      input
    });

    process.exit(0); // Exit the process successfully
  } catch (error) {
    console.error("Error scheduling workflow:", error);
    process.exit(1); // Exit the process with an error code
  }
}

scheduleWorkflow({
  topic: "On-the-job upskilling",
});


### File: ./gotohuman/handleHumanResponse.ts ###
import express, { Request, Response } from 'express';
import { client } from "./src/client";
import { HumanResponseEvent } from './src/events';

const app = express();
app.use(express.json());
const port = process.env.PORT || 4000;

app.post('/', async (req: Request, res: Response) => {
  console.log(`Received webhook from gotoHuman`, req.body)

  try {
    // Our gotoHuman review form also contains a buttonSelect with the ID 'publishDecision', value: publish|discard
    const workflowResponse = await client.sendWorkflowEvent({
      event: {
        name: HumanResponseEvent.name,
        input: { 
          linkedInPost: req.body?.responseValues?.linkedInPost?.value || "",
          publishDecision: req.body?.responseValues?.publishDecision?.value || ""
        },
      },
      workflow: {
        workflowId: req.body?.meta?.restackWorkflowId,
        runId: req.body?.meta?.restackRunId,
      },
    });
    console.log(`Sent event to workflow. Returned ${workflowResponse}`)
  
    res.status(200).json({msg: 'Processed human feedback ' + workflowResponse})
  } catch(e) {
    console.error("Sending event to workflow failed!", e)
    res.status(500).json({error: `Could not process webhook!`})
  }
});

app.listen(port, () => {
  console.log(`Server running at http://localhost:${port}`);
});

### File: ./gotohuman/src/workflows/writePost.ts ###
import { log, step, condition, workflowInfo } from "@restackio/ai/workflow";
import { onEvent } from "@restackio/ai/event";
import { z } from "zod";
import zodToJsonSchema from "zod-to-json-schema";
import { HumanResponseInput, HumanResponseEvent } from "../events";
import * as functions from "../functions";

interface TopicInput {
  topic: string;
}

export async function writePostWorkflow({ topic }: TopicInput) {
  const workflowId = workflowInfo().workflowId
  const runId = workflowInfo().runId
  let endWorkflow = false;

  // Step 1: Create LinkedIn post with openai
  
  const prompt = `Write a short LinkedIn post about '${topic}' and how it will be affected by AI.`;

  const ResponseSchema = z.object({
    linkedInPost: z.string().describe("The drafted LinkedIn post."),
  });
  const jsonSchema = {
    name: "post",
    schema: zodToJsonSchema(ResponseSchema),
  };

  const openaiOutput = await step<typeof functions>({
    taskQueue: "openai",
  }).openaiChatCompletionsBase({
    userContent: prompt,
    jsonSchema,
  });

  const response = openaiOutput.result.choices[0].message.content ?? "";
  const {linkedInPost} = JSON.parse(response)

  log.info("drafted post: ", { DRAFT: linkedInPost });

  // Step 2: Request a human review
  
  const { reviewId: gotoHumanReviewId } = await step<typeof functions>({}).requestReview({
    topic,
    postDraft: linkedInPost,
    workflowId,
    runId
  });

  log.info("awaiting gotoHuman review, ID: ", { gotoHumanReviewId });

  // Step 3: Listen to review response and publish if approved

  let message;
  onEvent(HumanResponseEvent, async (event: HumanResponseInput) => {
    log.info("Received human response from gotoHuman", event);

    if (event.publishDecision === 'publish') {
      message = await step<typeof functions>({}).publishPost({
        post: event.linkedInPost,
      });
    } else {
      message = "Discarded"
    }
    endWorkflow = true;
    return message;
  });

  await condition(() => endWorkflow);
  return message;
}


### File: ./gotohuman/src/workflows/index.ts ###
export * from "./writePost";

### File: ./gotohuman/src/client.ts ###
import Restack from "@restackio/ai";

import "dotenv/config";

export const connectionOptions = {
  engineId: process.env.RESTACK_ENGINE_ID!,
  address: process.env.RESTACK_ENGINE_ADDRESS!,
  apiKey: process.env.RESTACK_ENGINE_API_KEY!,
};

export const client = new Restack(
  process.env.RESTACK_ENGINE_API_KEY ? connectionOptions : undefined
);

### File: ./gotohuman/src/functions/publishPost.ts ###
interface Input {
  post: string;
}

export async function publishPost({ post }: Input): Promise<string> {
  // TODO: Publish the post here
  return `Published: ${post.slice(0,20)}`;
}


### File: ./gotohuman/src/functions/requestReview.ts ###
import { gotoHumanClient } from "./utils/client";

interface Input {
  topic: string;
  postDraft: string;
  workflowId: string;
  runId: string;
}

interface Output {
  reviewId: string;
}

export async function requestReview({topic, postDraft, workflowId, runId}: Input): Promise<Output> {
  const formId = process.env.GOTOHUMAN_FORM_ID
  if (!formId) {
    throw new Error("A form ID is required to request a review for a gotoHuman form.");
  }
  // Our gotoHuman review form contains dynamic components with the IDs 'topic' and 'linkedInPost'
  const gotoHuman = gotoHumanClient();
  const { reviewId } = await gotoHuman.createReview(formId)
    .addFieldData("topic", topic)
    .addFieldData("linkedInPost", postDraft)
    .addMetaData("restackWorkflowId", workflowId)
    .addMetaData("restackRunId", runId)
    .sendRequest()
  return { reviewId };
}


### File: ./gotohuman/src/functions/utils/client.ts ###
import { GotoHuman } from "gotohuman";

let instance: GotoHuman | null = null;

export const gotoHumanClient = ({
  apiKey = process.env.GOTOHUMAN_API_KEY,
}: {
  apiKey?: string;
} = {}): GotoHuman => {
  if (!apiKey) {
    throw new Error("API key is required to create gotoHuman client.");
  }

  if (!instance) {
    instance = new GotoHuman(apiKey);
  }
  return instance;
};

### File: ./gotohuman/src/functions/index.ts ###
export * from "./requestReview";
export * from "./publishPost";
export * from "./openai";


### File: ./gotohuman/src/functions/openai/types/events.ts ###
import OpenAI from "openai/index";

export type StreamEvent = {
  chunkId?: string;
  response: string;
  assistantName?: string;
  isLast: boolean;
};

export type ToolCallEvent =
  OpenAI.Chat.Completions.ChatCompletionChunk.Choice.Delta.ToolCall & {
    function: {
      name: string;
      input: JSON;
    };
    assistantName?: string;
  };


### File: ./gotohuman/src/functions/openai/types/index.ts ###
export * from "./events";


### File: ./gotohuman/src/functions/openai/chat/completionsBase.ts ###
import { FunctionFailure, log } from "@restackio/ai/function";
import OpenAI from "openai/index";
import { ChatCompletionCreateParamsNonStreaming } from "openai/resources/chat/completions";
import { openaiClient } from "../utils/client";
import { openaiCost, Price } from "../utils/cost";
import { ChatCompletion, ChatModel } from "openai/resources/index";

export type UsageOutput = { tokens: number; cost: number };

export type OpenAIChatInput = {
  userContent: string;
  systemContent?: string;
  model?: ChatModel;
  jsonSchema?: {
    name: string;
    schema: Record<string, unknown>;
  };
  price?: Price;
  apiKey?: string;
  params?: ChatCompletionCreateParamsNonStreaming;
  tools?: OpenAI.Chat.Completions.ChatCompletionTool[];
  toolChoice?: OpenAI.Chat.Completions.ChatCompletionToolChoiceOption;
};

export const openaiChatCompletionsBase = async ({
  userContent,
  systemContent = "",
  model = "gpt-4o-mini",
  jsonSchema,
  price,
  apiKey,
  params,
  tools,
  toolChoice,
}: OpenAIChatInput): Promise<{ result: ChatCompletion; cost?: number }> => {
  try {
    const openai = openaiClient({ apiKey });

    const isO1Model = model.startsWith("o1-");

    const o1ModelParams = {
      temperature: 1,
      top_p: 1,
      frequency_penalty: 0,
      presence_penalty: 0,
    };

    const chatParams: ChatCompletionCreateParamsNonStreaming = {
      messages: [
        ...(systemContent ? [{ role: "system" as const, content: systemContent }] : []),
        { role: "user" as const, content: userContent },
        ...(params?.messages ?? []),
      ],
      ...(jsonSchema && {
        response_format: {
          type: "json_schema",
          json_schema: {
            name: jsonSchema.name,
            strict: true,
            schema: jsonSchema.schema,
          },
        },
      }),
      model,
      ...(tools && { tools }),
      ...(toolChoice && { tool_choice: toolChoice }),
      ...params,
      ...(isO1Model && o1ModelParams),
    };

    log.debug("OpenAI chat completion params", {
      chatParams,
    });

    const completion = await openai.chat.completions.create(chatParams);

    return {
      result: completion,
      cost:
        price &&
        openaiCost({
          price,
          tokensCount: {
            input: completion.usage?.prompt_tokens ?? 0,
            output: completion.usage?.completion_tokens ?? 0,
          },
        }),
    };
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error OpenAI chat: ${error}`);
  }
};

### File: ./gotohuman/src/functions/openai/chat/completionsStream.ts ###
import OpenAI from "openai/index";
import { ChatCompletionChunk } from "openai/resources/chat/completions";

import Restack from "@restackio/ai";
import { currentWorkflow, log } from "@restackio/ai/function";

import { StreamEvent, ToolCallEvent } from "../types/events";

import { aggregateStreamChunks } from "../utils/aggregateStream";
import { mergeToolCalls } from "../utils/mergeToolCalls";
import { openaiClient } from "../utils/client";
import { openaiCost, Price } from "../utils/cost";
import { SendWorkflowEvent } from "@restackio/ai/event";
import { ChatModel } from "openai/resources/index";

export async function openaiChatCompletionsStream({
  model = "gpt-4o-mini",
  userName,
  newMessage,
  assistantName,
  messages = [],
  tools,
  toolEvent,
  streamAtCharacter,
  streamEvent,
  apiKey,
  price,
}: {
  model?: ChatModel;
  userName?: string;
  newMessage?: string;
  assistantName?: string;
  messages?: OpenAI.Chat.Completions.ChatCompletionMessageParam[];
  tools?: OpenAI.Chat.Completions.ChatCompletionTool[];
  toolEvent?: {
    workflowEventName: string;
    workflow?: SendWorkflowEvent["workflow"];
  };
  streamAtCharacter?: string;
  streamEvent?: {
    workflowEventName: string;
    workflow?: SendWorkflowEvent["workflow"];
  };
  apiKey?: string;
  price?: Price;
}) {
  const restack = new Restack();
  const workflow = currentWorkflow().workflowExecution;

  log.debug("workflow", { workflow });

  if (newMessage) {
    messages.push({
      role: "user",
      name: userName,
      content: newMessage,
    });
  }

  const openai = openaiClient({ apiKey });
  const chatStream = await openai.chat.completions.create({
    model: model,
    messages,
    tools,
    stream: true,
    stream_options: {
      include_usage: true,
    },
  });

  const [stream, streamEnd] = chatStream.tee();
  const readableStream = streamEnd.toReadableStream() as unknown as ReadableStream<any>;
  const aggregatedStream = await aggregateStreamChunks(readableStream);

  let finishReason: ChatCompletionChunk.Choice["finish_reason"];
  let response: ChatCompletionChunk.Choice.Delta["content"] = "";
  let tokensCountInput = 0;
  let tokensCountOutput = 0;

  for await (const chunk of stream) {
    let content = chunk.choices[0]?.delta?.content || "";
    finishReason = chunk.choices[0]?.finish_reason;
    tokensCountInput += chunk.usage?.prompt_tokens ?? 0;
    tokensCountOutput += chunk.usage?.completion_tokens ?? 0;

    if (finishReason === "tool_calls") {
      const { toolCalls } = mergeToolCalls(aggregatedStream);
      await Promise.all(
        toolCalls.map((toolCall) => {
          if (toolEvent) {
            const functionArguments = JSON.parse(
              toolCall.function?.arguments ?? ""
            );

            const input: ToolCallEvent = {
              ...toolCall,
              function: {
                name: toolCall.function?.name ?? "",
                input: functionArguments,
              },
              assistantName,
            };

            if (toolEvent) {
              const workflowEvent = {
                event: {
                  name: toolEvent.workflowEventName,
                  input,
                },
                workflow: {
                  ...workflow,
                  ...toolEvent.workflow,
                },
              };
              log.debug("toolEvent sendWorkflowEvent", { workflowEvent });

              restack.sendWorkflowEvent(workflowEvent);
            }
          }
        })
      );
      return {
        result: {
          messages,
          toolCalls,
        },
        cost:
          price &&
          openaiCost({
            price,
            tokensCount: {
              input: tokensCountInput,
              output: tokensCountOutput,
            },
          }),
      };
    } else {
      response += content;
      if (
        content.trim().slice(-1) === streamAtCharacter ||
        finishReason === "stop"
      ) {
        if (response.length) {
          const input: StreamEvent = {
            chunkId: chunk.id,
            response,
            assistantName,
            isLast: finishReason === "stop",
          };
          if (streamEvent) {
            const workflowEvent = {
              event: {
                name: streamEvent.workflowEventName,
                input,
              },
              workflow: {
                ...workflow,
                ...streamEvent.workflow,
              },
            };
            log.debug("streamEvent sendWorkflowEvent", { workflowEvent });
            restack.sendWorkflowEvent(workflowEvent);
          }
        }
      }

      if (finishReason === "stop") {
        const newMessage: OpenAI.Chat.Completions.ChatCompletionMessageParam = {
          content: response,
          role: "assistant",
          name: assistantName,
        };

        messages.push(newMessage);

        return {
          result: {
            messages,
          },
          cost:
            price &&
            openaiCost({
              price,
              tokensCount: {
                input: tokensCountInput,
                output: tokensCountOutput,
              },
            }),
        };
      }
    }
  }
}


### File: ./gotohuman/src/functions/openai/chat/index.ts ###
export * from "./completionsBase";
export * from "./completionsStream";


### File: ./gotohuman/src/functions/openai/utils/cost.ts ###
export type TokensCount = {
  input: number;
  output: number;
};

export type Price = {
  input: number;
  output: number;
};
export const openaiCost = ({
  tokensCount,
  price,
}: {
  tokensCount: TokensCount;
  price: Price;
}): number => {
  let cost = 0;
  const { input: inputTokens, output: outputTokens } = tokensCount;
  const { input: inputPrice, output: outputPrice } = price;

  cost = inputTokens * inputPrice + outputTokens * outputPrice;

  return cost;
};


### File: ./gotohuman/src/functions/openai/utils/aggregateStream.ts ###
export async function aggregateStreamChunks(stream: ReadableStream) {
  const reader = stream.getReader();
  const chunks: Uint8Array[] = [];

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    if (value) chunks.push(value);
  }

  const aggregated = new Uint8Array(
    chunks.reduce((acc, chunk) => acc + chunk.length, 0)
  );
  let offset = 0;
  for (const chunk of chunks) {
    aggregated.set(chunk, offset);
    offset += chunk.length;
  }

  const textContent = new TextDecoder().decode(aggregated);
  const jsonObjects = textContent
    .split("\n")
    .filter((line) => line.trim())
    .map((line) => JSON.parse(line));
  return jsonObjects;
}


### File: ./gotohuman/src/functions/openai/utils/client.ts ###
import OpenAI from "openai/index";
import "dotenv/config";

let openaiInstance: OpenAI | null = null;

export const openaiClient = ({
  apiKey = process.env.OPENAI_API_KEY,
}: {
  apiKey?: string;
}): OpenAI => {
  if (!apiKey) {
    throw new Error("API key is required to create OpenAI client.");
  }

  if (!openaiInstance) {
    openaiInstance = new OpenAI({
      apiKey,
    });
  }
  return openaiInstance;
};


### File: ./gotohuman/src/functions/openai/utils/index.ts ###
export * from "./aggregateStream";
export * from "./client";
export * from "./cost";
export * from "./mergeToolCalls";


### File: ./gotohuman/src/functions/openai/utils/mergeToolCalls.ts ###
import OpenAI from "openai/index";
import { ChatCompletionChunk } from "openai/resources/chat/completions.mjs";

export function mergeToolCalls(aggregatedStream: ChatCompletionChunk[]) {
  const toolCalls: OpenAI.Chat.Completions.ChatCompletionChunk.Choice.Delta.ToolCall[] =
    [];

  aggregatedStream.forEach((chunk) => {
    chunk.choices.forEach((choice) => {
      if (choice.delta.tool_calls) {
        choice.delta.tool_calls.forEach((toolCall) => {
          const lastToolCall = toolCalls[toolCalls.length - 1];
          if (toolCall.id) {
            toolCalls.push({
              ...toolCall,
              function: { ...toolCall.function, arguments: "" },
            });
          } else if (
            lastToolCall &&
            lastToolCall.function &&
            toolCall.function?.arguments
          ) {
            lastToolCall.function.arguments += toolCall.function.arguments;
          }
        });
      }
    });
  });

  return { toolCalls };
}


### File: ./gotohuman/src/functions/openai/index.ts ###
export * from "./chat";
export * from "./thread";


### File: ./gotohuman/src/functions/openai/thread/createMessageOnThread.ts ###
import OpenAI from "openai/index";
import { FunctionFailure } from "@restackio/ai/function";

import { openaiClient } from "../utils/client";

export async function createMessageOnThread({
  apiKey,
  threadId,
  content,
  role,
}: {
  apiKey: string;
  threadId: string;
  content: string;
  role: OpenAI.Beta.Threads.MessageCreateParams["role"];
}) {
  try {
    const openai = openaiClient({ apiKey });
    await openai.beta.threads.messages.create(threadId, {
      role,
      content,
    });
  } catch (error) {
    throw FunctionFailure.nonRetryable(
      `Error creating message thread: ${error}`
    );
  }
}


### File: ./gotohuman/src/functions/openai/thread/runThread.ts ###
import { FunctionFailure } from "@restackio/ai/function";
import { Stream } from "openai/streaming";
import { AssistantStreamEvent } from "openai/resources/beta/index";
import { Run } from "openai/resources/beta/threads/runs/index";

import { openaiClient } from "../utils/client";

export async function runThread({
  apiKey,
  threadId,
  assistantId,
  stream = false,
}: {
  apiKey: string;
  threadId: string;
  assistantId: string;
  stream: boolean;
}): Promise<Stream<AssistantStreamEvent> | Run> {
  try {
    const openai = openaiClient({ apiKey });

    const run = await openai.beta.threads.runs.create(threadId, {
      assistant_id: assistantId,
      ...(stream && { stream }),
    });

    return run;
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error running thread: ${error}`);
  }
}

### File: ./gotohuman/src/functions/openai/thread/createAssistant.ts ###
import { ChatModel } from "openai/resources/index";
import { FunctionFailure } from "@restackio/ai/function";
import { Assistant, AssistantTool } from "openai/resources/beta/index";

import { openaiClient } from "../utils/client";

export async function createAssistant({
  apiKey,
  name,
  instructions,
  model = "gpt-4o-mini",
  tools = [],
}: {
  apiKey: string;
  name: string;
  instructions: string;
  tools?: AssistantTool[];
  model: ChatModel;
}): Promise<Assistant> {
  try {
    const openai = openaiClient({ apiKey });

    const assistant = await openai.beta.assistants.create({
      name,
      instructions,
      model,
      tools,
    });

    return assistant;
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error creating assistant: ${error}`);
  }
}


### File: ./gotohuman/src/functions/openai/thread/index.ts ###
export * from "./createAssistant";
export * from "./createMessageOnThread";
export * from "./createThread";
export * from "./runThread";


### File: ./gotohuman/src/functions/openai/thread/createThread.ts ###
import { FunctionFailure } from "@restackio/ai/function";
import { Thread } from "openai/resources/beta/index";

import { openaiClient } from "../utils/client";

export async function createThread({
  apiKey,
}: {
  apiKey: string;
}): Promise<Thread> {
  try {
    const openai = openaiClient({ apiKey });
    const thread = await openai.beta.threads.create();

    return thread;
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error creating thread: ${error}`);
  }
}


### File: ./gotohuman/src/events/humanResponseEvent.ts ###
import { defineEvent } from "@restackio/ai/event";

export type HumanResponseInput = {
  linkedInPost: string;
  publishDecision: string;
};

export const HumanResponseEvent = defineEvent<string>("humanResponseEvent");


### File: ./gotohuman/src/events/index.ts ###
export * from "./humanResponseEvent";


### File: ./gotohuman/src/services.ts ###
import { requestReview, publishPost } from "./functions";
import { client } from "./client";

async function services() {
  const workflowsPath = require.resolve("./workflows");
  try {
    await Promise.all([
      // Start service with current workflows and functions
      client.startService({
        workflowsPath,
        functions: { requestReview, publishPost },
      }),
    ]);

    console.log("Services running successfully.");
  } catch (e) {
    console.error("Failed to run services", e);
  }
}

services().catch((err) => {
  console.error("Error running services:", err);
});


### File: ./swagger/src/client.ts ###
import Restack from '@restackio/ai';
import dotenv from 'dotenv';

dotenv.config();

export const connectionOptions = {
  engineId: process.env.RESTACK_ENGINE_ID!,
  address: process.env.RESTACK_ENGINE_ADDRESS!,
  apiKey: process.env.RESTACK_ENGINE_API_KEY!,
};

export const client = new Restack(
  process.env.RESTACK_ENGINE_API_KEY ? connectionOptions : undefined
);

### File: ./swagger/src/server.ts ###
import express from 'express';
import dotenv from 'dotenv';
import swaggerUi from 'swagger-ui-express';
import { client } from './client';
import { specs } from './swagger';

dotenv.config();

const app = express();
const PORT = process.env.PORT || 8000;

app.use(express.json());

// Swagger UI setup
app.use('/api-docs', swaggerUi.serve, swaggerUi.setup(specs));

/**
 * @swagger
 * /:
 *   post:
 *     summary: Schedule a workflow
 *     description: Schedule a new workflow with the given name and ID
 *     requestBody:
 *       required: true
 *       content:
 *         application/json:
 *           schema:
 *             type: object
 *             required:
 *               - workflowName
 *               - workflowId
 *             properties:
 *               workflowName:
 *                 type: string
 *                 description: Name of the workflow to schedule
 *               workflowId:
 *                 type: string
 *                 description: Unique identifier for the workflow
 *     responses:
 *       200:
 *         description: Workflow scheduled successfully
 *         content:
 *           application/json:
 *             schema:
 *               type: object
 *               properties:
 *                 runId:
 *                   type: string
 *                   description: The ID of the scheduled workflow run
 */
app.post('/', async (req, res) => {
    const { workflowName, workflowId } = req.body;
    const runId = await client.scheduleWorkflow({ workflowName, workflowId });
    res.json({ runId });
});

app.listen(PORT, () => {
    console.log(`Server is running on port ${PORT}`);
    console.log(`Swagger documentation available at http://localhost:${PORT}/api-docs`);
});

### File: ./swagger/src/swagger.ts ###
import swaggerJsdoc from 'swagger-jsdoc';

const options = {
  definition: {
    openapi: '3.0.0',
    info: {
      title: 'Restack Express API',
      version: '1.0.0',
      description: 'API documentation for Restack Express example',
    },
    servers: [
      {
        url: 'http://localhost:8000',
        description: 'Development server',
      },
    ],
  },
  apis: ['src/server.ts'],
};

export const specs = swaggerJsdoc(options);

### File: ./express-together-llamaindex/src/workflows/chatCompletionBasic.ts ###
import { step } from "@restackio/ai/workflow";
import * as functions from "../functions";
import { z } from "zod";
import zodToJsonSchema from "zod-to-json-schema";

const MessageSchema = z.object({
    message: z.string().describe("The greeting message."),
});

const jsonSchema = zodToJsonSchema(MessageSchema, 'Message');

export async function chatCompletionBasic({ name }: { name: string }) {

    // Step 1 create greeting message with meta-llama without response format

    const greetingOutput = await step<typeof functions>({
        taskQueue: 'together',
    }).togetherChatCompletionBasic({
        messages: [{ "role": "user", "content": `Write a greeting message to ${name}` }],
        model: 'meta-llama/Llama-3.2-3B-Instruct-Turbo',


    });

    // Step 2 create a response with response format with a model supporting response format

    const goodbyeOutput = await step<typeof functions>({
        taskQueue: 'together',
    }).togetherChatCompletionBasic({
        messages: [{ "role": "user", "content": `Write a goodbye message to ${name}` }],
        model: 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo',
        response_format: {
            type: 'json_object',
            // @ts-ignore
            schema: jsonSchema
        },
    });

    return {
        greetingOutput,
        goodbyeOutput
    };
}



### File: ./express-together-llamaindex/src/workflows/index.ts ###
export * from './chatCompletionBasic';
export * from './llamaindexTogetherSimple';

### File: ./express-together-llamaindex/src/workflows/llamaindexTogetherSimple.ts ###
import { step } from "@restackio/ai/workflow";
import * as functions from "../functions";


export async function llamaindexTogetherSimple() {
    // Step 1: Query a model with the llamaIndex and Together integration
    const response = await step<typeof functions>({
    }).llamaIndexQueryTogether({
        query: "What is the meaning of life?",
        model: "meta-llama/Llama-3.2-3B-Instruct-Turbo"
    });

    return response;
}

### File: ./express-together-llamaindex/src/client.ts ###
import Restack from '@restackio/ai';
import dotenv from 'dotenv';

dotenv.config();

export const connectionOptions = {
  engineId: process.env.RESTACK_ENGINE_ID!,
  address: process.env.RESTACK_ENGINE_ADDRESS!,
  apiKey: process.env.RESTACK_ENGINE_API_KEY!,
};

export const client = new Restack(
  process.env.RESTACK_ENGINE_API_KEY ? connectionOptions : undefined
);

### File: ./express-together-llamaindex/src/functions/llamaindex/utils/llamaIndexTogetherClient.ts ###
import { TogetherLLM, Settings } from "llamaindex";

export function llamaIndexTogetherClient({ model }: { model: TogetherLLM["model"] }) {
    Settings.llm = new TogetherLLM({
        apiKey: process.env.TOGETHER_API_KEY,
        model: model,
    });
    return Settings.llm;
}

### File: ./express-together-llamaindex/src/functions/llamaindex/queryTogether.ts ###
import { llamaIndexTogetherClient } from "./utils/llamaIndexTogetherClient";
import { TogetherLLM } from "llamaindex";

export async function llamaIndexQueryTogether({ query, model }: { query: string, model: TogetherLLM["model"] }) {
    const client = llamaIndexTogetherClient({ model });

    const response = await client.chat({
        messages: [{ role: "user", content: query }],
    });

    return response;
}

### File: ./express-together-llamaindex/src/functions/llamaindex/index.ts ###
export * from './queryTogether';

### File: ./express-together-llamaindex/src/functions/together-ai/utils/client.ts ###
import Together from "together-ai";

export const togetherClient = new Together({
    apiKey: process.env.TOGETHER_API_KEY,
});


### File: ./express-together-llamaindex/src/functions/together-ai/chatCompletionBasic.ts ###
import Together from 'together-ai';
import { togetherClient } from './utils/client';

export async function togetherChatCompletionBasic(params: Together.Chat.CompletionCreateParamsNonStreaming) {
    const response = await togetherClient.chat.completions.create(params);
    return response as Together.Chat.ChatCompletion;
}


### File: ./express-together-llamaindex/src/functions/together-ai/index.ts ###
export * from './chatCompletionBasic';

### File: ./express-together-llamaindex/src/functions/index.ts ###
export * from './llamaindex';
export * from './together-ai';

### File: ./express-together-llamaindex/src/services.ts ###
// Simple example to start two services in the same file
import { togetherChatCompletionBasic, llamaIndexQueryTogether } from "./functions";
import { client } from "./client";

export async function services() {
    const workflowsPath = require.resolve("./workflows");
    try {
        await Promise.all([
            // Generic service with current workflows and functions
            client.startService({
                workflowsPath,
                functions: {

                    // add other functions here
                },
            }),
            // Start the together service to queue function calls to the Together API with rate limiting
            // https://docs.together.ai/docs/rate-limits
            client.startService({
                taskQueue: 'together',
                functions: { togetherChatCompletionBasic, llamaIndexQueryTogether },
                options: {
                    rateLimit: (60 / 60), // 60 RPM -> 1 RPS 
                },
            }),
        ]);

        console.log("Services running successfully.");
    } catch (e) {
        console.error("Failed to run services", e);
    }
}

services().catch((err) => {
    console.error("Error running services:", err);
});


### File: ./express-together-llamaindex/src/server.ts ###
import express from 'express';
import dotenv from 'dotenv';
import { client } from './client';
import { services } from './services';

dotenv.config();

const app = express();
const PORT = process.env.PORT || 8000;

// Start the services
services();

app.use(express.json());

app.post('/', async (req, res) => {
    const { workflowName, workflowId } = req.body;
    const runId = await client.scheduleWorkflow({ workflowName, workflowId, input: req.body });
    res.json({ runId });
});

app.listen(PORT, () => {
    console.log(`Server is running on port ${PORT}`);
});

### File: ./express/src/client.ts ###
import Restack from '@restackio/ai';
import dotenv from 'dotenv';

dotenv.config();

export const connectionOptions = {
  engineId: process.env.RESTACK_ENGINE_ID!,
  address: process.env.RESTACK_ENGINE_ADDRESS!,
  apiKey: process.env.RESTACK_ENGINE_API_KEY!,
};

export const client = new Restack(
  process.env.RESTACK_ENGINE_API_KEY ? connectionOptions : undefined
);

### File: ./express/src/server.ts ###
import express from 'express';
import dotenv from 'dotenv';
import { client } from './client';

dotenv.config();

const app = express();
const PORT = process.env.PORT || 8000;

app.use(express.json());

app.post('/', async (req, res) => {
    const { workflowName, workflowId } = req.body;
    const runId = await client.scheduleWorkflow({ workflowName, workflowId });
    res.json({ runId });
});

app.listen(PORT, () => {
    console.log(`Server is running on port ${PORT}`);
});

### File: ./email-sender/scheduleWorkflow.ts ###
import { client } from './src/client';

import 'dotenv/config';

type SendEmailWorkflowInput = {
  emailContext: string;
  subject: string;
  to: string;
};

async function scheduleWorkflow(input: SendEmailWorkflowInput) {
  try {
    const workflowId = `${Date.now()}-sendEmailWorkflow`;
    await client.scheduleWorkflow({
      workflowName: 'sendEmailWorkflow',
      workflowId,
      input,
    });

    console.log('Workflow scheduled successfully');

    process.exit(0); // Exit the process successfully
  } catch (error) {
    console.error('Error scheduling workflow:', error);
    process.exit(1); // Exit the process with an error code
  }
}

scheduleWorkflow({
  emailContext:
    'This email should contain a greeting. And telling user we have launched a new AI feature with Restack workflows. Workflows now offer logging and automatic retries when one of its steps fails. Name of user is not provided. You can set as goodbye message on the email just say "Best regards" or something like that. No need to mention name of user or name of person sending the email.',
  subject: 'Hello from Restack',
  to: process.env.TO_EMAIL!,
});


### File: ./email-sender/scheduleWorkflowRetries.ts ###
import { client } from './src/client';

import 'dotenv/config';

type SendEmailWorkflowInput = {
  emailContext: string;
  subject: string;
  to: string;
};

async function scheduleWorkflowWithRetries(input: SendEmailWorkflowInput) {
  try {
    const workflowId = `${Date.now()}-sendEmailWorkflow`;
    await client.scheduleWorkflow({
      workflowName: 'sendEmailWorkflow',
      workflowId,
      input: {
        ...input,
        simulateFailure: true,
      },
    });

    console.log('Workflow scheduled successfully');

    process.exit(0); // Exit the process successfully
  } catch (error) {
    console.error('Error scheduling workflow:', error);
    process.exit(1); // Exit the process with an error code
  }
}

scheduleWorkflowWithRetries({
  emailContext:
    'This email should contain a greeting. And telling user we have launched a new AI feature with Restack workflows. Workflows now offer logging and automatic retries when one of its steps fails. Name of user is not provided. You can set as goodbye message on the email just say "Best regards" or something like that. No need to mention name of user or name of person sending the email.',
  subject: 'Hello from Restack',
  to: process.env.TO_EMAIL!,
});


### File: ./email-sender/src/workflows/sendEmail.ts ###
import { log, step } from '@restackio/ai/workflow';

import * as functions from '../functions';

type SendEmailWorkflowInput = {
  emailContext: string;
  subject: string;
  to: string;
  simulateFailure?: boolean;
};

const emailRetryPolicy = {
  initialInterval: '10s', // after each failure, wait 10 seconds before retrying
  backoffCoefficient: 1, // no exponential backoff, meaning we wait the same amount of time between retries.
};

export async function sendEmailWorkflow({
  emailContext,
  subject,
  to,
  simulateFailure = false,
}: SendEmailWorkflowInput) {
  const text = await step<typeof functions>({
    retry: emailRetryPolicy,
  }).generateEmailContent({
    emailContext,
    simulateFailure,
  });

  log.info('Email content generated');

  await step<typeof functions>({}).sendEmail({
    text,
    subject,
    to,
  });

  log.info('Email sent successfully');
  return 'Email sent successfully';
}


### File: ./email-sender/src/workflows/index.ts ###
export * from './sendEmail';


### File: ./email-sender/src/client.ts ###
import Restack from '@restackio/ai';

import 'dotenv/config';

export const connectionOptions = {
  engineId: process.env.RESTACK_ENGINE_ID!,
  address: process.env.RESTACK_ENGINE_ADDRESS!,
  apiKey: process.env.RESTACK_ENGINE_API_KEY!,
};

export const client = new Restack(
  process.env.RESTACK_ENGINE_API_KEY ? connectionOptions : undefined
);


### File: ./email-sender/src/functions/generateEmailContent.ts ###
import OpenAI from 'openai/index';
import { FunctionFailure } from '@restackio/ai/function';

type GenerateEmailContentInput = {
  emailContext: string;
  simulateFailure?: boolean;
};

let tries = 0;

export async function generateEmailContent({
  emailContext,
  simulateFailure = false,
}: GenerateEmailContentInput) {
  // this is just to simulate a failure and showcase how Restack will automatically retry this function when called from a workflow.
  // after it is successful, your workflow will continue with its next steps.
  if (simulateFailure && tries === 0) {
    tries += 1;
    throw FunctionFailure.retryable(
      'Intentional failure to showcase retry logic'
    );
  }
  if (!process.env.OPENAI_API_KEY) {
    throw FunctionFailure.nonRetryable('OPENAI_API_KEY is not set');
  }

  const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
  });

  const response = await openai.chat.completions.create({
    model: 'gpt-4o-mini',
    messages: [
      {
        role: 'system',
        content:
          'You are a helpful assistant that generates short emails based on the provided context.',
      },
      {
        role: 'user',
        content: `Generate a short email based on the following context: ${emailContext}`,
      },
    ],
    max_tokens: 150,
  });

  const text = response.choices[0].message.content;

  if (!text) {
    throw FunctionFailure.retryable(
      'Ai could not generate email content, retrying...'
    );
  }

  return text;
}


### File: ./email-sender/src/functions/sendEmail.ts ###
import sgMail from '@sendgrid/mail';
import { FunctionFailure } from '@restackio/ai/function';

import { generateEmailContent } from './generateEmailContent';

import 'dotenv/config';

type EmailInput = {
  text: string;
  subject: string;
  to: string;
};

export async function sendEmail({ text, subject, to }: EmailInput) {
  const fromEmail = process.env.FROM_EMAIL;

  if (!fromEmail) {
    throw FunctionFailure.nonRetryable('FROM_EMAIL is not set');
  }

  if (!process.env.SENDGRID_API_KEY) {
    throw FunctionFailure.nonRetryable('SENDGRID_API_KEY is not set');
  }

  sgMail.setApiKey(process.env.SENDGRID_API_KEY);

  const msg = {
    to,
    from: fromEmail,
    subject,
    text,
  };

  try {
    await sgMail.send(msg);
  } catch (error) {
    throw FunctionFailure.nonRetryable('Failed to send email');
  }

  return 'Email sent successfully';
}


### File: ./email-sender/src/functions/index.ts ###
export * from './generateEmailContent';
export * from './sendEmail';


### File: ./email-sender/src/services.ts ###
import { generateEmailContent, sendEmail } from './functions';
import { client } from './client';

async function services() {
  const workflowsPath = require.resolve('./workflows');
  try {
    await Promise.all([
      // Start service with current workflows and functions
      client.startService({
        workflowsPath,
        functions: { sendEmail, generateEmailContent },
      }),
    ]);

    console.log('Services running successfully.');
  } catch (e) {
    console.error('Failed to run services', e);
  }
}

services().catch((err) => {
  console.error('Error running services:', err);
});


### File: ./google-gemini/scheduleWorkflow.ts ###
import { client } from "./src/client";

export type InputSchedule = {
  name: string;
};

async function scheduleWorkflow(input: InputSchedule) {
  try {
    const workflowId = `${Date.now()}-helloWorkflow`;
    const runId = await client.scheduleWorkflow({
      workflowName: "helloWorkflow",
      workflowId,
      input,
    });

    const result = await client.getWorkflowResult({ workflowId, runId });

    console.log("Workflow result:", result);

    process.exit(0); // Exit the process successfully
  } catch (error) {
    console.error("Error scheduling workflow:", error);
    process.exit(1); // Exit the process with an error code
  }
}

scheduleWorkflow({
  name: "John",
});


### File: ./google-gemini/src/workflows/index.ts ###
export * from "./hello";


### File: ./google-gemini/src/workflows/hello.ts ###
import { log, step } from "@restackio/ai/workflow";
import * as functions from "../functions";

interface Input {
  name: string;
}

export async function helloWorkflow({ name }: Input) {
  const userContent = `Greet this person: ${name}. In 4 words or less.`;

  // Step 1 create greeting message with google gemini

  const geminiOutput = await step<typeof functions>({
    taskQueue: "gemini",
  }).geminiGenerateContent({
    userContent,
  });

  const greetMessage = geminiOutput.result.choices[0].message.content ?? "";

  log.info("greeted", { greetMessage });

  // Step 2 create goodbye message with simple function

  const { message: goodbyeMessage } = await step<typeof functions>({}).goodbye({
    name,
  });

  log.info("goodbye", { goodbyeMessage });

  return {
    messages: [greetMessage, goodbyeMessage],
  };
}


### File: ./google-gemini/src/client.ts ###
import Restack from "@restackio/ai";

import "dotenv/config";

export const connectionOptions = {
  engineId: process.env.RESTACK_ENGINE_ID!,
  address: process.env.RESTACK_ENGINE_ADDRESS!,
  apiKey: process.env.RESTACK_ENGINE_API_KEY!,
};

export const client = new Restack(
  process.env.RESTACK_ENGINE_API_KEY ? connectionOptions : undefined
);


### File: ./google-gemini/src/functions/goodbye.ts ###
interface Input {
  name: string;
}

interface Output {
  message: string;
}

export async function goodbye(input: Input): Promise<Output> {
  return { message: `Goodbye, ${input.name}!` };
}


### File: ./google-gemini/src/functions/google-gemini/types/inputs.ts ###
export type GeminiChatInput = {
    userContent: string;
    systemContent?: string;
    model?: string;
    apiKey?: string;
    params?: {
      temperature?: number;
      topP?: number;
      topK?: number;
      candidateCount?: number;
      maxOutputTokens?: number;
      stopSequences?: string[];
    };
  };


### File: ./google-gemini/src/functions/google-gemini/types/index.ts ###
export * from "./inputs"

### File: ./google-gemini/src/functions/google-gemini/utils/client.ts ###
import { GoogleGenerativeAI } from "@google/generative-ai";

let geminiInstance: GoogleGenerativeAI | null = null;

export const geminiClient = ({
  apiKey = process.env.GEMINI_API_KEY,
}: {
  apiKey?: string;
}): GoogleGenerativeAI => {
  if (!apiKey) {
    throw new Error("API key is required to create Google Gemini client.");
  }

  if (!geminiInstance) {
    geminiInstance = new GoogleGenerativeAI(apiKey);
  }
  return geminiInstance;
};


### File: ./google-gemini/src/functions/google-gemini/utils/index.ts ###
export * from "./client";


### File: ./google-gemini/src/functions/google-gemini/index.ts ###
export * from "./text";


### File: ./google-gemini/src/functions/google-gemini/text/generateContentStream.ts ###
import { FunctionFailure, log } from "@restackio/ai/function";
import { geminiClient } from "../utils/client";
import { GeminiChatInput } from "../types";

export const geminiGenerateContentStream = async ({
  userContent,
  systemContent = "",
  model = "gemini-pro",
  apiKey,
  params,
}: GeminiChatInput): Promise<ReadableStream> => {
  try {
    const gemini = geminiClient({ apiKey });
    const model_ = gemini.getGenerativeModel({ model });

    const chatParams = {
      temperature: params?.temperature ?? 0.9,
      topP: params?.topP ?? 1,
      topK: params?.topK ?? 1,
      candidateCount: params?.candidateCount ?? 1,
      maxOutputTokens: params?.maxOutputTokens,
      stopSequences: params?.stopSequences,
    };

    log.debug("Gemini chat completion params", {
      chatParams,
    });

    const prompt = systemContent ? `${systemContent}\n${userContent}` : userContent;

    const result = await model_.generateContentStream(prompt);

    return new ReadableStream({
      async start(controller) {
        try {
          for await (const chunk of result.stream) {
            const text = chunk.text();
            controller.enqueue({
              choices: [{
                delta: {
                  role: "assistant",
                  content: text
                }
              }]
            });
          }
          controller.close();
        } catch (error) {
          controller.error(error);
        }
      }
    });

  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error Gemini chat stream: ${error}`);
  }
};


### File: ./google-gemini/src/functions/google-gemini/text/index.ts ###
export * from "./generateContent";
export * from "./generateContentStream";


### File: ./google-gemini/src/functions/google-gemini/text/generateContent.ts ###
import { FunctionFailure, log } from "@restackio/ai/function";
import { geminiClient } from "../utils/client";
import { GeminiChatInput } from "../types";

export const geminiGenerateContent = async ({
  userContent,
  systemContent = "",
  model = "gemini-pro",
  apiKey,
  params,
}: GeminiChatInput): Promise<{ result: any }> => {
  try {
    const gemini = geminiClient({ apiKey });
    const model_ = gemini.getGenerativeModel({ model });

    const chatParams = {
      temperature: params?.temperature ?? 0.9,
      topP: params?.topP ?? 1,
      topK: params?.topK ?? 1,
      candidateCount: params?.candidateCount ?? 1,
      ...(params?.maxOutputTokens && {
        maxOutputTokens: params.maxOutputTokens,
      }),
      ...(params?.stopSequences && { stopSequences: params.stopSequences }),
    };

    log.debug("Gemini chat completion params", {
      chatParams,
    });

    const prompt = systemContent
      ? `${systemContent}\n${userContent}`
      : userContent;

    const result = await model_.generateContent(prompt);
    const response = await result.response;
    const text = response.text();

    return {
      result: {
        choices: [
          {
            message: {
              role: "assistant",
              content: text,
            },
          },
        ],
      },
    };
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error Gemini chat: ${error}`);
  }
};


### File: ./google-gemini/src/functions/index.ts ###
export * from "./google-gemini";
export * from "./goodbye";


### File: ./google-gemini/src/services.ts ###
import { goodbye, geminiGenerateContent, geminiGenerateContentStream } from "./functions";

import { client } from "./client";

async function services() {
  const workflowsPath = require.resolve("./workflows");
  try {
    await Promise.all([
      // Start service with current workflows and functions
      client.startService({
        workflowsPath,
        functions: { goodbye },
      }),
      // Start the gemini service
      client.startService({
        taskQueue: "gemini",
        functions: { geminiGenerateContent, geminiGenerateContentStream },
      }),
    ]);

    console.log("Services running successfully.");
  } catch (e) {
    console.error("Failed to run services", e);
  }
}

services().catch((err) => {
  console.error("Error running services:", err);
});


### File: ./voice/callWorkflow.ts ###
import { client } from "./src/client";

async function scheduleWorkflow() {
  try {
    const workflowRunId = await client.scheduleWorkflow({
      workflowName: "twilioCallWorkflow",
      workflowId: `${Date.now()}-twilioCallWorkflow`,
      input: {
        to: process.env.FROM_NUMBER,
        from: process.env.YOUR_NUMBER,
        url: `https://${process.env.SERVER}/incoming`,
      },
    });

    console.log("Workflow scheduled successfully:", workflowRunId);

    process.exit(0); // Exit the process successfully
  } catch (error) {
    console.error("Error scheduling workflow:", error);
    process.exit(1); // Exit the process with an error code
  }
}

scheduleWorkflow();


### File: ./voice/src/workflows/room/room.ts ###
import {
  step,
  log,
  condition,
  startChild,
  workflowInfo,
} from "@restackio/ai/workflow";
import { onEvent } from "@restackio/ai/event";
import * as functions from "../../functions";
import { conversationWorkflow } from "../conversation/conversation";
import {
  audioInEvent,
  userEvent,
  streamEndEvent,
  RoomInfo,
  streamInfoEvent,
  UserEvent,
  roomMessageEvent,
} from "./events";
import { streamEvent } from "../conversation/events";
import { StreamEvent } from "../../functions/openai/types";
import { WebsocketEvent } from "../../functions/websocket/types";

export async function roomWorkflow({ address }: { address?: string }) {
  try {
    let currentstreamSid: string;
    let interactionCount = 0;
    let audioQueue: {
      audio: string;
      text: string;
    }[] = [];
    let isSendingAudio = false;
    let childConversationWorkflowRunId = "";

    const assistantName = "agent";

    console.log("address", address);
    // Start long running websocket and stream welcome message to websocket.
    onEvent(streamInfoEvent, async ({ streamSid }: RoomInfo) => {
      log.info(`Workflow update with streamSid: ${streamSid}`);
      step<typeof functions>({
        taskQueue: "websocket",
        scheduleToCloseTimeout: "1 hour",
        heartbeatTimeout: "2 minutes",
      }).websocketListen({
        streamSid,
        address,
        events: [
          {
            websocketEventName: "media",
            workflowEventName: audioInEvent.name,
          },
          {
            websocketEventName: "stop",
            workflowEventName: streamEndEvent.name,
          },
        ],
      });

      const welcomeMessage = "Hello! I am Pete from Apple.";

      const { media } = await step<typeof functions>({
        taskQueue: "deepgram",
      }).deepgramSpeak({
        text: welcomeMessage,
        twilioEncoding: true,
      });

      await step<typeof functions>({
        taskQueue: "websocket",
      }).websocketSend({
        name: "media",
        input: {
          streamSid,
          media: {
            trackId: assistantName,
            payload: media.payload,
          },
        },
        address,
      });

      await step<typeof functions>({
        taskQueue: "websocket",
      }).websocketSend({
        name: roomMessageEvent.name,
        input: {
          streamSid,
          data: { trackId: assistantName, text: welcomeMessage },
        },
        address,
      });

      currentstreamSid = streamSid;
      return { streamSid };
    });

    // Receives audio, transcribe it and send transcription to conversation with AI .

    onEvent(audioInEvent, async ({ streamSid, media }: WebsocketEvent) => {
      log.info(`Workflow update with streamSid: ${streamSid}`);

      if (!media?.payload || media.trackId === assistantName) return;

      const { result } = await step<typeof functions>({
        taskQueue: "deepgram",
      }).deepgramListen({
        base64Payload: media?.payload,
        twilioEncoding: true,
      });

      const transcript = result?.results.channels[0].alternatives[0].transcript;

      if (!transcript?.length) {
        const input: StreamEvent = {
          response: "Sorry i didn't understand. Can you repeat?",
          assistantName,
          isLast: true,
        };

        log.info("Answer to transcript ", { input });

        await step<typeof functions>({}).workflowSendEvent({
          event: {
            name: roomMessageEvent.name,
            input,
          },
          workflow: {
            workflowId: workflowInfo().workflowId,
            runId: workflowInfo().runId,
          },
        });
      }

      interactionCount += 1;

      step<typeof functions>({
        taskQueue: "websocket",
      }).websocketSend({
        name: userEvent.name,
        input: {
          streamSid,
          data: {
            trackId: media.trackId,
            text: transcript,
          },
        },
        address,
      });

      if (!childConversationWorkflowRunId) {
        const childWorkflow = await startChild(conversationWorkflow, {
          args: [
            {
              assistantName,
              userName: media.trackId,
              message: transcript!,
            },
          ],
          workflowId: `${streamSid}-conversationWorkflow`,
        });
        childConversationWorkflowRunId = childWorkflow.firstExecutionRunId;
      } else {
        const input: UserEvent = {
          userName: media.trackId,
          message: transcript!,
        };
        step<typeof functions>({
          taskQueue: `restack`,
        }).workflowSendEvent({
          event: {
            name: userEvent.name,
            input,
          },
          workflow: {
            workflowId: `${streamSid}-conversationWorkflow`,
            runId: childConversationWorkflowRunId,
          },
        });
      }
      return { streamSid };
    });

    // Receives AI answer, generates audio and stream it to websocket.

    onEvent(
      streamEvent,
      async ({ response, isLast, assistantName }: StreamEvent) => {
        const { media } = await step<typeof functions>({
          taskQueue: "deepgram",
        }).deepgramSpeak({
          text: response,
          twilioEncoding: true,
        });

        audioQueue.push({ audio: media.payload, text: response });

        if (!isSendingAudio && isLast) {
          isSendingAudio = true;

          while (audioQueue.length > 0) {
            const { audio } = audioQueue.shift()!;

            await step<typeof functions>({
              taskQueue: "websocket",
            }).websocketSend({
              name: "media",
              input: {
                streamSid: currentstreamSid,
                media: {
                  trackId: assistantName!,
                  payload: audio,
                },
              },
              address,
            });
          }

          await step<typeof functions>({
            taskQueue: "websocket",
          }).websocketSend({
            name: roomMessageEvent.name,
            input: {
              streamSid: currentstreamSid,
              data: { trackId: assistantName!, text: response },
            },
            address,
          });

          isSendingAudio = false;
        }

        return { response };
      }
    );

    // Terminates stream workflow.

    let ended = false;

    onEvent(streamEndEvent, async () => {
      log.info(`streamEnd received`);
      ended = true;
    });

    await condition(() => ended);

    return;
  } catch (error) {
    log.error("Error in streamRoom", { error });
    throw error;
  }
}


### File: ./voice/src/workflows/room/events.ts ###
import { defineEvent } from "@restackio/ai/event";
import { WebsocketEvent } from "../../functions/websocket/types";

export type RoomInfo = {
  streamSid: string;
};

export type UserEvent = {
  message: string;
  userName?: string;
};

export type RoomMessageEvent = {
  trackId: string;
  text: string;
};

export const streamInfoEvent = defineEvent<RoomInfo>("streamInfo");

export const audioInEvent = defineEvent<WebsocketEvent>("audioIn");

export const userEvent = defineEvent<UserEvent>("userMessage");

export const roomMessageEvent = defineEvent<RoomMessageEvent>("roomMessage");

export const streamEndEvent = defineEvent("streamEnd");


### File: ./voice/src/workflows/twilioCall.ts ###
import { log, step } from "@restackio/ai/workflow";
import * as functions from "../functions";
interface Output {
  sid: string;
}

export async function twilioCallWorkflow({
  to,
  from,
  url,
}: {
  to: string;
  from: string;
  url: string;
}): Promise<Output> {
  const { sid } = await step<typeof functions>({
    taskQueue: "twilio",
    scheduleToCloseTimeout: "1 minute",
  }).twilioCall({
    options: {
      to,
      from,
      url,
    },
  });

  if (!sid) {
    throw new Error("Not able to create Twilio call");
  }

  log.info("sid", { sid });

  return {
    sid,
  };
}


### File: ./voice/src/workflows/index.ts ###
export * from "./twilioCall";
export * from "./room/room";
export * from "./conversation/conversation";


### File: ./voice/src/workflows/conversation/events.ts ###
import { defineEvent } from "@restackio/ai/event";
import {
  StreamEvent,
  ToolCallEvent,
} from "../../functions/openai/types";

export const streamEvent = defineEvent<StreamEvent>("stream");
export const toolCallEvent = defineEvent<ToolCallEvent>("toolCall");
export const conversationEndEvent = defineEvent("conversationEndEvent");


### File: ./voice/src/workflows/conversation/conversation.ts ###
import { step, log, workflowInfo, condition } from "@restackio/ai/workflow";
import * as functions from "../../functions";
import { onEvent } from "@restackio/ai/event";
import { streamEvent, toolCallEvent, conversationEndEvent } from "./events";
import { UserEvent, userEvent } from "../room/events";

import {
  StreamEvent,
  ToolCallEvent,
} from "../../functions/openai/types";
import { agentPrompt } from "../../functions/openai/prompt";
import {
  ChatModel,
  ChatCompletionAssistantMessageParam,
  ChatCompletionMessageParam,
  ChatCompletionTool,
} from "openai/resources/index";

export async function conversationWorkflow({
  assistantName,
  userName,
  message,
}: {
  assistantName: string;
  userName: string;
  message: string;
}) {
  try {
    const parentWorkflow = workflowInfo().parent;
    if (!parentWorkflow) throw "no parent Workflow";

    let openaiChatMessages: ChatCompletionMessageParam[] = [];

    const tools = await step<typeof functions>({
      taskQueue: "erp",
    }).erpGetTools();

    const model: ChatModel = "gpt-4o-mini";

    const commonOpenaiOptions = {
      model,
      assistantName,
      tools,
      streamAtCharacter: "•",
      streamEvent: {
        workflowEventName: streamEvent.name,
        workflow: parentWorkflow,
      },
      toolEvent: {
        workflowEventName: toolCallEvent.name,
      },
    };

    const response = await step<typeof functions>({
      taskQueue: "openai",
    }).openaiChatCompletionsStream({
      userName,
      newMessage: message,
      messages: agentPrompt,
      ...commonOpenaiOptions,
    });

    if (response?.result?.messages) {
      openaiChatMessages = response.result.messages;
    }

    onEvent(userEvent, async ({ message, userName }: UserEvent) => {
      const response = await step<typeof functions>({
        taskQueue: "openai",
      }).openaiChatCompletionsStream({
        newMessage: message,
        userName,
        messages: openaiChatMessages,
        ...commonOpenaiOptions,
      });

      if (response?.result?.messages) {
        openaiChatMessages = response.result.messages;
      }

      if (response?.result?.toolCalls) {
        response.result.toolCalls.map(async (toolCall) => {
          const toolResponse = `Sure, let me ${toolCall?.function?.name}...`;
          const toolMessage: ChatCompletionAssistantMessageParam = {
            content: toolResponse,
            role: "assistant",
          };
          openaiChatMessages.push(toolMessage);

          const input: StreamEvent = {
            response: toolResponse,
            assistantName,
            isLast: true,
          };

          log.info("toolCall ", { input });

          await step<typeof functions>({}).workflowSendEvent({
            event: {
              name: streamEvent.name,
              input,
            },
            workflow: parentWorkflow,
          });
        });
      }

      return { message };
    });

    onEvent(
      toolCallEvent,
      async ({ function: toolFunction }: ToolCallEvent) => {
        log.info("toolCallEvent", { toolFunction });

        async function callERPFunction(
          toolFunction: ToolCallEvent["function"]
        ) {
          const erpStep = step<typeof functions>({
            taskQueue: "erp",
          });

          switch (toolFunction.name) {
            case "checkPrice":
              return erpStep.erpCheckPrice(
                toolFunction.input as unknown as functions.PriceInput
              );
            case "checkInventory":
              return erpStep.erpCheckInventory(
                toolFunction.input as unknown as functions.InventoryInput
              );
            case "placeOrder":
              return erpStep.erpPlaceOrder(
                toolFunction.input as unknown as functions.OrderInput
              );
            default:
              throw new Error(`Unknown function name: ${toolFunction.name}`);
          }
        }

        const toolResult = await callERPFunction(toolFunction);

        openaiChatMessages.push({
          content: JSON.stringify(toolResult),
          role: "function",
          name: toolFunction.name,
        });

        const response = await step<typeof functions>({
          taskQueue: "openai",
        }).openaiChatCompletionsStream({
          messages: openaiChatMessages,
          ...commonOpenaiOptions,
        });

        if (response?.result?.messages) {
          openaiChatMessages = response.result.messages;
        }

        return { function: toolFunction };
      }
    );

    let ended = false;
    onEvent(conversationEndEvent, async () => {
      log.info(`conversationEndEvent received`);
      ended = true;
    });

    await condition(() => ended);

    return;
  } catch (error) {
    log.error("Error in conversationWorkflow", { error });
    throw error;
  }
}

### File: ./voice/src/client.ts ###
import Restack from "@restackio/ai";

import "dotenv/config";

export const connectionOptions = {
  engineId: process.env.RESTACK_ENGINE_ID!,
  address: process.env.RESTACK_ENGINE_ADDRESS!,
  apiKey: process.env.RESTACK_ENGINE_API_KEY!,
};

export const client = new Restack(
  process.env.RESTACK_ENGINE_API_KEY ? connectionOptions : undefined
);


### File: ./voice/src/functions/websocket/listen.ts ###
import { heartbeat, currentWorkflow, log } from "@restackio/ai/function";
import { websocketConnect } from "./utils/client";
import Restack from "@restackio/ai";
import { SendWorkflowEvent } from "@restackio/ai/event";

export async function websocketListen({
  streamSid,
  events,
  address,
}: {
  streamSid: string;
  events?: {
    websocketEventName: string;
    workflowEventName: string;
    workflow?: SendWorkflowEvent["workflow"];
  }[];
  address?: string;
}) {
  return new Promise<void>(async (resolve) => {
    const ws = await websocketConnect({ address });

    const restack = new Restack();
    const workflow = currentWorkflow().workflowExecution;

    ws.on("message", (data) => {
      const message = JSON.parse(data.toString());
      if (message.streamSid === streamSid) {
        if (events) {
          events.forEach((listenEvent) => {
            if (message.event === listenEvent.websocketEventName) {
              if (message.media && message.media.track !== "inbound") {
                return;
              }

              const workflowEvent: SendWorkflowEvent = {
                event: {
                  name: listenEvent.workflowEventName,
                  input: {
                    streamSid,
                    data: message.data,
                    media: message.media,
                  },
                },
                workflow: {
                  ...workflow,
                  ...listenEvent.workflow,
                },
              };
              log.debug(`${message.event} sendWorkflowEvent`, {
                workflowEvent,
              });

              restack.sendWorkflowEvent(workflowEvent);
            }
          });
        }
        heartbeat(message.streamSid);
        if (message.event === "stop") {
          resolve();
        }
      }
    });
  });
}


### File: ./voice/src/functions/websocket/types/events.ts ###
export type WebsocketEvent = {
  streamSid: string; // For Twilio compatibility
  media?: {
    track?: "inbound" | "outbound"; // For Twilio compatibility
    trackId: string;
    payload?: string;
  };
  data?: {
    trackId: string;
    [key: string]: any;
  };
};


### File: ./voice/src/functions/websocket/types/index.ts ###
export * from "./events";


### File: ./voice/src/functions/websocket/utils/client.ts ###
import WebSocket from "ws";
import "dotenv/config";
import { FunctionFailure } from "@restackio/ai/function";

export function websocketConnect({
  address = process.env.WEBSOCKET_ADDRESS,
}: {
  address?: string;
}): Promise<WebSocket> {
  return new Promise((resolve, reject) => {
    try {
      const ws = new WebSocket(address!);

      ws.on("open", () => {
        resolve(ws);
      });

      ws.on("error", (error) => {
        reject(
          FunctionFailure.nonRetryable(
            `Error connecting to WebSocket: ${error}`
          )
        );
      });
    } catch (error) {
      reject(
        FunctionFailure.nonRetryable(`Error connecting to WebSocket: ${error}`)
      );
    }
  });
}


### File: ./voice/src/functions/websocket/send.ts ###
import { WebsocketEvent } from "./types";
import { websocketConnect } from "./utils/client";

export async function websocketSend({
  name,
  input,
  address,
}: {
  name: string;
  input: WebsocketEvent;
  address?: string;
}) {
  const ws = await websocketConnect({ address });

  const { streamSid, data, media } = input;

  const event = {
    streamSid,
    event: name,
    data,
    media,
  };

  ws.send(JSON.stringify(event));
  ws.close();
  return true;
}


### File: ./voice/src/functions/websocket/index.ts ###
export * from "./listen";
export * from "./send";


### File: ./voice/src/functions/deepgram/listen.ts ###
import { FunctionFailure, log } from "@restackio/ai/function";
import { Buffer } from "node:buffer";
import { deepgramClient } from "./utils/client";
import { PrerecordedSchema } from "@deepgram/sdk";

export async function deepgramListen({
  base64Payload,
  options = {
    model: "nova-2",
    punctuate: true,
    interim_results: true,
    endpointing: 500,
    utterance_end_ms: 2000,
  },
  twilioEncoding,
}: {
  base64Payload: string;
  options?: PrerecordedSchema;
  twilioEncoding?: boolean;
}) {
  if (!base64Payload) {
    throw FunctionFailure.nonRetryable("No audio file");
  }

  try {
    const decodedBuffer = Buffer.from(base64Payload, "base64");

    const deepgram = deepgramClient();

    const response = await deepgram.listen.prerecorded.transcribeFile(
      decodedBuffer,
      {
        ...options,
        ...(twilioEncoding && {
          encoding: "mulaw",
          sample_rate: 8000,
        }),
      }
    );

    if (response.error) {
      log.error("deepgramListen error", { error: response.error });
    }

    const result = response.result;
    const firstChannel = result?.results?.channels?.[0];

    const transcript = firstChannel?.alternatives?.[0]?.transcript;

    let language = "";
    if (options.detect_language) {
      language = firstChannel?.detected_language!;
    }

    return {
      transcript,
      language,
      result,
    };
  } catch (error) {
    throw new Error(`Deepgram TTS error ${error}`);
  }
}


### File: ./voice/src/functions/deepgram/utils/client.ts ###
import { createClient, DeepgramClient } from "@deepgram/sdk";
import "dotenv/config";

let clientDeepgram: DeepgramClient;

export function deepgramClient() {
  const apiKey = process.env.DEEPGRAM_API_KEY;
  if (!apiKey) {
    throw new Error("API key is required to create Deepgram client.");
  }

  if (!clientDeepgram) {
    clientDeepgram = createClient(apiKey);
  }
  return clientDeepgram;
}


### File: ./voice/src/functions/deepgram/index.ts ###
export * from "./listen";
export * from "./speak";


### File: ./voice/src/functions/deepgram/speak.ts ###
import { FunctionFailure, log } from "@restackio/ai/function";
import { Buffer } from "node:buffer";
import { deepgramClient } from "./utils/client";
import { SpeakSchema } from "@deepgram/sdk";

const getAudioBuffer = async (stream: ReadableStream<Uint8Array>) => {
  const reader = stream.getReader();
  const chunks = [];

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    chunks.push(value);
  }

  const dataArray = chunks.reduce(
    (acc, chunk) => Uint8Array.from([...acc, ...chunk]),
    new Uint8Array(0)
  );

  const buffer = Buffer.from(dataArray.buffer);
  return buffer;
};

export async function deepgramSpeak({
  text,
  options = {
    model: "aura-arcas-en",
  },
  twilioEncoding,
  apiKey,
}: {
  text: string;
  options?: SpeakSchema;
  twilioEncoding?: boolean;
  apiKey?: string;
}) {
  if (!text.length) {
    log.error("Text is empty");
    throw FunctionFailure.nonRetryable("Text is empty");
  }

  try {
    const deepgram = deepgramClient();
    const response = await deepgram.speak.request(
      { text },
      {
        ...options,
        ...(twilioEncoding && {
          encoding: "mulaw",
          sample_rate: 8000,
          container: "none",
        }),
      }
    );
    const stream = await response.getStream();

    if (!stream) {
      log.error("Deepgram speak stream error", { response });
      throw new Error(`Deepgram speak stream error ${response}`);
    }

    const buffer = await getAudioBuffer(stream);
    if (!buffer) {
      log.error("Deepgram audio buffer error", { stream });
      throw new Error(`Deepgram audio buffer error ${stream}`);
    }
    const base64String = buffer.toString("base64");
    log.info("deepgramSpeak: ", {
      audioLength: base64String.length,
    });
    return {
      media: {
        payload: base64String,
      },
    };
  } catch (error) {
    log.error("Deepgram TTS error", { error });
    throw new Error(`Deepgram TTS error ${error}`);
  }
}


### File: ./voice/src/functions/utils/index.ts ###
export * from "./sendEventToWorkflow";


### File: ./voice/src/functions/utils/sendEventToWorkflow.ts ###
import Restack from "@restackio/ai";
import { SendWorkflowEvent } from "@restackio/ai/event";

export async function workflowSendEvent({
  event,
  workflow,
}: SendWorkflowEvent) {
  const restack = new Restack();

  return restack.sendWorkflowEvent({
    event,
    workflow,
  });
}


### File: ./voice/src/functions/index.ts ###
export * from "./erp";
export * from "./utils";
export * from "./deepgram";
export * from "./twilio";
export * from "./websocket";
export * from "./openai";


### File: ./voice/src/functions/erp/placeOrder.ts ###
import { log, sleep } from "@restackio/ai/function";
import { z } from "zod";
import { toolInputWithQty } from "./tools";

export type OrderInput = z.infer<typeof toolInputWithQty>;

export async function erpPlaceOrder({ model, quantity = 1 }: OrderInput) {
  log.info("GPT -> called placeOrder function");
  sleep(200);
  const orderNum = Math.floor(
    Math.random() * (9999999 - 1000000 + 1) + 1000000
  );

  if (model?.toLowerCase().includes("pro")) {
    return JSON.stringify({
      orderNumber: orderNum,
      price: Math.floor(quantity * 249 * 1.079),
    });
  } else if (model?.toLowerCase().includes("max")) {
    return JSON.stringify({
      orderNumber: orderNum,
      price: Math.floor(quantity * 549 * 1.079),
    });
  }
  return JSON.stringify({
    orderNumber: orderNum,
    price: Math.floor(quantity * 179 * 1.079),
  });
}


### File: ./voice/src/functions/erp/checkPrice.ts ###
import { log, sleep } from "@restackio/ai/function";
import { z } from "zod";
import { toolInput } from "./tools";

export type PriceInput = z.infer<typeof toolInput>;

export async function erpCheckPrice({ model }: PriceInput) {
  log.info("GPT -> called checkPrice function");
  sleep(500);
  if (model?.toLowerCase().includes("pro")) {
    return JSON.stringify({ price: 249 });
  } else if (model?.toLowerCase().includes("max")) {
    return JSON.stringify({ price: 549 });
  } else {
    return JSON.stringify({ price: 149 });
  }
}


### File: ./voice/src/functions/erp/tools.ts ###
import { z } from "zod";
import { zodFunction } from "openai/helpers/zod";

export const toolInput = z.object({
  model: z
    .enum(["airpods", "airpods pro", "airpods max"])
    .describe(
      "The model of airpods, either the airpods, airpods pro or airpods max"
    ),
});

export const toolInputWithQty = z.object({
  model: z
    .enum(["airpods", "airpods pro", "airpods max"])
    .describe(
      "The model of airpods, either the airpods, airpods pro or airpods max"
    ),
  quantity: z.number().describe("The number of airpods they want to order"),
});

export async function erpGetTools() {
  return [
    zodFunction({
      name: "checkInventory",
      description:
        "Check the inventory of airpods, airpods pro or airpods max.",
      parameters: toolInput,
    }),
    zodFunction({
      name: "checkPrice",
      description:
        "Check the price of given model of airpods, airpods pro or airpods max.",
      parameters: toolInput,
    }),
    zodFunction({
      name: "placeOrder",
      description: "Places an order for a set of airpods.",
      parameters: toolInputWithQty,
    }),
  ];
}


### File: ./voice/src/functions/erp/checkInventory.ts ###
import { log, sleep } from "@restackio/ai/function";
import { z } from "zod";
import { toolInput } from "./tools";

export type InventoryInput = z.infer<typeof toolInput>;

export async function erpCheckInventory({ model }: InventoryInput) {
  log.info("GPT -> called checkInventory function");
  sleep(200);
  if (model?.toLowerCase().includes("pro")) {
    return JSON.stringify({ stock: 10 });
  } else if (model?.toLowerCase().includes("max")) {
    return JSON.stringify({ stock: 0 });
  } else {
    return JSON.stringify({ stock: 100 });
  }
}


### File: ./voice/src/functions/erp/index.ts ###
export * from "./checkInventory";
export * from "./checkPrice";
export * from "./placeOrder";
export * from "./tools";


### File: ./voice/src/functions/openai/types/events.ts ###
import OpenAI from "openai/index";

export type StreamEvent = {
  chunkId?: string;
  response: string;
  assistantName?: string;
  isLast: boolean;
};

export type ToolCallEvent =
  OpenAI.Chat.Completions.ChatCompletionChunk.Choice.Delta.ToolCall & {
    function: {
      name: string;
      input: JSON;
    };
    assistantName?: string;
  };


### File: ./voice/src/functions/openai/types/index.ts ###
export * from "./events";


### File: ./voice/src/functions/openai/chat/completionsBase.ts ###
import { FunctionFailure, log } from "@restackio/ai/function";
import OpenAI from "openai/index";
import { ChatCompletionCreateParamsNonStreaming } from "openai/resources/chat/completions";
import { openaiClient } from "../utils/client";
import { openaiCost, Price } from "../utils/cost";
import { ChatCompletion, ChatModel } from "openai/resources/index";

export type UsageOutput = { tokens: number; cost: number };

export type OpenAIChatInput = {
  userContent: string;
  systemContent?: string;
  model?: ChatModel;
  jsonSchema?: {
    name: string;
    schema: Record<string, unknown>;
  };
  price?: Price;
  apiKey?: string;
  params?: ChatCompletionCreateParamsNonStreaming;
  tools?: OpenAI.Chat.Completions.ChatCompletionTool[];
  toolChoice?: OpenAI.Chat.Completions.ChatCompletionToolChoiceOption;
};

export const openaiChatCompletionsBase = async ({
  userContent,
  systemContent = "",
  model = "gpt-4o-mini",
  jsonSchema,
  price,
  apiKey,
  params,
  tools,
  toolChoice,
}: OpenAIChatInput): Promise<{ result: ChatCompletion; cost?: number }> => {
  try {
    const openai = openaiClient({ apiKey });

    const isO1Model = model.startsWith("o1-");

    const o1ModelParams = {
      temperature: 1,
      top_p: 1,
      frequency_penalty: 0,
      presence_penalty: 0,
    };

    const chatParams: ChatCompletionCreateParamsNonStreaming = {
      messages: [
        ...(systemContent ? [{ role: "system" as const, content: systemContent }] : []),
        { role: "user" as const, content: userContent },
        ...(params?.messages ?? []),
      ],
      ...(jsonSchema && {
        response_format: {
          type: "json_schema",
          json_schema: {
            name: jsonSchema.name,
            strict: true,
            schema: jsonSchema.schema,
          },
        },
      }),
      model,
      ...(tools && { tools }),
      ...(toolChoice && { tool_choice: toolChoice }),
      ...params,
      ...(isO1Model && o1ModelParams),
    };

    log.debug("OpenAI chat completion params", {
      chatParams,
    });

    const completion = await openai.chat.completions.create(chatParams);

    return {
      result: completion,
      cost:
        price &&
        openaiCost({
          price,
          tokensCount: {
            input: completion.usage?.prompt_tokens ?? 0,
            output: completion.usage?.completion_tokens ?? 0,
          },
        }),
    };
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error OpenAI chat: ${error}`);
  }
};

### File: ./voice/src/functions/openai/chat/completionsStream.ts ###
import OpenAI from "openai/index";
import { ChatCompletionChunk } from "openai/resources/chat/completions";

import Restack from "@restackio/ai";
import { currentWorkflow, log } from "@restackio/ai/function";

import { StreamEvent, ToolCallEvent } from "../types/events";

import { aggregateStreamChunks } from "../utils/aggregateStream";
import { mergeToolCalls } from "../utils/mergeToolCalls";
import { openaiClient } from "../utils/client";
import { openaiCost, Price } from "../utils/cost";
import { SendWorkflowEvent } from "@restackio/ai/event";
import { ChatModel } from "openai/resources/index";

export async function openaiChatCompletionsStream({
  model = "gpt-4o-mini",
  userName,
  newMessage,
  assistantName,
  messages = [],
  tools,
  toolEvent,
  streamAtCharacter,
  streamEvent,
  apiKey,
  price,
}: {
  model?: ChatModel;
  userName?: string;
  newMessage?: string;
  assistantName?: string;
  messages?: OpenAI.Chat.Completions.ChatCompletionMessageParam[];
  tools?: OpenAI.Chat.Completions.ChatCompletionTool[];
  toolEvent?: {
    workflowEventName: string;
    workflow?: SendWorkflowEvent["workflow"];
  };
  streamAtCharacter?: string;
  streamEvent?: {
    workflowEventName: string;
    workflow?: SendWorkflowEvent["workflow"];
  };
  apiKey?: string;
  price?: Price;
}) {
  const restack = new Restack();
  const workflow = currentWorkflow().workflowExecution;

  log.debug("workflow", { workflow });

  if (newMessage) {
    messages.push({
      role: "user",
      name: userName,
      content: newMessage,
    });
  }

  const openai = openaiClient({ apiKey });
  const chatStream = await openai.chat.completions.create({
    model: model,
    messages,
    tools,
    stream: true,
    stream_options: {
      include_usage: true,
    },
  });

  const [stream, streamEnd] = chatStream.tee();
  const readableStream = streamEnd.toReadableStream() as unknown as ReadableStream<any>;
  const aggregatedStream = await aggregateStreamChunks(readableStream);

  let finishReason: ChatCompletionChunk.Choice["finish_reason"];
  let response: ChatCompletionChunk.Choice.Delta["content"] = "";
  let tokensCountInput = 0;
  let tokensCountOutput = 0;

  for await (const chunk of stream) {
    let content = chunk.choices[0]?.delta?.content || "";
    finishReason = chunk.choices[0]?.finish_reason;
    tokensCountInput += chunk.usage?.prompt_tokens ?? 0;
    tokensCountOutput += chunk.usage?.completion_tokens ?? 0;

    if (finishReason === "tool_calls") {
      const { toolCalls } = mergeToolCalls(aggregatedStream);
      await Promise.all(
        toolCalls.map((toolCall) => {
          if (toolEvent) {
            const functionArguments = JSON.parse(
              toolCall.function?.arguments ?? ""
            );

            const input: ToolCallEvent = {
              ...toolCall,
              function: {
                name: toolCall.function?.name ?? "",
                input: functionArguments,
              },
              assistantName,
            };

            if (toolEvent) {
              const workflowEvent = {
                event: {
                  name: toolEvent.workflowEventName,
                  input,
                },
                workflow: {
                  ...workflow,
                  ...toolEvent.workflow,
                },
              };
              log.debug("toolEvent sendWorkflowEvent", { workflowEvent });

              restack.sendWorkflowEvent(workflowEvent);
            }
          }
        })
      );
      return {
        result: {
          messages,
          toolCalls,
        },
        cost:
          price &&
          openaiCost({
            price,
            tokensCount: {
              input: tokensCountInput,
              output: tokensCountOutput,
            },
          }),
      };
    } else {
      response += content;
      if (
        content.trim().slice(-1) === streamAtCharacter ||
        finishReason === "stop"
      ) {
        if (response.length) {
          const input: StreamEvent = {
            chunkId: chunk.id,
            response,
            assistantName,
            isLast: finishReason === "stop",
          };
          if (streamEvent) {
            const workflowEvent = {
              event: {
                name: streamEvent.workflowEventName,
                input,
              },
              workflow: {
                ...workflow,
                ...streamEvent.workflow,
              },
            };
            log.debug("streamEvent sendWorkflowEvent", { workflowEvent });
            restack.sendWorkflowEvent(workflowEvent);
          }
        }
      }

      if (finishReason === "stop") {
        const newMessage: OpenAI.Chat.Completions.ChatCompletionMessageParam = {
          content: response,
          role: "assistant",
          name: assistantName,
        };

        messages.push(newMessage);

        return {
          result: {
            messages,
          },
          cost:
            price &&
            openaiCost({
              price,
              tokensCount: {
                input: tokensCountInput,
                output: tokensCountOutput,
              },
            }),
        };
      }
    }
  }
}


### File: ./voice/src/functions/openai/chat/index.ts ###
export * from "./completionsBase";
export * from "./completionsStream";


### File: ./voice/src/functions/openai/prompt.ts ###
import OpenAI from "openai";

export const agentPrompt: OpenAI.Chat.Completions.ChatCompletionMessageParam[] =
  [
    {
      role: "system",
      content: `You are an outbound sales representative selling Apple Airpods.
      You have a youthful and cheery personality.
      Keep your responses as brief as possible but make every attempt to keep the caller on the phone without being rude.
      Don't ask more than 1 question at a time.
      Don't make assumptions about what values to plug into functions.
      Ask for clarification if a user request is ambiguous.
      Speak out all prices to include the currency.
      Please help them decide between airpods airpods pro and airpods max by asking questions like 'Do you prefer headphones that go in your ear or over the ear?'.
      If they are trying to choose between the airpods and airpods pro try asking them if they need noise canceling.
      Once you know which model they would like ask them how many they would like to purchase and try to get them to place an order.
      This reponse is used by text to speech, make it as natural as possible by using filler words like 'um' and 'uh' when necessary.
      A comma (,) or a period (.) present in your text will be treated as a very short pause.
      If you need to insert a longer pause in your audio, use the ellipsis: ...
      You must add a '•' symbol every 5 to 10 words at natural pauses where your response can be split for text to speech.
      `,
    },
  ];


### File: ./voice/src/functions/openai/utils/cost.ts ###
export type TokensCount = {
  input: number;
  output: number;
};

export type Price = {
  input: number;
  output: number;
};
export const openaiCost = ({
  tokensCount,
  price,
}: {
  tokensCount: TokensCount;
  price: Price;
}): number => {
  let cost = 0;
  const { input: inputTokens, output: outputTokens } = tokensCount;
  const { input: inputPrice, output: outputPrice } = price;

  cost = inputTokens * inputPrice + outputTokens * outputPrice;

  return cost;
};


### File: ./voice/src/functions/openai/utils/aggregateStream.ts ###
export async function aggregateStreamChunks(stream: ReadableStream) {
  const reader = stream.getReader();
  const chunks: Uint8Array[] = [];

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    if (value) chunks.push(value);
  }

  const aggregated = new Uint8Array(
    chunks.reduce((acc, chunk) => acc + chunk.length, 0)
  );
  let offset = 0;
  for (const chunk of chunks) {
    aggregated.set(chunk, offset);
    offset += chunk.length;
  }

  const textContent = new TextDecoder().decode(aggregated);
  const jsonObjects = textContent
    .split("\n")
    .filter((line) => line.trim())
    .map((line) => JSON.parse(line));
  return jsonObjects;
}


### File: ./voice/src/functions/openai/utils/client.ts ###
import OpenAI from "openai/index";
import "dotenv/config";

let openaiInstance: OpenAI | null = null;

export const openaiClient = ({
  apiKey = process.env.OPENAI_API_KEY,
}: {
  apiKey?: string;
}): OpenAI => {
  if (!apiKey) {
    throw new Error("API key is required to create OpenAI client.");
  }

  if (!openaiInstance) {
    openaiInstance = new OpenAI({
      apiKey,
    });
  }
  return openaiInstance;
};


### File: ./voice/src/functions/openai/utils/index.ts ###
export * from "./aggregateStream";
export * from "./client";
export * from "./cost";
export * from "./mergeToolCalls";


### File: ./voice/src/functions/openai/utils/mergeToolCalls.ts ###
import OpenAI from "openai/index";
import { ChatCompletionChunk } from "openai/resources/chat/completions.mjs";

export function mergeToolCalls(aggregatedStream: ChatCompletionChunk[]) {
  const toolCalls: OpenAI.Chat.Completions.ChatCompletionChunk.Choice.Delta.ToolCall[] =
    [];

  aggregatedStream.forEach((chunk) => {
    chunk.choices.forEach((choice) => {
      if (choice.delta.tool_calls) {
        choice.delta.tool_calls.forEach((toolCall) => {
          const lastToolCall = toolCalls[toolCalls.length - 1];
          if (toolCall.id) {
            toolCalls.push({
              ...toolCall,
              function: { ...toolCall.function, arguments: "" },
            });
          } else if (
            lastToolCall &&
            lastToolCall.function &&
            toolCall.function?.arguments
          ) {
            lastToolCall.function.arguments += toolCall.function.arguments;
          }
        });
      }
    });
  });

  return { toolCalls };
}


### File: ./voice/src/functions/openai/index.ts ###
export * from "./chat";
export * from "./thread";


### File: ./voice/src/functions/openai/thread/createMessageOnThread.ts ###
import OpenAI from "openai/index";
import { FunctionFailure } from "@restackio/ai/function";

import { openaiClient } from "../utils/client";

export async function createMessageOnThread({
  apiKey,
  threadId,
  content,
  role,
}: {
  apiKey: string;
  threadId: string;
  content: string;
  role: OpenAI.Beta.Threads.MessageCreateParams["role"];
}) {
  try {
    const openai = openaiClient({ apiKey });
    await openai.beta.threads.messages.create(threadId, {
      role,
      content,
    });
  } catch (error) {
    throw FunctionFailure.nonRetryable(
      `Error creating message thread: ${error}`
    );
  }
}


### File: ./voice/src/functions/openai/thread/runThread.ts ###
import { FunctionFailure } from "@restackio/ai/function";
import { Stream } from "openai/streaming";
import { AssistantStreamEvent } from "openai/resources/beta/index";
import { Run } from "openai/resources/beta/threads/runs/index";

import { openaiClient } from "../utils/client";

export async function runThread({
  apiKey,
  threadId,
  assistantId,
  stream = false,
}: {
  apiKey: string;
  threadId: string;
  assistantId: string;
  stream: boolean;
}): Promise<Stream<AssistantStreamEvent> | Run> {
  try {
    const openai = openaiClient({ apiKey });

    const run = await openai.beta.threads.runs.create(threadId, {
      assistant_id: assistantId,
      ...(stream && { stream }),
    });

    return run;
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error running thread: ${error}`);
  }
}

### File: ./voice/src/functions/openai/thread/createAssistant.ts ###
import { ChatModel } from "openai/resources/index";
import { FunctionFailure } from "@restackio/ai/function";
import { Assistant, AssistantTool } from "openai/resources/beta/index";

import { openaiClient } from "../utils/client";

export async function createAssistant({
  apiKey,
  name,
  instructions,
  model = "gpt-4o-mini",
  tools = [],
}: {
  apiKey: string;
  name: string;
  instructions: string;
  tools?: AssistantTool[];
  model: ChatModel;
}): Promise<Assistant> {
  try {
    const openai = openaiClient({ apiKey });

    const assistant = await openai.beta.assistants.create({
      name,
      instructions,
      model,
      tools,
    });

    return assistant;
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error creating assistant: ${error}`);
  }
}


### File: ./voice/src/functions/openai/thread/index.ts ###
export * from "./createAssistant";
export * from "./createMessageOnThread";
export * from "./createThread";
export * from "./runThread";


### File: ./voice/src/functions/openai/thread/createThread.ts ###
import { FunctionFailure } from "@restackio/ai/function";
import { Thread } from "openai/resources/beta/index";

import { openaiClient } from "../utils/client";

export async function createThread({
  apiKey,
}: {
  apiKey: string;
}): Promise<Thread> {
  try {
    const openai = openaiClient({ apiKey });
    const thread = await openai.beta.threads.create();

    return thread;
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error creating thread: ${error}`);
  }
}


### File: ./voice/src/functions/twilio/call.ts ###
import { FunctionFailure } from "@restackio/ai/function";
import { CallListInstanceCreateOptions } from "twilio/lib/rest/api/v2010/account/call";
import { twilioClient } from "./utils/client";

interface Output {
  sid: string;
}

export async function twilioCall({
  accountSid,
  authToken,
  options,
}: {
  accountSid?: string;
  authToken?: string;
  options: CallListInstanceCreateOptions;
}): Promise<Output> {
  const client = twilioClient({ accountSid, authToken });

  if (!accountSid || !authToken) {
    throw FunctionFailure.nonRetryable("Twilio credentials are missing");
  }

  try {
    if (options.to && options.from && options.url) {
      const { sid } = await client.calls.create(options);
      return { sid };
    } else {
      throw FunctionFailure.nonRetryable(`No to, from or url`);
    }
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error Twilio call create: ${error}`);
  }
}


### File: ./voice/src/functions/twilio/utils/client.ts ###
import twilio from "twilio/index";
import "dotenv/config";

let clientTwilio: twilio.Twilio;

export function twilioClient({
  accountSid = process.env.TWILIO_ACCOUNT_SID,
  authToken = process.env.TWILIO_AUTH_TOKEN,
}: {
  accountSid?: string;
  authToken?: string;
}) {
  if (!accountSid || !authToken) {
    throw new Error(
      "Account SID and auth token are required to create Twilio client."
    );
  }

  if (!clientTwilio) {
    clientTwilio = twilio(accountSid, authToken);
  }
  return clientTwilio;
}


### File: ./voice/src/functions/twilio/index.ts ###
export * from "./call";


### File: ./voice/src/services.ts ###
import {
  workflowSendEvent,
  erpGetTools,
  erpPlaceOrder,
  erpCheckInventory,
  erpCheckPrice,
  websocketSend,
  websocketListen,
  twilioCall,
  openaiChatCompletionsBase,
  openaiChatCompletionsStream,
  deepgramListen,
  deepgramSpeak,
} from "./functions";
import { client } from "./client";

export async function services() {
  const workflowsPath = require.resolve("./workflows");

  try {
    await Promise.all([
      client.startService({
        workflowsPath,
        functions: { workflowSendEvent },
      }),
      client.startService({
        taskQueue: "erp",
        functions: {
          erpGetTools,
          erpCheckPrice,
          erpCheckInventory,
          erpPlaceOrder,
        },
      }),
      client.startService({
        taskQueue: "websocket",
        functions: {
          websocketSend,
          websocketListen,
        },
      }),
      client.startService({
        taskQueue: "twilio",
        functions: {
          twilioCall,
        },
      }),
      client.startService({
        taskQueue: "openai",
        functions: {
          openaiChatCompletionsBase,
          openaiChatCompletionsStream,
        },
      }),
      client.startService({
        taskQueue: "deepgram",
        functions: {
          deepgramListen,
          deepgramSpeak,
        },
      }),

    ]);

    console.log("Services running successfully.");
  } catch (e) {
    console.error("Failed to run worker", e);
  }
}

services().catch((err) => {
  console.error("Error in services:", err);
});


### File: ./voice/src/server.ts ###
import "dotenv/config";
import express from "express";
import { createServer } from "http";
import WebSocket, { WebSocketServer } from "ws";
import VoiceResponse from "twilio/lib/twiml/VoiceResponse";
import Restack from "@restackio/ai";
import { roomWorkflow } from "./workflows/room/room";
import { RoomInfo, streamInfoEvent } from "./workflows/room/events";
import cors from "cors";

const app = express();
const server = createServer(app);
const wss = new WebSocketServer({ server });
const PORT = process.env.PORT || 4000;
export const websocketAddress = `ws://${
  process.env.SERVER_HOST ? process.env.SERVER_HOST : `localhost:${PORT}`
}/connection`;

app.use(cors());
app.use(express.json());

app.post("/start", async (req, res) => {
  try {
    const restack = new Restack();

    const workflowId = `${Date.now()}-${roomWorkflow.name}`;

    const workflowRunId = await restack.scheduleWorkflow({
      workflowName: roomWorkflow.name,
      workflowId,
      input: { address: websocketAddress },
    });
    if (workflowRunId) {
      try {
        restack.sendWorkflowEvent({
          event: {
            name: streamInfoEvent.name,
            input: { streamSid: workflowRunId },
          },
          workflow: {
            workflowId,
            runId: workflowRunId,
          },
        });
      } catch (error) {
        console.log("update error", error);
      }

      res.status(200).send({ streamSid: workflowRunId, websocketAddress });
    } else {
      console.log("error");
      throw new Error("Could not start session.");
    }
  } catch (error) {
    console.error("Error scheduling workflow:", error);
    res.status(500).send({ error: "Failed to schedule workflow" });
  }
});

app.post("/incoming", async (req, res) => {
  try {
    const workflowId = `${Date.now()}-${roomWorkflow.name}`;
    const restack = new Restack();
    const runId = await restack.scheduleWorkflow({
      workflowName: roomWorkflow.name,
      workflowId,
      input: { address: websocketAddress },
    });

    console.log(`Started workflow with runId: ${runId}`);

    if (runId) {
      const response = new VoiceResponse();
      const connect = response.connect();
      const stream = connect.stream({ url: `${websocketAddress}` });

      stream.parameter({ name: "runId", value: runId });
      stream.parameter({ name: "workflowId", value: workflowId });
      res.type("text/xml");
      res.end(response.toString());
    } else {
      throw new Error("Failed to get runId from workflow handle");
    }
  } catch (err) {
    console.log(err);
  }
});

wss.on("connection", (ws) => {
  const restack = new Restack();

  let workflowId: string;
  let runId: string;

  ws.on("error", console.error);

  ws.on("message", async function message(data, isBinary) {
    // allows broadcast to all clients except this one (otherwise echo)

    wss.clients.forEach(function each(client) {
      if (client !== ws && client.readyState === WebSocket.OPEN) {
        client.send(data, { binary: isBinary });
      }
    });

    const message = JSON.parse(data.toString());
    const streamSid = message.streamSid;

    if (message.event === "start") {
      console.log(`Starting stream ${streamSid}`);
      runId = message.start.customParameters.runId;
      workflowId = message.start.customParameters.workflowId;
      if (runId) {
        try {
          if (streamSid) {
            const input: RoomInfo = { streamSid };
            await restack.sendWorkflowEvent({
              event: {
                name: streamInfoEvent.name,
                input,
              },
              workflow: {
                workflowId,
                runId,
              },
            });
            console.log(
              `Sent workflow ${workflowId} runId ${runId} streamSid: ${streamSid}`
            );
          }
        } catch (error) {
          console.log("Error signaling workflow", error);
        }
      }
    }

    if (message.event === "stop") {
      console.log(`Websocket stream ${streamSid} ended.`);
    }
  });
});

// function shutdown() {
//   wss.close(() => {
//     server.close(() => {
//       process.exit(0);
//     });
//   });
// }

// // Listen for termination signals
// process.on("SIGTERM", shutdown);
// process.on("SIGINT", shutdown);

server.listen(PORT, () => {
  console.log(`Server running on port ${PORT}`);
  console.log(`WebSocket address: ${websocketAddress}`);
});


### File: ./nextjs/tailwind.config.ts ###
import type { Config } from "tailwindcss";

const config: Config = {
  content: [
    "./src/pages/**/*.{js,ts,jsx,tsx,mdx}",
    "./src/components/**/*.{js,ts,jsx,tsx,mdx}",
    "./src/app/**/*.{js,ts,jsx,tsx,mdx}",
  ],
  theme: {
    extend: {
      colors: {
        background: "var(--background)",
        foreground: "var(--foreground)",
      },
    },
  },
  plugins: [],
};
export default config;


### File: ./nextjs/src/app/actions/trigger.ts ###
"use server";
import Restack from "@restackio/ai";
import { Example } from "../components/examplesList";

const connectionOptions = {
  engineId: process.env.RESTACK_ENGINE_ID!,
  address: process.env.RESTACK_ENGINE_ADDRESS!,
  apiKey: process.env.RESTACK_ENGINE_API_KEY!,
};

const client = new Restack(
  process.env.RESTACK_ENGINE_API_KEY ? connectionOptions : undefined
);

export async function triggerWorkflow(
  workflowName: Example["workflowName"],
  input: Example["input"]
) {
  if (!workflowName || !input) {
    throw new Error("Workflow name and input are required");
  }

  const workflowId = `${Date.now()}-${workflowName.toString()}`;

  const runId = await client.scheduleWorkflow({
    workflowName: workflowName as string,
    workflowId,
    input,
  });

  const result = await client.getWorkflowResult({
    workflowId,
    runId,
  });

  return result;
}


### File: ./child-workflows/scheduleWorkflow.ts ###
import { client } from "./src/client";

export type InputSchedule = {
  name: string;
};

async function scheduleWorkflow(input: InputSchedule) {
  try {
    const workflowId = `${Date.now()}-parentWorkflow`;
    const runId = await client.scheduleWorkflow({
      workflowName: "parentWorkflow",
      workflowId,
      input,
    });

    const result = await client.getWorkflowResult({ workflowId, runId });

    console.log("Workflow result:", result);

    process.exit(0); // Exit the process successfully
  } catch (error) {
    console.error("Error scheduling workflow:", error);
    process.exit(1); // Exit the process with an error code
  }
}

scheduleWorkflow({
  name: "test",
});


### File: ./child-workflows/src/workflows/child.ts ###
import zodToJsonSchema from "zod-to-json-schema";
import { z } from "zod";
import { log, step } from "@restackio/ai/workflow";
import * as functions from "../functions";

interface Input {
  name: string;
}
const GoodbyeMessageSchema = z.object({
  message: z.string().describe("The goodbye message."),
});

const goodbyeJsonSchema = {
  name: "goodbye",
  schema: zodToJsonSchema(GoodbyeMessageSchema),
};

export async function childWorkflow({ name }: Input) {
  // Step 1: Create hello message with simple function
  const { message: greetMessage } = await step<typeof functions>({}).hello({
    name,
  });

  log.info("Hello", { greetMessage });

  return {
    messages: { greetMessage },
  };
}


### File: ./child-workflows/src/workflows/parent.ts ###
import { startChild, executeChild } from "@restackio/ai/workflow";
import { childWorkflow } from "./child";

interface Input {
  name: string;
}
export async function parentWorkflow({ name }: Input) {


  const startedChild = await startChild(childWorkflow, {
    workflowId: `startChild-workflow`,
    args: [{ name }],
  });

  const executedChild = await executeChild(childWorkflow, {
    workflowId: `executeChild-workflow`,
    args: [{ name }],
  });

  return {
    messages: { startedChild, executedChild },
  };
}


### File: ./child-workflows/src/workflows/index.ts ###
export * from "./parent";
export * from "./child";

### File: ./child-workflows/src/client.ts ###
import Restack from "@restackio/ai";

import "dotenv/config";

export const connectionOptions = {
  engineId: process.env.RESTACK_ENGINE_ID!,
  address: process.env.RESTACK_ENGINE_ADDRESS!,
  apiKey: process.env.RESTACK_ENGINE_API_KEY!,
};

export const client = new Restack(
  process.env.RESTACK_ENGINE_API_KEY ? connectionOptions : undefined
);


### File: ./child-workflows/src/functions/index.ts ###
export * from "./hello";


### File: ./child-workflows/src/functions/hello.ts ###
interface Input {
  name: string;
}

interface Output {
  message: string;
}

export async function hello(input: Input): Promise<Output> {
  return { message: `Hello, ${input.name}!` };
}


### File: ./child-workflows/src/services.ts ###
import { hello } from "./functions";
import { client } from "./client";

async function services() {
  const workflowsPath = require.resolve("./workflows");
  try {
    await Promise.all([
      // Start service with current workflows and functions
      client.startService({
        workflowsPath,
        functions: { hello },
      }),
    ]);

    console.log("Services running successfully.");
  } catch (e) {
    console.error("Failed to run services", e);
  }
}

services().catch((err) => {
  console.error("Error running services:", err);
});


### File: ./posthog/scheduleWorkflow.ts ###
import "dotenv/config";
import { client } from "./src/client";

async function scheduleWorkflow() {
  try {
    const workflowRunId = await client.scheduleWorkflow({
      workflowName: "digestWorkflow",
      workflowId: `${Date.now()}-digestWorkflow`,
      input: {
        projectId: process.env.POSTHOG_PROJECT_ID,
        host: process.env.POSTHOG_HOST,
        maxRecordings: 2, // Useful to limit cost when debugging locally. Comment out to run for all recordings
        maxChunksPerRecordingBlob: 2, // Useful to limit cost when debugging locally. Comment out to process all chunks per recording
        linearTeamId: process.env.LINEAR_TEAM_ID,
      },
      // Uncomment to schedule workflow to run at a specific time
      // schedule: {
      //   calendars: [
      //     {
      //       dayOfWeek: "*",
      //       hour: 17, // Everyday at 5pm UTC = 10am PST
      //     },
      //   ],
      // },
    });

    console.log("Workflow scheduled successfully:", workflowRunId);

    process.exit(0); // Exit the process successfully
  } catch (error) {
    console.error("Error scheduling workflow:", error);
    process.exit(1); // Exit the process with an error code
  }
}

scheduleWorkflow();


### File: ./posthog/src/workflows/chunk.ts ###
import { log, step } from "@restackio/ai/workflow";
import * as functions from "../functions";
import z from "zod";
import zodToJsonSchema from "zod-to-json-schema";

import {
  chunkSummaryEvent,
  ChunkSummaryEvent,
  summaryEndEvent,
} from "./recording";

const chunkSummarySchema = z.object({
  recordingId: z.string().describe("The id of the recording."),
  fromTimestamp: z
    .string()
    .describe("The start timestamp of this recording chunk."),
  toTimestamp: z
    .string()
    .describe("The end timestamp of this recording chunk."),
  summary: z.string().describe("The summary of this recording chunk."),
});

export type ChunkSummary = z.infer<typeof chunkSummarySchema>;

export async function chunkWorkflow({
  recordingId,
  chunk,
  workflow,
  isLastChunk,
}: {
  recordingId: string;
  chunk: string;
  workflow: {
    workflowId: string;
    runId: string;
  };
  isLastChunk?: boolean;
}) {
  const chunkSummaryJsonSchema = {
    name: "chunkSummary",
    schema: zodToJsonSchema(chunkSummarySchema),
  };

  const { cost, result } = await step<typeof functions>({
    taskQueue: "openai",
  }).openaiChatCompletionsBase({
    systemContent:
      "You are a helpful assistant that summarizes posthog recordings. Here is the snapshot blob of it",
    model: "gpt-4o-mini",
    userContent: `
      Here is a chunk of the recording blob:
      ${chunk}
      For the particular extract the behavior of the user and summarize it.
      Highlight if the user is doing something interesting or unexpected.
      If nothing interesting, just write "No interesting behavior".
    `,
    jsonSchema: chunkSummaryJsonSchema,
    price: {
      input: 0.00000015,
      output: 0.0000006,
    },
  });

  const summaryResult = result.choices[0].message.content;

  if (!summaryResult) {
    throw new Error("No summary result");
  }

  const summary = JSON.parse(summaryResult) as ChunkSummary;

  const input: ChunkSummaryEvent = {
    recordingId,
    summary,
    cost: cost ?? 0,
  };

  try {
    await step<typeof functions>({
      taskQueue: "restack",
    }).workflowSendEvent({
      event: {
        name: chunkSummaryEvent.name,
        input,
      },
      workflow,
    });

    if (isLastChunk) {
      await step<typeof functions>({
        taskQueue: "restack",
      }).workflowSendEvent({
        event: {
          name: summaryEndEvent.name,
        },
        workflow,
      });
    }
  } catch (error) {
    log.error("Encountered exception. ", { error });
    throw error;
  }

  return input;
}


### File: ./posthog/src/workflows/recording.ts ###
import { defineEvent, onEvent } from "@restackio/ai/event";
import * as functions from "../functions";
import { condition, log, step } from "@restackio/ai/workflow";
import z from "zod";
import { ChunkSummary } from "./chunk";
import zodToJsonSchema from "zod-to-json-schema";

export type ChunkSummaryEvent = {
  recordingId: string;
  summary: ChunkSummary;
  cost: number;
};

export const chunkSummaryEvent = defineEvent<ChunkSummaryEvent>("chunkSummary");

export const summaryEndEvent = defineEvent("summaryEnd");

export async function recordingWorkflow({
  recordingId,
  blobKeys,
  projectId,
  host,
  maxChunks,
}: {
  recordingId: string;
  blobKeys: string[];
  projectId: string;
  host: string;
  maxChunks?: number;
}) {
  const events = await step<typeof functions>({
    taskQueue: "posthog",
  }).posthogSessionEvents({
    recordingId,
    projectId,
    host,
  });

  let summaries: ChunkSummaryEvent[] = [];
  let totalCost = 0;

  // Get chunks and create unique workflow for each and stream back event summary

  await step<typeof functions>({
    taskQueue: "posthog",
  }).posthogBlobChunks({
    recordingId,
    blobKeys,
    projectId,
    host,
    maxChunks,
  });

  // Push summary to check

  onEvent(
    chunkSummaryEvent,
    async ({ recordingId, summary, cost }: ChunkSummaryEvent) => {
      summaries.push({
        recordingId,
        summary,
        cost,
      });
      totalCost += cost;
      return {
        recordingId,
        summary,
        cost,
      };
    }
  );

  let ended = false;

  // When all summaries are done, continue.

  onEvent(summaryEndEvent, async () => {
    log.info(`summaryEnd received`);
    ended = true;
  });

  await condition(() => ended);

  // Create a summary from all the chunks summaries

  const recordingSummarySchema = z.object({
    recordingId: z.string().describe("The id of the recording."),
    summary: z.string().describe("The summary of the recording."),
    highlights: z
      .array(
        z.object({
          name: z.string().describe("The name of the behavior highlight."),
          timestamp: z
            .string()
            .describe("The timestamp of the behavior highlight."),
        })
      )
      .describe("Noticeable user behavior to be highlighted."),
  });

  type RecordingSummary = z.infer<typeof recordingSummarySchema>;

  const recordingSummaryJsonSchema = {
    name: "recordingSummary",
    schema: zodToJsonSchema(recordingSummarySchema),
  };

  const { cost, result } = await step<typeof functions>({
    taskQueue: "openai",
  }).openaiChatCompletionsBase({
    systemContent:
      "You are a helpful assistant that summarizes posthog recordings.",
    model: "gpt-4o-mini",
    userContent: `
      Here are summaries of each chunk of the recording blob:
      ${summaries}
      The users events are ${JSON.stringify(events)}
      Group all this data, make a summary and highlight particular interesting or unexpected user behavior.
    `,
    jsonSchema: recordingSummaryJsonSchema,
    price: {
      input: 0.00000015,
      output: 0.0000006,
    },
  });

  totalCost += cost ?? 0;

  const summaryResult = result.choices[0].message.content;

  if (!summaryResult) {
    throw new Error("No summary result");
  }

  const summary = JSON.parse(summaryResult) as RecordingSummary;

  return {
    summary,
    totalCost,
    events,
    summaries,
  };
}


### File: ./posthog/src/workflows/index.ts ###
export * from "./digest";
export * from "./recording";
export * from "./chunk";


### File: ./posthog/src/workflows/digest.ts ###
import { executeChild, step } from "@restackio/ai/workflow";
import { recordingWorkflow } from "./recording";
import * as functions from "../functions";

export async function digestWorkflow({
  projectId,
  host,
  maxRecordings,
  maxChunksPerRecordingBlob,
  linearTeamId,
}: {
  projectId: string;
  host: string;
  maxRecordings?: number;
  maxChunksPerRecordingBlob?: number;
  linearTeamId?: string;
}) {
  let totalCost = 0;
  // Get last 24h recordings
  const { results: recordings } = await step<typeof functions>({
    taskQueue: "posthog",
  }).posthogGetRecordings({ projectId, host });

  let summaries: Awaited<ReturnType<typeof recordingWorkflow>>[] = [];

  await Promise.all(
    recordings.slice(0, maxRecordings).map(async (recording) => {
      // Get snapshots for recording
      const { blobKeys } = await step<typeof functions>({
        taskQueue: "posthog",
      }).posthogGetSnapshots({
        recordingId: recording.id,
        projectId,
        host,
      });

      if (blobKeys.length > 0) {
        // Get recording summary
        const recordingData = await executeChild(recordingWorkflow, {
          workflowId: `${recording.id}-recordingWorkflow`,
          args: [
            {
              recordingId: recording.id,
              blobKeys,
              projectId,
              host,
              maxChunks: maxChunksPerRecordingBlob,
            },
          ],
        });
        totalCost += recordingData.totalCost;
        summaries.push(recordingData);
      }
    })
  );

  // Create a digest from all the chunks summaries

  const { cost, result } = await step<typeof functions>({
    taskQueue: `openai-beta`,
  }).openaiChatCompletionsBase({
    model: "o1-preview",
    userContent: `
      Summarize the following PostHog recordings analysis into a linear issue in markdown format, following the structure:

        •	10-second overview: Briefly mention the most urgent and critical user behavior or anomalies.
        •	30-second summary: Highlight key user interactions and events, such as network requests, page views, and modal interactions. Focus on patterns or significant moments.
        •	1-minute deep dive: Provide a more detailed breakdown of the user’s behavior, including network activity, UI interactions, and any potential anomalies. Include timestamps or specific event details when relevant.
      Finally, end the issue description with a brief call to action or recommendation.
      Include the url to the recordings when necessary so user can easily access them.
      To make the recording url replace the RECORDING_ID in: ${host}/project/${projectId}/replay/RECORDING_ID

      Here is the data:
      ${JSON.stringify(summaries)}
    `,
    price: {
      input: 0.000015,
      output: 0.00006,
    },
  });

  totalCost += cost ?? 0;

  const digest = result.choices[0].message.content;

  if (linearTeamId) {
    const linearResult = await step<typeof functions>({
      taskQueue: `linear`,
    }).linearCreateIssue({
      issue: {
        teamId: linearTeamId,
        title: `PostHog Digest - ${new Date().toISOString()}`,
        description: digest,
      },
    });

    return {
      digest,
      totalCost,
      summaries,
      linearResult,
    };
  }

  return {
    digest,
    totalCost,
    summaries,
  };
}


### File: ./posthog/src/client.ts ###
import Restack from "@restackio/ai";

import "dotenv/config";

export const connectionOptions = {
  engineId: process.env.RESTACK_ENGINE_ID!,
  address: process.env.RESTACK_ENGINE_ADDRESS!,
  apiKey: process.env.RESTACK_ENGINE_API_KEY!,
};

export const client = new Restack(
  process.env.RESTACK_ENGINE_API_KEY ? connectionOptions : undefined
);


### File: ./posthog/src/functions/linear/utils/client.ts ###
import { LinearClient } from "@linear/sdk";
import "dotenv/config";

let clientLinear: LinearClient;

export function linearClient({
  apiKey = process.env.LINEAR_API_KEY,
}: {
  apiKey?: string;
}) {
  if (!apiKey) {
    throw new Error("API key is required to create Linear client.");
  }
  if (!clientLinear) {
    clientLinear = new LinearClient({ apiKey });
  }
  return clientLinear;
}


### File: ./posthog/src/functions/linear/index.ts ###
export * from "./createComment";
export * from "./createIssue";


### File: ./posthog/src/functions/linear/createIssue.ts ###
import { FunctionFailure, log } from "@restackio/ai/function";
import { IssueCreateInput } from "@linear/sdk/dist/_generated_documents";
import { linearClient } from "./utils/client";

export async function linearCreateIssue({
  issue,
  apiKey,
}: {
  issue: IssueCreateInput;
  apiKey?: string;
}) {
  if (!issue) {
    throw FunctionFailure.nonRetryable("No issue");
  }

  try {
    const linear = linearClient({ apiKey });

    const result = await linear.createIssue(issue);

    log.debug("result", { result });

    return result;
  } catch (error) {
    throw new Error(`Fal error ${error}`);
  }
}


### File: ./posthog/src/functions/linear/createComment.ts ###
import { FunctionFailure, log } from "@restackio/ai/function";
import { CommentCreateInput } from "@linear/sdk/dist/_generated_documents";
import { linearClient } from "./utils/client";

export async function linearCreateComment({
  comment,
  apiKey,
}: {
  comment: CommentCreateInput;
  apiKey?: string;
}) {
  if (!comment) {
    throw FunctionFailure.nonRetryable("No comment");
  }

  try {
    const linear = linearClient({ apiKey });

    const result = await linear.createComment(comment);

    log.debug("result", { result });

    return result;
  } catch (error) {
    throw new Error(`Linear error ${error}`);
  }
}


### File: ./posthog/src/functions/utils/index.ts ###
export * from "./sendEventToWorkflow";


### File: ./posthog/src/functions/utils/sendEventToWorkflow.ts ###
import Restack from "@restackio/ai";
import { SendWorkflowEvent } from "@restackio/ai/event";

export async function workflowSendEvent({
  event,
  workflow,
}: SendWorkflowEvent) {
  const restack = new Restack();

  return restack.sendWorkflowEvent({
    event,
    workflow,
  });
}


### File: ./posthog/src/functions/posthog/index.ts ###
export * from "./sessionRecording/snapshots";
export * from "./sessionRecording/recordings";
export * from "./sessionRecording/snapshotBlob";
export * from "./sessionRecording/blobChunks";
export * from "./sessionRecording/queryEvents";


### File: ./posthog/src/functions/posthog/sessionRecording/blobChunks.ts ###
import { functionInfo, log } from "@restackio/ai/function";
import "dotenv/config";
import { posthogGetSnapshotBlob } from "./snapshotBlob";
import Restack from "@restackio/ai";
import { chunkWorkflow } from "../../../workflows/chunk";

export async function posthogBlobChunks({
  recordingId,
  blobKeys,
  projectId,
  host,
  maxChunks = Infinity,
}: {
  recordingId: string;
  blobKeys: string[];
  projectId: string;
  host: string;
  maxChunks?: number;
}) {
  const { workflowExecution } = functionInfo();

  try {
    let recordingBlobs: string[] = [];
    const restack = new Restack();
    await Promise.all(
      blobKeys.map(async (blobKey) => {
        const recordingBlob = await posthogGetSnapshotBlob({
          recordingId,
          blobKey: blobKey,
          projectId,
          host,
        });
        recordingBlobs.push(recordingBlob);
      })
    );

    if (recordingBlobs.length === 0) {
      throw new Error("No recording blobs");
    }

    const maxCharacters = 20000;

    log.info("Recording blobs", {
      length: recordingBlobs.length,
      chunks: Math.ceil(recordingBlobs[0].length / maxCharacters),
    });

    const schedulePromises = recordingBlobs.map((blob, i) => {
      if (blob.length > maxCharacters) {
        const chunks = Math.min(
          Math.ceil(blob.length / maxCharacters),
          maxChunks
        ); // Limit chunks
        return Promise.all(
          Array.from({ length: chunks }, (_, j) => {
            const chunk = blob.slice(
              j * maxCharacters,
              (j + 1) * maxCharacters
            );
            return restack.scheduleWorkflow({
              workflowName: chunkWorkflow.name,
              workflowId: `${recordingId}-${i}-${j}-chunkWorkflow`,
              input: {
                recordingId,
                chunk,
                workflow: workflowExecution,
                isLastChunk: j === chunks - 1,
              },
            });
          })
        );
      } else {
        return restack.scheduleWorkflow({
          workflowName: chunkWorkflow.name,
          workflowId: `${recordingId}-${i}-0-chunkWorkflow`,
          input: {
            recordingId,
            chunk: JSON.stringify({
              recordingBlobs: [blob],
            }),
            workflow: workflowExecution,
            isLastChunk: true,
          },
        });
      }
    });

    await Promise.all(schedulePromises);

    return {
      recordingId,
      blobLength: recordingBlobs.length,
      blobchunks: Math.ceil(recordingBlobs[0].length / maxCharacters),
    };
  } catch (error) {
    log.error("Encountered exception. ", { error });
    throw error;
  }
}


### File: ./posthog/src/functions/posthog/sessionRecording/queryEvents.ts ###
export type PosthogEvent = {
  uuid: string;
  event: string;
  timestamp: string;
  elements_chain: string;
  window_id?: string;
  current_url?: string;
  event_type?: string;
};

export async function posthogSessionEvents({
  recordingId,
  projectId,
  host,
  apiKey = process.env.POSTHOG_API_KEY,
}: {
  recordingId: string;
  projectId: string;
  host: string;
  apiKey?: string;
}): Promise<Event[]> {
  const url = `${host}/api/projects/${projectId}/query/`;
  const headers = {
    "content-type": "application/json",
    Authorization: `Bearer ${apiKey}`,
  };

  const query = {
    kind: "EventsQuery",
    select: [
      "uuid",
      "event",
      "timestamp",
      "elements_chain",
      "properties.$window_id",
      "properties.$current_url",
      "properties.$event_type",
    ],
    orderBy: ["timestamp ASC"],
    limit: 1000000,
    properties: [
      {
        key: "$session_id",
        value: [recordingId],
        operator: "exact",
        type: "event",
      },
    ],
  };

  const payload = {
    query: query,
  };

  const response = await fetch(url, {
    method: "POST",
    headers: headers,
    body: JSON.stringify(payload),
  });

  const data = await response.json();

  // Remap the results to match the Event type
  return data.results.map(
    (result: any[]): PosthogEvent => ({
      uuid: result[0],
      event: result[1],
      timestamp: result[2],
      elements_chain: result[3],
      window_id: result[4],
      current_url: result[5],
      event_type: result[6],
    })
  );
}


### File: ./posthog/src/functions/posthog/sessionRecording/snapshotBlob.ts ###
import { log } from "@restackio/ai/function";
import "dotenv/config";

export async function posthogGetSnapshotBlob({
  recordingId,
  blobKey,
  apiKey = process.env.POSTHOG_API_KEY,
  projectId,
  host,
}: {
  recordingId: string;
  blobKey: string;
  apiKey?: string;
  projectId: string;
  host: string;
}) {
  try {
    if (!apiKey) {
      throw new Error("Posthog personal api key missing");
    }

    const headers = { Authorization: `Bearer ${apiKey}` };
    const url = `${host}/api/projects/${projectId}/session_recordings/${recordingId}/snapshots?blob_key=${blobKey}&source=blob`;

    const response = await fetch(url, { headers });

    if (!response.ok) {
      throw new Error(`HTTP error ${response.status}`);
    }

    const result = await response.text();

    return result;
  } catch (error) {
    log.error("Encountered exception. ", { error });
    throw error;
  }
}


### File: ./posthog/src/functions/posthog/sessionRecording/recordings.ts ###
import { log } from "@restackio/ai/function";
import "dotenv/config";

export type PosthogRecording = {
  id: string;
  distinct_id: string;
  viewed: boolean;
  recording_duration: number;
  active_seconds: number;
  inactive_seconds: number;
  start_time: string;
  end_time: string;
  click_count: number;
  keypress_count: number;
  mouse_activity_count: number;
  console_log_count: number;
  console_warn_count: number;
  console_error_count: number;
  start_url: string;
  person: {
    id: number;
    name: string;
    distinct_ids: string[];
    properties: any;
    created_at: string;
    uuid: string;
  };
  storage: string;
  snapshot_source: string;
};

type ApiResponse = {
  count: number;
  next: string | null;
  previous: string | null;
  results: PosthogRecording[];
};

export async function posthogGetRecordings({
  projectId,
  host,
  apiKey = process.env.POSTHOG_API_KEY,
}: {
  projectId: string;
  host: string;
  apiKey?: string;
}) {
  try {
    if (!apiKey) {
      throw new Error("Posthog personal api key missing");
    }

    const headers = { Authorization: `Bearer ${apiKey}` };
    const now = new Date();
    const yesterday = new Date(now);
    yesterday.setDate(now.getDate() - 1);

    let url = `${host}/api/projects/${projectId}/session_recordings/`;
    let allResults: PosthogRecording[] = [];
    let hasMore = true;

    while (hasMore) {
      const response = await fetch(url, { headers });

      if (!response.ok) {
        throw new Error(`HTTP error ${response.status}`);
      }

      const result: ApiResponse = await response.json();
      const filteredResults = result.results.filter(
        (recording: PosthogRecording) =>
          new Date(recording.start_time) >= yesterday
      );

      allResults = allResults.concat(filteredResults);

      if (result.next && filteredResults.length === result.results.length) {
        url = result.next;
      } else {
        hasMore = false;
      }
    }

    return {
      count: allResults.length,
      results: allResults,
    };
  } catch (error) {
    log.error("Encountered exception. ", { error });
    throw error;
  }
}


### File: ./posthog/src/functions/posthog/sessionRecording/snapshots.ts ###
import { log } from "@restackio/ai/function";
import "dotenv/config";

export type PosthogSnapshot = {
  source: "blob" | "realtime";
  start_timestamp: string;
  end_timestamp: string;
  blob_key: string;
};

export type ApiResponse = {
  sources: PosthogSnapshot[];
};

export async function posthogGetSnapshots({
  recordingId,
  apiKey = process.env.POSTHOG_API_KEY,
  projectId,
  host,
}: {
  recordingId: string;
  apiKey?: string;
  projectId: string;
  host: string;
}) {
  try {
    if (!apiKey) {
      throw new Error("Posthog personal api key missing");
    }

    const headers = { Authorization: `Bearer ${apiKey}` };
    const url = `${host}/api/projects/${projectId}/session_recordings/${recordingId}/snapshots`;

    const response = await fetch(url, { headers });

    if (!response.ok) {
      throw new Error(`HTTP error ${response.status}`);
    }

    const result: ApiResponse = await response.json();

    const blobKeys = result.sources
      .filter((source) => source.source === "blob" && source.blob_key)
      .map((source) => source.blob_key ?? "");

    log.info("blobKeys", { blobKeys });

    return {
      snapshots: result.sources,
      blobKeys: blobKeys,
    };
  } catch (error) {
    log.error("Encountered exception. ", { error });
    throw error;
  }
}


### File: ./posthog/src/functions/index.ts ###
export * from "./linear";
export * from "./openai";
export * from "./posthog";
export * from "./utils";



### File: ./posthog/src/functions/openai/types/events.ts ###
import OpenAI from "openai/index";

export type StreamEvent = {
  chunkId?: string;
  response: string;
  assistantName?: string;
  isLast: boolean;
};

export type ToolCallEvent =
  OpenAI.Chat.Completions.ChatCompletionChunk.Choice.Delta.ToolCall & {
    function: {
      name: string;
      input: JSON;
    };
    assistantName?: string;
  };


### File: ./posthog/src/functions/openai/types/index.ts ###
export * from "./events";


### File: ./posthog/src/functions/openai/chat/completionsBase.ts ###
import { FunctionFailure, log } from "@restackio/ai/function";
import OpenAI from "openai/index";
import { ChatCompletionCreateParamsNonStreaming } from "openai/resources/chat/completions";
import { openaiClient } from "../utils/client";
import { openaiCost, Price } from "../utils/cost";
import { ChatCompletion, ChatModel } from "openai/resources/index";

export type UsageOutput = { tokens: number; cost: number };

export type OpenAIChatInput = {
  userContent: string;
  systemContent?: string;
  model?: ChatModel;
  jsonSchema?: {
    name: string;
    schema: Record<string, unknown>;
  };
  price?: Price;
  apiKey?: string;
  params?: ChatCompletionCreateParamsNonStreaming;
  tools?: OpenAI.Chat.Completions.ChatCompletionTool[];
  toolChoice?: OpenAI.Chat.Completions.ChatCompletionToolChoiceOption;
};

export const openaiChatCompletionsBase = async ({
  userContent,
  systemContent = "",
  model = "gpt-4o-mini",
  jsonSchema,
  price,
  apiKey,
  params,
  tools,
  toolChoice,
}: OpenAIChatInput): Promise<{ result: ChatCompletion; cost?: number }> => {
  try {
    const openai = openaiClient({ apiKey });

    const isO1Model = model.startsWith("o1-");

    const o1ModelParams = {
      temperature: 1,
      top_p: 1,
      frequency_penalty: 0,
      presence_penalty: 0,
    };

    const chatParams: ChatCompletionCreateParamsNonStreaming = {
      messages: [
        ...(systemContent ? [{ role: "system" as const, content: systemContent }] : []),
        { role: "user" as const, content: userContent },
        ...(params?.messages ?? []),
      ],
      ...(jsonSchema && {
        response_format: {
          type: "json_schema",
          json_schema: {
            name: jsonSchema.name,
            strict: true,
            schema: jsonSchema.schema,
          },
        },
      }),
      model,
      ...(tools && { tools }),
      ...(toolChoice && { tool_choice: toolChoice }),
      ...params,
      ...(isO1Model && o1ModelParams),
    };

    log.debug("OpenAI chat completion params", {
      chatParams,
    });

    const completion = await openai.chat.completions.create(chatParams);

    return {
      result: completion,
      cost:
        price &&
        openaiCost({
          price,
          tokensCount: {
            input: completion.usage?.prompt_tokens ?? 0,
            output: completion.usage?.completion_tokens ?? 0,
          },
        }),
    };
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error OpenAI chat: ${error}`);
  }
};

### File: ./posthog/src/functions/openai/chat/completionsStream.ts ###
import OpenAI from "openai/index";
import { ChatCompletionChunk } from "openai/resources/chat/completions";

import Restack from "@restackio/ai";
import { currentWorkflow, log } from "@restackio/ai/function";

import { StreamEvent, ToolCallEvent } from "../types/events";

import { aggregateStreamChunks } from "../utils/aggregateStream";
import { mergeToolCalls } from "../utils/mergeToolCalls";
import { openaiClient } from "../utils/client";
import { openaiCost, Price } from "../utils/cost";
import { SendWorkflowEvent } from "@restackio/ai/event";
import { ChatModel } from "openai/resources/index";

export async function openaiChatCompletionsStream({
  model = "gpt-4o-mini",
  userName,
  newMessage,
  assistantName,
  messages = [],
  tools,
  toolEvent,
  streamAtCharacter,
  streamEvent,
  apiKey,
  price,
}: {
  model?: ChatModel;
  userName?: string;
  newMessage?: string;
  assistantName?: string;
  messages?: OpenAI.Chat.Completions.ChatCompletionMessageParam[];
  tools?: OpenAI.Chat.Completions.ChatCompletionTool[];
  toolEvent?: {
    workflowEventName: string;
    workflow?: SendWorkflowEvent["workflow"];
  };
  streamAtCharacter?: string;
  streamEvent?: {
    workflowEventName: string;
    workflow?: SendWorkflowEvent["workflow"];
  };
  apiKey?: string;
  price?: Price;
}) {
  const restack = new Restack();
  const workflow = currentWorkflow().workflowExecution;

  log.debug("workflow", { workflow });

  if (newMessage) {
    messages.push({
      role: "user",
      name: userName,
      content: newMessage,
    });
  }

  const openai = openaiClient({ apiKey });
  const chatStream = await openai.chat.completions.create({
    model: model,
    messages,
    tools,
    stream: true,
    stream_options: {
      include_usage: true,
    },
  });

  const [stream, streamEnd] = chatStream.tee();
  const readableStream = streamEnd.toReadableStream() as unknown as ReadableStream<any>;
  const aggregatedStream = await aggregateStreamChunks(readableStream);

  let finishReason: ChatCompletionChunk.Choice["finish_reason"];
  let response: ChatCompletionChunk.Choice.Delta["content"] = "";
  let tokensCountInput = 0;
  let tokensCountOutput = 0;

  for await (const chunk of stream) {
    let content = chunk.choices[0]?.delta?.content || "";
    finishReason = chunk.choices[0]?.finish_reason;
    tokensCountInput += chunk.usage?.prompt_tokens ?? 0;
    tokensCountOutput += chunk.usage?.completion_tokens ?? 0;

    if (finishReason === "tool_calls") {
      const { toolCalls } = mergeToolCalls(aggregatedStream);
      await Promise.all(
        toolCalls.map((toolCall) => {
          if (toolEvent) {
            const functionArguments = JSON.parse(
              toolCall.function?.arguments ?? ""
            );

            const input: ToolCallEvent = {
              ...toolCall,
              function: {
                name: toolCall.function?.name ?? "",
                input: functionArguments,
              },
              assistantName,
            };

            if (toolEvent) {
              const workflowEvent = {
                event: {
                  name: toolEvent.workflowEventName,
                  input,
                },
                workflow: {
                  ...workflow,
                  ...toolEvent.workflow,
                },
              };
              log.debug("toolEvent sendWorkflowEvent", { workflowEvent });

              restack.sendWorkflowEvent(workflowEvent);
            }
          }
        })
      );
      return {
        result: {
          messages,
          toolCalls,
        },
        cost:
          price &&
          openaiCost({
            price,
            tokensCount: {
              input: tokensCountInput,
              output: tokensCountOutput,
            },
          }),
      };
    } else {
      response += content;
      if (
        content.trim().slice(-1) === streamAtCharacter ||
        finishReason === "stop"
      ) {
        if (response.length) {
          const input: StreamEvent = {
            chunkId: chunk.id,
            response,
            assistantName,
            isLast: finishReason === "stop",
          };
          if (streamEvent) {
            const workflowEvent = {
              event: {
                name: streamEvent.workflowEventName,
                input,
              },
              workflow: {
                ...workflow,
                ...streamEvent.workflow,
              },
            };
            log.debug("streamEvent sendWorkflowEvent", { workflowEvent });
            restack.sendWorkflowEvent(workflowEvent);
          }
        }
      }

      if (finishReason === "stop") {
        const newMessage: OpenAI.Chat.Completions.ChatCompletionMessageParam = {
          content: response,
          role: "assistant",
          name: assistantName,
        };

        messages.push(newMessage);

        return {
          result: {
            messages,
          },
          cost:
            price &&
            openaiCost({
              price,
              tokensCount: {
                input: tokensCountInput,
                output: tokensCountOutput,
              },
            }),
        };
      }
    }
  }
}


### File: ./posthog/src/functions/openai/chat/index.ts ###
export * from "./completionsBase";
export * from "./completionsStream";


### File: ./posthog/src/functions/openai/utils/cost.ts ###
export type TokensCount = {
  input: number;
  output: number;
};

export type Price = {
  input: number;
  output: number;
};
export const openaiCost = ({
  tokensCount,
  price,
}: {
  tokensCount: TokensCount;
  price: Price;
}): number => {
  let cost = 0;
  const { input: inputTokens, output: outputTokens } = tokensCount;
  const { input: inputPrice, output: outputPrice } = price;

  cost = inputTokens * inputPrice + outputTokens * outputPrice;

  return cost;
};


### File: ./posthog/src/functions/openai/utils/aggregateStream.ts ###
export async function aggregateStreamChunks(stream: ReadableStream) {
  const reader = stream.getReader();
  const chunks: Uint8Array[] = [];

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    if (value) chunks.push(value);
  }

  const aggregated = new Uint8Array(
    chunks.reduce((acc, chunk) => acc + chunk.length, 0)
  );
  let offset = 0;
  for (const chunk of chunks) {
    aggregated.set(chunk, offset);
    offset += chunk.length;
  }

  const textContent = new TextDecoder().decode(aggregated);
  const jsonObjects = textContent
    .split("\n")
    .filter((line) => line.trim())
    .map((line) => JSON.parse(line));
  return jsonObjects;
}


### File: ./posthog/src/functions/openai/utils/client.ts ###
import OpenAI from "openai/index";
import "dotenv/config";

let openaiInstance: OpenAI | null = null;

export const openaiClient = ({
  apiKey = process.env.OPENAI_API_KEY,
}: {
  apiKey?: string;
}): OpenAI => {
  if (!apiKey) {
    throw new Error("API key is required to create OpenAI client.");
  }

  if (!openaiInstance) {
    openaiInstance = new OpenAI({
      apiKey,
    });
  }
  return openaiInstance;
};


### File: ./posthog/src/functions/openai/utils/index.ts ###
export * from "./aggregateStream";
export * from "./client";
export * from "./cost";
export * from "./mergeToolCalls";


### File: ./posthog/src/functions/openai/utils/mergeToolCalls.ts ###
import OpenAI from "openai/index";
import { ChatCompletionChunk } from "openai/resources/chat/completions.mjs";

export function mergeToolCalls(aggregatedStream: ChatCompletionChunk[]) {
  const toolCalls: OpenAI.Chat.Completions.ChatCompletionChunk.Choice.Delta.ToolCall[] =
    [];

  aggregatedStream.forEach((chunk) => {
    chunk.choices.forEach((choice) => {
      if (choice.delta.tool_calls) {
        choice.delta.tool_calls.forEach((toolCall) => {
          const lastToolCall = toolCalls[toolCalls.length - 1];
          if (toolCall.id) {
            toolCalls.push({
              ...toolCall,
              function: { ...toolCall.function, arguments: "" },
            });
          } else if (
            lastToolCall &&
            lastToolCall.function &&
            toolCall.function?.arguments
          ) {
            lastToolCall.function.arguments += toolCall.function.arguments;
          }
        });
      }
    });
  });

  return { toolCalls };
}


### File: ./posthog/src/functions/openai/index.ts ###
export * from "./chat";
export * from "./thread";


### File: ./posthog/src/functions/openai/thread/createMessageOnThread.ts ###
import OpenAI from "openai/index";
import { FunctionFailure } from "@restackio/ai/function";

import { openaiClient } from "../utils/client";

export async function createMessageOnThread({
  apiKey,
  threadId,
  content,
  role,
}: {
  apiKey: string;
  threadId: string;
  content: string;
  role: OpenAI.Beta.Threads.MessageCreateParams["role"];
}) {
  try {
    const openai = openaiClient({ apiKey });
    await openai.beta.threads.messages.create(threadId, {
      role,
      content,
    });
  } catch (error) {
    throw FunctionFailure.nonRetryable(
      `Error creating message thread: ${error}`
    );
  }
}


### File: ./posthog/src/functions/openai/thread/runThread.ts ###
import { FunctionFailure } from "@restackio/ai/function";
import { Stream } from "openai/streaming";
import { AssistantStreamEvent } from "openai/resources/beta/index";
import { Run } from "openai/resources/beta/threads/runs/index";

import { openaiClient } from "../utils/client";

export async function runThread({
  apiKey,
  threadId,
  assistantId,
  stream = false,
}: {
  apiKey: string;
  threadId: string;
  assistantId: string;
  stream: boolean;
}): Promise<Stream<AssistantStreamEvent> | Run> {
  try {
    const openai = openaiClient({ apiKey });

    const run = await openai.beta.threads.runs.create(threadId, {
      assistant_id: assistantId,
      ...(stream && { stream }),
    });

    return run;
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error running thread: ${error}`);
  }
}

### File: ./posthog/src/functions/openai/thread/createAssistant.ts ###
import { ChatModel } from "openai/resources/index";
import { FunctionFailure } from "@restackio/ai/function";
import { Assistant, AssistantTool } from "openai/resources/beta/index";

import { openaiClient } from "../utils/client";

export async function createAssistant({
  apiKey,
  name,
  instructions,
  model = "gpt-4o-mini",
  tools = [],
}: {
  apiKey: string;
  name: string;
  instructions: string;
  tools?: AssistantTool[];
  model: ChatModel;
}): Promise<Assistant> {
  try {
    const openai = openaiClient({ apiKey });

    const assistant = await openai.beta.assistants.create({
      name,
      instructions,
      model,
      tools,
    });

    return assistant;
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error creating assistant: ${error}`);
  }
}


### File: ./posthog/src/functions/openai/thread/index.ts ###
export * from "./createAssistant";
export * from "./createMessageOnThread";
export * from "./createThread";
export * from "./runThread";


### File: ./posthog/src/functions/openai/thread/createThread.ts ###
import { FunctionFailure } from "@restackio/ai/function";
import { Thread } from "openai/resources/beta/index";

import { openaiClient } from "../utils/client";

export async function createThread({
  apiKey,
}: {
  apiKey: string;
}): Promise<Thread> {
  try {
    const openai = openaiClient({ apiKey });
    const thread = await openai.beta.threads.create();

    return thread;
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error creating thread: ${error}`);
  }
}


### File: ./posthog/src/services.ts ###
import {
  posthogGetRecordings,
  posthogGetSnapshotBlob,
  posthogGetSnapshots,
  posthogBlobChunks,
  posthogSessionEvents,
  workflowSendEvent,
  linearCreateComment,
  linearCreateIssue,
  openaiChatCompletionsBase,
  openaiChatCompletionsStream,
} from "./functions";
import { client } from "./client";

async function services() {
  const workflowsPath = require.resolve("./workflows");

  try {
    // https://posthog.com/docs/api#rate-limiting

    await Promise.all([
      client.startService({
        workflowsPath,
        functions: {
          workflowSendEvent,
        },
      }),
      client.startService({
        taskQueue: "posthog",
        functions: {
          posthogGetRecordings,
          posthogGetSnapshotBlob,
          posthogGetSnapshots,
          posthogBlobChunks,
          posthogSessionEvents,
        },
        options: {
          rateLimit: 240 * 60,
        },
      }),
      client.startService({
        taskQueue: "openai",
        functions: {
          openaiChatCompletionsBase,
          openaiChatCompletionsStream,
        },
        options: {
          rateLimit: 240 * 60,
        },
      }),
      client.startService({
        taskQueue: "openai-beta",
        functions: {
          openaiChatCompletionsBase,
          openaiChatCompletionsStream,
        },
        options: {
          rateLimit: 240 * 60,
        },
      }),
      client.startService({
        taskQueue: "linear",
        functions: {
          linearCreateComment,
          linearCreateIssue,
        },
        options: {
          rateLimit: 240 * 60,
        },
      }),
    ]);

    console.log("Services running successfully.");
  } catch (e) {
    console.error("Failed to run worker", e);
  }
}

services().catch((err) => {
  console.error("Error running services:", err);
});


### File: ./composio/scheduleWorkflow.ts ###
import { client } from "./src/client";

export type InputSchedule = {
  entityId: string;
  calendarInstruction: string;
};

const today = new Date().toDateString();

async function scheduleWorkflow(input: InputSchedule) {
  try {
    const workflowId = `${Date.now()}-createCalendarEvent`;
    const runId = await client.scheduleWorkflow({
      workflowName: "createCalendarEventWorkflow",
      workflowId,
      input,
    });

    const result = await client.getWorkflowResult({ workflowId, runId });

    console.log("Workflow result:", result);

    process.exit(0); // Exit the process successfully
  } catch (error) {
    console.error("Error scheduling workflow:", error);
    process.exit(1); // Exit the process with an error code
  }
}

scheduleWorkflow({
  entityId: "default",
  calendarInstruction: `Create a 1 hour meeting event at 5:30PM tomorrow. Today's date is ${today}`,
});


### File: ./composio/src/workflows/createCalendarEvent.ts ###
import { step, log } from "@restackio/ai/workflow";
import * as functions from "../functions";

export async function createCalendarEventWorkflow({
  entityId,
  calendarInstruction,
}: {
  entityId: string;
  calendarInstruction: string;
}) {
  const connection = await step<typeof functions>({
    taskQueue: "composio",
  }).initiateConnection({
    entityId,
    appName: 'googlecalendar',
  });

  if (!connection.authenticated) {
    log.info(
      `Follow the link to authenticate with google calendar ${connection.redirectUrl}`
    );
    return connection;
  }

  const calendarEvent = await step<typeof functions>({
    taskQueue: "composio",
  }).createCalendarEvent({
    entityId,
    calendarInstruction,
  });

  return calendarEvent;
}


### File: ./composio/src/workflows/index.ts ###
export * from "./createCalendarEvent";


### File: ./composio/src/client.ts ###
import Restack from "@restackio/ai";

import "dotenv/config";

export const connectionOptions = {
  engineId: process.env.RESTACK_ENGINE_ID!,
  address: process.env.RESTACK_ENGINE_ADDRESS!,
  apiKey: process.env.RESTACK_ENGINE_API_KEY!,
};

export const client = new Restack(
  process.env.RESTACK_ENGINE_API_KEY ? connectionOptions : undefined
);


### File: ./composio/src/functions/index.ts ###
export * from "./composio";
export * from "./openai";

### File: ./composio/src/functions/composio/getEntity.ts ###
import { FunctionFailure } from "@restackio/ai/function";
import { Entity } from "composio-core/lib/src/sdk/models/Entity";

import { composioClient } from "./utils/client";

export async function getEntity({
  composioApiKey,
  entityId,
}: {
  composioApiKey?: string;
  entityId: string;
}): Promise<Entity> {
  const client = composioClient({ composioApiKey });
  console.log("client", client);
  try {
    const entity = client.getEntity(entityId);
    return entity;
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error getting entity: ${error}`);
  }
}


### File: ./composio/src/functions/composio/createCalendarEvent.ts ###
import { openaiChatCompletionsBase } from "../openai";

import { openAiToolsetClient } from "./utils/toolsets";

export async function createCalendarEvent({
  entityId,
  composioApiKey,
  calendarInstruction,
}: {
  entityId?: string;
  composioApiKey?: string;
  calendarInstruction: string;
}) {
  const composioOpenAiClient = openAiToolsetClient({
    composioApiKey,
    entityId,
  });

  const tools = await composioOpenAiClient.getTools({
    actions: ["googlecalendar_create_event"],
  });

  const { result } = await openaiChatCompletionsBase({
    userContent: calendarInstruction,
    tools,
    toolChoice: "auto",
  });

  return composioOpenAiClient.handleToolCall(result);
}


### File: ./composio/src/functions/composio/getExpectedParamsForUser.ts ###
import { openAiToolsetClient } from "./utils/toolsets";

export async function getExpectedParamsForUser({
  composioApiKey,
  app,
  authScheme,
  entityId,
}: {
  composioApiKey?: string;
  app: string;
  authScheme?:
    | "OAUTH2"
    | "OAUTH1"
    | "API_KEY"
    | "BASIC"
    | "BEARER_TOKEN"
    | "BASIC_WITH_JWT";
  entityId?: string;
}) {
  const toolset = openAiToolsetClient({ composioApiKey, entityId });
  const response = await toolset.getExpectedParamsForUser({
    app,
    ...(authScheme && { authScheme }),
    ...(entityId && { entityId }),
  });

  return response.expectedInputFields;
}


### File: ./composio/src/functions/composio/getEntityConnections.ts ###
import { openAiToolsetClient } from "./utils/toolsets";

export async function getEntityConnections({
  entityId,
  composioApiKey,
}: {
  entityId: string;
  composioApiKey?: string;
}) {
  const toolSetClient = openAiToolsetClient({ composioApiKey, entityId });
  const connections = await toolSetClient.client.connectedAccounts.list({});
  return connections;
}

### File: ./composio/src/functions/composio/utils/toolsets.ts ###
import { OpenAIToolSet } from "composio-core";

export function openAiToolsetClient({
  composioApiKey = process.env.COMPOSIO_API_KEY,
  entityId,
}: {
  composioApiKey?: string;
  entityId?: string;
}) {
  return new OpenAIToolSet({
    apiKey: composioApiKey,
    entityId,
  });
}


### File: ./composio/src/functions/composio/utils/client.ts ###
import { Composio } from "composio-core";

export const composioClient = ({
  composioApiKey = process.env.COMPOSIO_API_KEY,
}: {
  composioApiKey?: string;
}) => {
  return new Composio(composioApiKey);
};


### File: ./composio/src/functions/composio/initiateConnection.ts ###
import { FunctionFailure, log } from "@restackio/ai/function";

import { getEntity } from "./getEntity";

export async function initiateConnection({
  entityId,
  appName,
  composioApiKey,
  waitUntilActive,
}: {
  entityId: string;
  appName: string;
  composioApiKey?: string;
  waitUntilActive?: number;
}) {
  const entity = await getEntity({ composioApiKey, entityId });

  const entityAppConnection = await entity.getConnection(appName);

  if (entityAppConnection) {
    return {
      authenticated: true,
      message: `Already connected to ${appName}`,
      redirectUrl: entityAppConnection.redirectUrl,
    };
  }

  const connection = await entity.initiateConnection(appName);

  if (!waitUntilActive) {
    return {
      authenticated: false,
      message: `User needs to follow redirect URL to authenticate: ${connection.redirectUrl}`,
      redirectUrl: connection.redirectUrl,
    };
  }

  try {
    await connection.waitUntilActive(waitUntilActive);
    return {
      authenticated: true,
      message: `Connected to ${appName}`,
      redirectUrl: connection.redirectUrl,
    };
  } catch (error) {
    log.error("User did not authenticate in time for application", {
      appName,
      error,
    });

    return {
      authenticated: false,
      message: `User did not authenticate in time for application: ${appName}`,
      redirectUrl: connection.redirectUrl,
    };
  }
}


### File: ./composio/src/functions/composio/index.ts ###
export * from "./getEntity";
export * from "./initiateConnection";
export * from "./createCalendarEvent";
export * from "./getEntityConnections";
export * from "./getExpectedParamsForUser";


### File: ./composio/src/functions/openai/types/events.ts ###
import OpenAI from "openai/index";

export type StreamEvent = {
  chunkId?: string;
  response: string;
  assistantName?: string;
  isLast: boolean;
};

export type ToolCallEvent =
  OpenAI.Chat.Completions.ChatCompletionChunk.Choice.Delta.ToolCall & {
    function: {
      name: string;
      input: JSON;
    };
    assistantName?: string;
  };


### File: ./composio/src/functions/openai/types/index.ts ###
export * from "./events";


### File: ./composio/src/functions/openai/chat/completionsBase.ts ###
import { FunctionFailure, log } from "@restackio/ai/function";
import OpenAI from "openai/index";
import { ChatCompletionCreateParamsNonStreaming } from "openai/resources/chat/completions";
import { openaiClient } from "../utils/client";
import { openaiCost, Price } from "../utils/cost";
import { ChatCompletion, ChatModel } from "openai/resources/index";

export type UsageOutput = { tokens: number; cost: number };

export type OpenAIChatInput = {
  userContent: string;
  systemContent?: string;
  model?: ChatModel;
  jsonSchema?: {
    name: string;
    schema: Record<string, unknown>;
  };
  price?: Price;
  apiKey?: string;
  params?: ChatCompletionCreateParamsNonStreaming;
  tools?: OpenAI.Chat.Completions.ChatCompletionTool[];
  toolChoice?: OpenAI.Chat.Completions.ChatCompletionToolChoiceOption;
};

export const openaiChatCompletionsBase = async ({
  userContent,
  systemContent = "",
  model = "gpt-4o-mini",
  jsonSchema,
  price,
  apiKey,
  params,
  tools,
  toolChoice,
}: OpenAIChatInput): Promise<{ result: ChatCompletion; cost?: number }> => {
  try {
    const openai = openaiClient({ apiKey });

    const isO1Model = model.startsWith("o1-");

    const o1ModelParams = {
      temperature: 1,
      top_p: 1,
      frequency_penalty: 0,
      presence_penalty: 0,
    };

    const chatParams: ChatCompletionCreateParamsNonStreaming = {
      messages: [
        ...(systemContent ? [{ role: "system" as const, content: systemContent }] : []),
        { role: "user" as const, content: userContent },
        ...(params?.messages ?? []),
      ],
      ...(jsonSchema && {
        response_format: {
          type: "json_schema",
          json_schema: {
            name: jsonSchema.name,
            strict: true,
            schema: jsonSchema.schema,
          },
        },
      }),
      model,
      ...(tools && { tools }),
      ...(toolChoice && { tool_choice: toolChoice }),
      ...params,
      ...(isO1Model && o1ModelParams),
    };

    log.debug("OpenAI chat completion params", {
      chatParams,
    });

    const completion = await openai.chat.completions.create(chatParams);

    return {
      result: completion,
      cost:
        price &&
        openaiCost({
          price,
          tokensCount: {
            input: completion.usage?.prompt_tokens ?? 0,
            output: completion.usage?.completion_tokens ?? 0,
          },
        }),
    };
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error OpenAI chat: ${error}`);
  }
};

### File: ./composio/src/functions/openai/chat/completionsStream.ts ###
import OpenAI from "openai/index";
import { ChatCompletionChunk } from "openai/resources/chat/completions";

import Restack from "@restackio/ai";
import { currentWorkflow, log } from "@restackio/ai/function";

import { StreamEvent, ToolCallEvent } from "../types/events";

import { aggregateStreamChunks } from "../utils/aggregateStream";
import { mergeToolCalls } from "../utils/mergeToolCalls";
import { openaiClient } from "../utils/client";
import { openaiCost, Price } from "../utils/cost";
import { SendWorkflowEvent } from "@restackio/ai/event";
import { ChatModel } from "openai/resources/index";

export async function openaiChatCompletionsStream({
  model = "gpt-4o-mini",
  userName,
  newMessage,
  assistantName,
  messages = [],
  tools,
  toolEvent,
  streamAtCharacter,
  streamEvent,
  apiKey,
  price,
}: {
  model?: ChatModel;
  userName?: string;
  newMessage?: string;
  assistantName?: string;
  messages?: OpenAI.Chat.Completions.ChatCompletionMessageParam[];
  tools?: OpenAI.Chat.Completions.ChatCompletionTool[];
  toolEvent?: {
    workflowEventName: string;
    workflow?: SendWorkflowEvent["workflow"];
  };
  streamAtCharacter?: string;
  streamEvent?: {
    workflowEventName: string;
    workflow?: SendWorkflowEvent["workflow"];
  };
  apiKey?: string;
  price?: Price;
}) {
  const restack = new Restack();
  const workflow = currentWorkflow().workflowExecution;

  log.debug("workflow", { workflow });

  if (newMessage) {
    messages.push({
      role: "user",
      name: userName,
      content: newMessage,
    });
  }

  const openai = openaiClient({ apiKey });
  const chatStream = await openai.chat.completions.create({
    model: model,
    messages,
    tools,
    stream: true,
    stream_options: {
      include_usage: true,
    },
  });

  const [stream, streamEnd] = chatStream.tee();
  const readableStream = streamEnd.toReadableStream() as unknown as ReadableStream<any>;
  const aggregatedStream = await aggregateStreamChunks(readableStream);

  let finishReason: ChatCompletionChunk.Choice["finish_reason"];
  let response: ChatCompletionChunk.Choice.Delta["content"] = "";
  let tokensCountInput = 0;
  let tokensCountOutput = 0;

  for await (const chunk of stream) {
    let content = chunk.choices[0]?.delta?.content || "";
    finishReason = chunk.choices[0]?.finish_reason;
    tokensCountInput += chunk.usage?.prompt_tokens ?? 0;
    tokensCountOutput += chunk.usage?.completion_tokens ?? 0;

    if (finishReason === "tool_calls") {
      const { toolCalls } = mergeToolCalls(aggregatedStream);
      await Promise.all(
        toolCalls.map((toolCall) => {
          if (toolEvent) {
            const functionArguments = JSON.parse(
              toolCall.function?.arguments ?? ""
            );

            const input: ToolCallEvent = {
              ...toolCall,
              function: {
                name: toolCall.function?.name ?? "",
                input: functionArguments,
              },
              assistantName,
            };

            if (toolEvent) {
              const workflowEvent = {
                event: {
                  name: toolEvent.workflowEventName,
                  input,
                },
                workflow: {
                  ...workflow,
                  ...toolEvent.workflow,
                },
              };
              log.debug("toolEvent sendWorkflowEvent", { workflowEvent });

              restack.sendWorkflowEvent(workflowEvent);
            }
          }
        })
      );
      return {
        result: {
          messages,
          toolCalls,
        },
        cost:
          price &&
          openaiCost({
            price,
            tokensCount: {
              input: tokensCountInput,
              output: tokensCountOutput,
            },
          }),
      };
    } else {
      response += content;
      if (
        content.trim().slice(-1) === streamAtCharacter ||
        finishReason === "stop"
      ) {
        if (response.length) {
          const input: StreamEvent = {
            chunkId: chunk.id,
            response,
            assistantName,
            isLast: finishReason === "stop",
          };
          if (streamEvent) {
            const workflowEvent = {
              event: {
                name: streamEvent.workflowEventName,
                input,
              },
              workflow: {
                ...workflow,
                ...streamEvent.workflow,
              },
            };
            log.debug("streamEvent sendWorkflowEvent", { workflowEvent });
            restack.sendWorkflowEvent(workflowEvent);
          }
        }
      }

      if (finishReason === "stop") {
        const newMessage: OpenAI.Chat.Completions.ChatCompletionMessageParam = {
          content: response,
          role: "assistant",
          name: assistantName,
        };

        messages.push(newMessage);

        return {
          result: {
            messages,
          },
          cost:
            price &&
            openaiCost({
              price,
              tokensCount: {
                input: tokensCountInput,
                output: tokensCountOutput,
              },
            }),
        };
      }
    }
  }
}


### File: ./composio/src/functions/openai/chat/index.ts ###
export * from "./completionsBase";
export * from "./completionsStream";


### File: ./composio/src/functions/openai/utils/cost.ts ###
export type TokensCount = {
  input: number;
  output: number;
};

export type Price = {
  input: number;
  output: number;
};
export const openaiCost = ({
  tokensCount,
  price,
}: {
  tokensCount: TokensCount;
  price: Price;
}): number => {
  let cost = 0;
  const { input: inputTokens, output: outputTokens } = tokensCount;
  const { input: inputPrice, output: outputPrice } = price;

  cost = inputTokens * inputPrice + outputTokens * outputPrice;

  return cost;
};


### File: ./composio/src/functions/openai/utils/aggregateStream.ts ###
export async function aggregateStreamChunks(stream: ReadableStream) {
  const reader = stream.getReader();
  const chunks: Uint8Array[] = [];

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    if (value) chunks.push(value);
  }

  const aggregated = new Uint8Array(
    chunks.reduce((acc, chunk) => acc + chunk.length, 0)
  );
  let offset = 0;
  for (const chunk of chunks) {
    aggregated.set(chunk, offset);
    offset += chunk.length;
  }

  const textContent = new TextDecoder().decode(aggregated);
  const jsonObjects = textContent
    .split("\n")
    .filter((line) => line.trim())
    .map((line) => JSON.parse(line));
  return jsonObjects;
}


### File: ./composio/src/functions/openai/utils/client.ts ###
import OpenAI from "openai/index";
import "dotenv/config";

let openaiInstance: OpenAI | null = null;

export const openaiClient = ({
  apiKey = process.env.OPENAI_API_KEY,
}: {
  apiKey?: string;
}): OpenAI => {
  if (!apiKey) {
    throw new Error("API key is required to create OpenAI client.");
  }

  if (!openaiInstance) {
    openaiInstance = new OpenAI({
      apiKey,
    });
  }
  return openaiInstance;
};


### File: ./composio/src/functions/openai/utils/index.ts ###
export * from "./aggregateStream";
export * from "./client";
export * from "./cost";
export * from "./mergeToolCalls";


### File: ./composio/src/functions/openai/utils/mergeToolCalls.ts ###
import OpenAI from "openai/index";
import { ChatCompletionChunk } from "openai/resources/chat/completions.mjs";

export function mergeToolCalls(aggregatedStream: ChatCompletionChunk[]) {
  const toolCalls: OpenAI.Chat.Completions.ChatCompletionChunk.Choice.Delta.ToolCall[] =
    [];

  aggregatedStream.forEach((chunk) => {
    chunk.choices.forEach((choice) => {
      if (choice.delta.tool_calls) {
        choice.delta.tool_calls.forEach((toolCall) => {
          const lastToolCall = toolCalls[toolCalls.length - 1];
          if (toolCall.id) {
            toolCalls.push({
              ...toolCall,
              function: { ...toolCall.function, arguments: "" },
            });
          } else if (
            lastToolCall &&
            lastToolCall.function &&
            toolCall.function?.arguments
          ) {
            lastToolCall.function.arguments += toolCall.function.arguments;
          }
        });
      }
    });
  });

  return { toolCalls };
}


### File: ./composio/src/functions/openai/index.ts ###
export * from "./chat";
export * from "./thread";


### File: ./composio/src/functions/openai/thread/createMessageOnThread.ts ###
import OpenAI from "openai/index";
import { FunctionFailure } from "@restackio/ai/function";

import { openaiClient } from "../utils/client";

export async function createMessageOnThread({
  apiKey,
  threadId,
  content,
  role,
}: {
  apiKey: string;
  threadId: string;
  content: string;
  role: OpenAI.Beta.Threads.MessageCreateParams["role"];
}) {
  try {
    const openai = openaiClient({ apiKey });
    await openai.beta.threads.messages.create(threadId, {
      role,
      content,
    });
  } catch (error) {
    throw FunctionFailure.nonRetryable(
      `Error creating message thread: ${error}`
    );
  }
}


### File: ./composio/src/functions/openai/thread/runThread.ts ###
import { FunctionFailure } from "@restackio/ai/function";
import { Stream } from "openai/streaming";
import { AssistantStreamEvent } from "openai/resources/beta/index";
import { Run } from "openai/resources/beta/threads/runs/index";

import { openaiClient } from "../utils/client";

export async function runThread({
  apiKey,
  threadId,
  assistantId,
  stream = false,
}: {
  apiKey: string;
  threadId: string;
  assistantId: string;
  stream: boolean;
}): Promise<Stream<AssistantStreamEvent> | Run> {
  try {
    const openai = openaiClient({ apiKey });

    const run = await openai.beta.threads.runs.create(threadId, {
      assistant_id: assistantId,
      ...(stream && { stream }),
    });

    return run;
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error running thread: ${error}`);
  }
}

### File: ./composio/src/functions/openai/thread/createAssistant.ts ###
import { ChatModel } from "openai/resources/index";
import { FunctionFailure } from "@restackio/ai/function";
import { Assistant, AssistantTool } from "openai/resources/beta/index";

import { openaiClient } from "../utils/client";

export async function createAssistant({
  apiKey,
  name,
  instructions,
  model = "gpt-4o-mini",
  tools = [],
}: {
  apiKey: string;
  name: string;
  instructions: string;
  tools?: AssistantTool[];
  model: ChatModel;
}): Promise<Assistant> {
  try {
    const openai = openaiClient({ apiKey });

    const assistant = await openai.beta.assistants.create({
      name,
      instructions,
      model,
      tools,
    });

    return assistant;
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error creating assistant: ${error}`);
  }
}


### File: ./composio/src/functions/openai/thread/index.ts ###
export * from "./createAssistant";
export * from "./createMessageOnThread";
export * from "./createThread";
export * from "./runThread";


### File: ./composio/src/functions/openai/thread/createThread.ts ###
import { FunctionFailure } from "@restackio/ai/function";
import { Thread } from "openai/resources/beta/index";

import { openaiClient } from "../utils/client";

export async function createThread({
  apiKey,
}: {
  apiKey: string;
}): Promise<Thread> {
  try {
    const openai = openaiClient({ apiKey });
    const thread = await openai.beta.threads.create();

    return thread;
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error creating thread: ${error}`);
  }
}


### File: ./composio/src/services.ts ###
import { createCalendarEvent, initiateConnection, getExpectedParamsForUser, getEntityConnections, getEntity } from "./functions";
import { client } from "./client";

async function services() {
  const workflowsPath = require.resolve("./workflows");
  try {
    await Promise.all([
      // Start service with current workflows and functions
      client.startService({
        workflowsPath,
      }),
      // Start the composio service
      client.startService({
        taskQueue: "composio",
        functions: {
          createCalendarEvent,
          initiateConnection,
          getExpectedParamsForUser,
          getEntityConnections,
          getEntity,
        },
      }),
    ]);

    console.log("Services running successfully.");
  } catch (e) {
    console.error("Failed to run services", e);
  }
}

services().catch((err) => {
  console.error("Error running services:", err);
});


### File: ./nextjs-together-llamaindex/frontend/tailwind.config.ts ###
import type { Config } from "tailwindcss";

const config: Config = {
  content: [
    "./src/pages/**/*.{js,ts,jsx,tsx,mdx}",
    "./src/components/**/*.{js,ts,jsx,tsx,mdx}",
    "./src/app/**/*.{js,ts,jsx,tsx,mdx}",
  ],
  theme: {
    extend: {
      colors: {
        background: "var(--background)",
        foreground: "var(--foreground)",
      },
    },
  },
  plugins: [],
};
export default config;


### File: ./nextjs-together-llamaindex/frontend/src/app/actions/schedule.ts ###
"use server";
import Restack from "@restackio/ai";
import { Example } from "../components/examplesList";

const connectionOptions = {
  engineId: process.env.RESTACK_ENGINE_ID!,
  address: process.env.RESTACK_ENGINE_ADDRESS!,
  apiKey: process.env.RESTACK_ENGINE_API_KEY!,
};

const client = new Restack(
  process.env.RESTACK_ENGINE_API_KEY ? connectionOptions : undefined
);

export async function scheduleWorkflow(
  workflowName: Example["workflowName"],
  input: Example["input"]
) {
  if (!workflowName || !input) {
    throw new Error("Workflow name and input are required");
  }

  const workflowId = `${Date.now()}-${workflowName.toString()}`;

  const runId = await client.scheduleWorkflow({
    workflowName: workflowName as string,
    workflowId,
    input,
  });

  const result = await client.getWorkflowResult({
    workflowId,
    runId,
  });

  return result;
}


### File: ./nextjs-together-llamaindex/backend/src/workflows/chatCompletionBasic.ts ###
import { step } from "@restackio/ai/workflow";
import * as functions from "../functions";
import { z } from "zod";
import zodToJsonSchema from "zod-to-json-schema";

const MessageSchema = z.object({
    message: z.string().describe("The greeting message."),
});

const jsonSchema = zodToJsonSchema(MessageSchema, 'Message');

export async function chatCompletionBasic({ name }: { name: string }) {

    // Step 1 create greeting message with meta-llama without response format

    const greetingOutput = await step<typeof functions>({
        taskQueue: 'together',
    }).togetherChatCompletionBasic({
        messages: [{ "role": "user", "content": `Write a greeting message to ${name}` }],
        model: 'meta-llama/Llama-3.2-3B-Instruct-Turbo',


    });

    // Step 2 create a response with response format with a model supporting response format

    const goodbyeOutput = await step<typeof functions>({
        taskQueue: 'together',
    }).togetherChatCompletionBasic({
        messages: [{ "role": "user", "content": `Write a goodbye message to ${name}` }],
        model: 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo',
        response_format: {
            type: 'json_object',
            // @ts-ignore
            schema: jsonSchema
        },
    });

    return {
        greetingOutput,
        goodbyeOutput
    };
}



### File: ./nextjs-together-llamaindex/backend/src/workflows/index.ts ###
export * from './chatCompletionBasic';
export * from './llamaindexTogetherSimple';

### File: ./nextjs-together-llamaindex/backend/src/workflows/llamaindexTogetherSimple.ts ###
import { step } from "@restackio/ai/workflow";
import * as functions from "../functions";


export async function llamaindexTogetherSimple({ query }: { query: string }) {
    // Step 1: Query a model with the llamaIndex and Together integration
    const response = await step<typeof functions>({
        taskQueue: 'together',
    }).llamaIndexQueryTogether({
        query,
        model: "meta-llama/Llama-3.2-3B-Instruct-Turbo"
    });

    return response;
}

### File: ./nextjs-together-llamaindex/backend/src/client.ts ###
import Restack from '@restackio/ai';
import dotenv from 'dotenv';

dotenv.config();

export const connectionOptions = {
  engineId: process.env.RESTACK_ENGINE_ID!,
  address: process.env.RESTACK_ENGINE_ADDRESS!,
  apiKey: process.env.RESTACK_ENGINE_API_KEY!,
};

export const client = new Restack(
  process.env.RESTACK_ENGINE_API_KEY ? connectionOptions : undefined
);

### File: ./nextjs-together-llamaindex/backend/src/functions/llamaindex/utils/llamaIndexTogetherClient.ts ###
import { TogetherLLM, Settings } from "llamaindex";

export function llamaIndexTogetherClient({ model }: { model: TogetherLLM["model"] }) {
    Settings.llm = new TogetherLLM({
        apiKey: process.env.TOGETHER_API_KEY,
        model: model,
    });
    return Settings.llm;
}

### File: ./nextjs-together-llamaindex/backend/src/functions/llamaindex/queryTogether.ts ###
import { llamaIndexTogetherClient } from "./utils/llamaIndexTogetherClient";
import { TogetherLLM } from "llamaindex";

export async function llamaIndexQueryTogether({ query, model }: { query: string, model: TogetherLLM["model"] }) {
    const client = llamaIndexTogetherClient({ model });

    const response = await client.chat({
        messages: [{ role: "user", content: query }],
    });

    return response;
}

### File: ./nextjs-together-llamaindex/backend/src/functions/llamaindex/index.ts ###
export * from './queryTogether';

### File: ./nextjs-together-llamaindex/backend/src/functions/together-ai/utils/client.ts ###
import Together from "together-ai";

export const togetherClient = new Together({
    apiKey: process.env.TOGETHER_API_KEY,
});


### File: ./nextjs-together-llamaindex/backend/src/functions/together-ai/chatCompletionBasic.ts ###
import Together from 'together-ai';
import { togetherClient } from './utils/client';

export async function togetherChatCompletionBasic(params: Together.Chat.CompletionCreateParamsNonStreaming) {
    const response = await togetherClient.chat.completions.create(params);
    return response as Together.Chat.ChatCompletion;
}


### File: ./nextjs-together-llamaindex/backend/src/functions/together-ai/index.ts ###
export * from './chatCompletionBasic';

### File: ./nextjs-together-llamaindex/backend/src/functions/index.ts ###
export * from './llamaindex';
export * from './together-ai';

### File: ./nextjs-together-llamaindex/backend/src/services.ts ###
// Simple example to start two services in the same file
import { config } from 'dotenv';
import { togetherChatCompletionBasic, llamaIndexQueryTogether } from "./functions";
import { client } from "./client";

config();

export async function services() {
    const workflowsPath = require.resolve("./workflows");
    try {
        await Promise.all([
            // Generic service with current workflows and functions
            client.startService({
                workflowsPath,
                functions: {

                    // add other functions here
                },
            }),
            // Start the together service to queue function calls to the Together API with rate limiting
            // https://docs.together.ai/docs/rate-limits
            client.startService({
                taskQueue: 'together',
                functions: { togetherChatCompletionBasic, llamaIndexQueryTogether },
                options: {
                    rateLimit: (60 / 60), // 60 RPM -> 1 RPS 
                },
            }),
        ]);

        console.log("Services running successfully.");
    } catch (e) {
        console.error("Failed to run services", e);
    }
}

services().catch((err) => {
    console.error("Error running services:", err);
});


### File: ./audio-transcript/scheduleWorkflow.ts ###
import { client } from './src/client';

import 'dotenv/config';

type TranscribeAndTranslateWorkflowInput = {
  filePath: string;
  targetLanguage: string;
};

async function scheduleWorkflow(input: TranscribeAndTranslateWorkflowInput) {
  try {
    const workflowId = `${Date.now()}-transcribeAndTranslateWorkflow`;
    await client.scheduleWorkflow({
      workflowName: 'transcribeAndTranslateWorkflow',
      workflowId,
      input,
    });

    console.log('Workflow scheduled successfully');

    process.exit(0); // Exit the process successfully
  } catch (error) {
    console.error('Error scheduling workflow:', error);
    process.exit(1); // Exit the process with an error code
  }
}

scheduleWorkflow({
  filePath: './test.mp3',
  targetLanguage: 'es',
});


### File: ./audio-transcript/src/workflows/index.ts ###
export * from './transcribeAndTranslate';


### File: ./audio-transcript/src/workflows/transcribeAndTranslate.ts ###
import { step } from '@restackio/ai/workflow';

import * as functions from '../functions';

type TranscribeAndTranslateInput = {
  filePath: string;
  targetLanguage: string;
};

export async function transcribeAndTranslateWorkflow({ filePath, targetLanguage }: TranscribeAndTranslateInput) {
  const transcription = await step<typeof functions>({}).transcribeAudio({
    filePath,
  });

  const translation = await step<typeof functions>({}).translateText({
    text: transcription,
    targetLanguage,
  });

  return { transcription, translation };
}


### File: ./audio-transcript/src/client.ts ###
import Restack from '@restackio/ai';

import 'dotenv/config';

export const connectionOptions = {
  engineId: process.env.RESTACK_ENGINE_ID!,
  address: process.env.RESTACK_ENGINE_ADDRESS!,
  apiKey: process.env.RESTACK_ENGINE_API_KEY!,
};

export const client = new Restack(
  process.env.RESTACK_ENGINE_API_KEY ? connectionOptions : undefined
);


### File: ./audio-transcript/src/functions/translateText.ts ###
import OpenAI from "openai";
import { FunctionFailure } from "@restackio/ai/function";

type TranslateTextInput = {
  text: string;
  targetLanguage: string;
};

export async function translateText({ text, targetLanguage }: TranslateTextInput) {
  if (!process.env.OPENAI_API_KEY) {
    throw FunctionFailure.nonRetryable("OPENAI_API_KEY is not set");
  }

  const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
  });

  try {
      const prompt = `Translate the following text to ${targetLanguage}:\n\n${text}`;
      const response = await openai.chat.completions.create({
          model: "gpt-4",
          messages: [{ role: "user", content: prompt }],
      });
      return response.choices[0].message.content || "Translation failed.";
  } catch (error) {
      throw FunctionFailure.nonRetryable(`Error translating text ${error}`);
  }
}


### File: ./audio-transcript/src/functions/transcribeAudio.ts ###
import fs from 'fs';
import OpenAI from 'openai';
import { FunctionFailure } from '@restackio/ai/function';

import 'dotenv/config';

type TranscribeAudioInput = {
  filePath: string;
};

export async function transcribeAudio({ filePath }: TranscribeAudioInput) {
  if (!process.env.OPENAI_API_KEY) {
    throw FunctionFailure.nonRetryable("OPENAI_API_KEY is not set");
  }

  const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
  });

  try {
    const audioFile = fs.createReadStream(filePath);
    const response = await openai.audio.transcriptions.create({
      model: "whisper-1",
      file: audioFile,
    });

    return response.text;
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error transcribing audio ${error}`);
  }
}


### File: ./audio-transcript/src/functions/index.ts ###
export * from './transcribeAudio';
export * from './translateText';


### File: ./audio-transcript/src/services.ts ###
import { transcribeAudio, translateText } from './functions';
import { client } from './client';

async function services() {
  const workflowsPath = require.resolve('./workflows');

  try {
    await Promise.all([
      // Start service with current workflows and functions
      client.startService({
        workflowsPath,
        functions: { transcribeAudio, translateText },
      }),
    ]);

    console.log('Services running successfully.');
  } catch (e) {
    console.error('Failed to run services', e);
  }
}

services().catch((err) => {
  console.error('Error running services:', err);
});


### File: ./openai/scheduleWorkflow.ts ###
import { client } from "./src/client";

export type InputSchedule = {
  name: string;
};

async function scheduleWorkflow(input: InputSchedule) {
  try {
    const workflowId = `${Date.now()}-helloWorkflow`;
    const runId = await client.scheduleWorkflow({
      workflowName: "helloWorkflow",
      workflowId,
      input,
    });

    const result = await client.getWorkflowResult({ workflowId, runId });

    console.log("Workflow result:", result);

    process.exit(0); // Exit the process successfully
  } catch (error) {
    console.error("Error scheduling workflow:", error);
    process.exit(1); // Exit the process with an error code
  }
}

scheduleWorkflow({
  name: "test",
});


### File: ./openai/src/workflows/index.ts ###
export * from "./hello";


### File: ./openai/src/workflows/hello.ts ###
import { log, step } from "@restackio/ai/workflow";
import * as functions from "../functions";
import { z } from "zod";
import zodToJsonSchema from "zod-to-json-schema";

interface Input {
  name: string;
}

export async function helloWorkflow({ name }: Input) {
  const userContent = `Greet this person: ${name}. In 4 words or less.`;

  const MessageSchema = z.object({
    message: z.string().describe("The greeting message."),
  });

  const jsonSchema = {
    name: "greet",
    schema: zodToJsonSchema(MessageSchema),
  };

  // Step 1 create greeting message with openai

  const openaiOutput = await step<typeof functions>({
    taskQueue: "openai",
  }).openaiChatCompletionsBase({
    userContent,
    jsonSchema,
  });

  const greetMessage = openaiOutput.result.choices[0].message.content ?? "";
  const greetCost = openaiOutput.cost;

  log.info("greeted", { greetMessage });

  // Step 2 create goodbye message with simple function

  const { message: goodbyeMessage } = await step<typeof functions>({}).goodbye({
    name,
  });

  log.info("goodbye", { goodbyeMessage });

  return {
    messages: [greetMessage, goodbyeMessage],
    cost: greetCost,
  };
}


### File: ./openai/src/client.ts ###
import Restack from "@restackio/ai";

import "dotenv/config";

export const connectionOptions = {
  engineId: process.env.RESTACK_ENGINE_ID!,
  address: process.env.RESTACK_ENGINE_ADDRESS!,
  apiKey: process.env.RESTACK_ENGINE_API_KEY!,
};

export const client = new Restack(
  process.env.RESTACK_ENGINE_API_KEY ? connectionOptions : undefined
);


### File: ./openai/src/functions/goodbye.ts ###
interface Input {
  name: string;
}

interface Output {
  message: string;
}

export async function goodbye(input: Input): Promise<Output> {
  return { message: `Goodbye, ${input.name}!` };
}


### File: ./openai/src/functions/index.ts ###
export * from "./goodbye";
export * from "./openai";

### File: ./openai/src/functions/openai/types/events.ts ###
import OpenAI from "openai/index";

export type StreamEvent = {
  chunkId?: string;
  response: string;
  assistantName?: string;
  isLast: boolean;
};

export type ToolCallEvent =
  OpenAI.Chat.Completions.ChatCompletionChunk.Choice.Delta.ToolCall & {
    function: {
      name: string;
      input: JSON;
    };
    assistantName?: string;
  };


### File: ./openai/src/functions/openai/types/index.ts ###
export * from "./events";


### File: ./openai/src/functions/openai/chat/completionsBase.ts ###
import { FunctionFailure, log } from "@restackio/ai/function";
import OpenAI from "openai/index";
import { ChatCompletionCreateParamsNonStreaming } from "openai/resources/chat/completions";
import { openaiClient } from "../utils/client";
import { openaiCost, Price } from "../utils/cost";
import { ChatCompletion, ChatModel } from "openai/resources/index";

export type UsageOutput = { tokens: number; cost: number };

export type OpenAIChatInput = {
  userContent: string;
  systemContent?: string;
  model?: ChatModel;
  jsonSchema?: {
    name: string;
    schema: Record<string, unknown>;
  };
  price?: Price;
  apiKey?: string;
  params?: ChatCompletionCreateParamsNonStreaming;
  tools?: OpenAI.Chat.Completions.ChatCompletionTool[];
  toolChoice?: OpenAI.Chat.Completions.ChatCompletionToolChoiceOption;
};

export const openaiChatCompletionsBase = async ({
  userContent,
  systemContent = "",
  model = "gpt-4o-mini",
  jsonSchema,
  price,
  apiKey,
  params,
  tools,
  toolChoice,
}: OpenAIChatInput): Promise<{ result: ChatCompletion; cost?: number }> => {
  try {
    const openai = openaiClient({ apiKey });

    const isO1Model = model.startsWith("o1-");

    const o1ModelParams = {
      temperature: 1,
      top_p: 1,
      frequency_penalty: 0,
      presence_penalty: 0,
    };

    const chatParams: ChatCompletionCreateParamsNonStreaming = {
      messages: [
        ...(systemContent ? [{ role: "system" as const, content: systemContent }] : []),
        { role: "user" as const, content: userContent },
        ...(params?.messages ?? []),
      ],
      ...(jsonSchema && {
        response_format: {
          type: "json_schema",
          json_schema: {
            name: jsonSchema.name,
            strict: true,
            schema: jsonSchema.schema,
          },
        },
      }),
      model,
      ...(tools && { tools }),
      ...(toolChoice && { tool_choice: toolChoice }),
      ...params,
      ...(isO1Model && o1ModelParams),
    };

    log.debug("OpenAI chat completion params", {
      chatParams,
    });

    const completion = await openai.chat.completions.create(chatParams);

    return {
      result: completion,
      cost:
        price &&
        openaiCost({
          price,
          tokensCount: {
            input: completion.usage?.prompt_tokens ?? 0,
            output: completion.usage?.completion_tokens ?? 0,
          },
        }),
    };
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error OpenAI chat: ${error}`);
  }
};

### File: ./openai/src/functions/openai/chat/completionsStream.ts ###
import OpenAI from "openai/index";
import { ChatCompletionChunk } from "openai/resources/chat/completions";

import Restack from "@restackio/ai";
import { currentWorkflow, log } from "@restackio/ai/function";

import { StreamEvent, ToolCallEvent } from "../types/events";

import { aggregateStreamChunks } from "../utils/aggregateStream";
import { mergeToolCalls } from "../utils/mergeToolCalls";
import { openaiClient } from "../utils/client";
import { openaiCost, Price } from "../utils/cost";
import { SendWorkflowEvent } from "@restackio/ai/event";
import { ChatModel } from "openai/resources/index";

export async function openaiChatCompletionsStream({
  model = "gpt-4o-mini",
  userName,
  newMessage,
  assistantName,
  messages = [],
  tools,
  toolEvent,
  streamAtCharacter,
  streamEvent,
  apiKey,
  price,
}: {
  model?: ChatModel;
  userName?: string;
  newMessage?: string;
  assistantName?: string;
  messages?: OpenAI.Chat.Completions.ChatCompletionMessageParam[];
  tools?: OpenAI.Chat.Completions.ChatCompletionTool[];
  toolEvent?: {
    workflowEventName: string;
    workflow?: SendWorkflowEvent["workflow"];
  };
  streamAtCharacter?: string;
  streamEvent?: {
    workflowEventName: string;
    workflow?: SendWorkflowEvent["workflow"];
  };
  apiKey?: string;
  price?: Price;
}) {
  const restack = new Restack();
  const workflow = currentWorkflow().workflowExecution;

  log.debug("workflow", { workflow });

  if (newMessage) {
    messages.push({
      role: "user",
      name: userName,
      content: newMessage,
    });
  }

  const openai = openaiClient({ apiKey });
  const chatStream = await openai.chat.completions.create({
    model: model,
    messages,
    tools,
    stream: true,
    stream_options: {
      include_usage: true,
    },
  });

  const [stream, streamEnd] = chatStream.tee();
  const readableStream = streamEnd.toReadableStream() as unknown as ReadableStream<any>;
  const aggregatedStream = await aggregateStreamChunks(readableStream);

  let finishReason: ChatCompletionChunk.Choice["finish_reason"];
  let response: ChatCompletionChunk.Choice.Delta["content"] = "";
  let tokensCountInput = 0;
  let tokensCountOutput = 0;

  for await (const chunk of stream) {
    let content = chunk.choices[0]?.delta?.content || "";
    finishReason = chunk.choices[0]?.finish_reason;
    tokensCountInput += chunk.usage?.prompt_tokens ?? 0;
    tokensCountOutput += chunk.usage?.completion_tokens ?? 0;

    if (finishReason === "tool_calls") {
      const { toolCalls } = mergeToolCalls(aggregatedStream);
      await Promise.all(
        toolCalls.map((toolCall) => {
          if (toolEvent) {
            const functionArguments = JSON.parse(
              toolCall.function?.arguments ?? ""
            );

            const input: ToolCallEvent = {
              ...toolCall,
              function: {
                name: toolCall.function?.name ?? "",
                input: functionArguments,
              },
              assistantName,
            };

            if (toolEvent) {
              const workflowEvent = {
                event: {
                  name: toolEvent.workflowEventName,
                  input,
                },
                workflow: {
                  ...workflow,
                  ...toolEvent.workflow,
                },
              };
              log.debug("toolEvent sendWorkflowEvent", { workflowEvent });

              restack.sendWorkflowEvent(workflowEvent);
            }
          }
        })
      );
      return {
        result: {
          messages,
          toolCalls,
        },
        cost:
          price &&
          openaiCost({
            price,
            tokensCount: {
              input: tokensCountInput,
              output: tokensCountOutput,
            },
          }),
      };
    } else {
      response += content;
      if (
        content.trim().slice(-1) === streamAtCharacter ||
        finishReason === "stop"
      ) {
        if (response.length) {
          const input: StreamEvent = {
            chunkId: chunk.id,
            response,
            assistantName,
            isLast: finishReason === "stop",
          };
          if (streamEvent) {
            const workflowEvent = {
              event: {
                name: streamEvent.workflowEventName,
                input,
              },
              workflow: {
                ...workflow,
                ...streamEvent.workflow,
              },
            };
            log.debug("streamEvent sendWorkflowEvent", { workflowEvent });
            restack.sendWorkflowEvent(workflowEvent);
          }
        }
      }

      if (finishReason === "stop") {
        const newMessage: OpenAI.Chat.Completions.ChatCompletionMessageParam = {
          content: response,
          role: "assistant",
          name: assistantName,
        };

        messages.push(newMessage);

        return {
          result: {
            messages,
          },
          cost:
            price &&
            openaiCost({
              price,
              tokensCount: {
                input: tokensCountInput,
                output: tokensCountOutput,
              },
            }),
        };
      }
    }
  }
}


### File: ./openai/src/functions/openai/chat/index.ts ###
export * from "./completionsBase";
export * from "./completionsStream";


### File: ./openai/src/functions/openai/utils/cost.ts ###
export type TokensCount = {
  input: number;
  output: number;
};

export type Price = {
  input: number;
  output: number;
};
export const openaiCost = ({
  tokensCount,
  price,
}: {
  tokensCount: TokensCount;
  price: Price;
}): number => {
  let cost = 0;
  const { input: inputTokens, output: outputTokens } = tokensCount;
  const { input: inputPrice, output: outputPrice } = price;

  cost = inputTokens * inputPrice + outputTokens * outputPrice;

  return cost;
};


### File: ./openai/src/functions/openai/utils/aggregateStream.ts ###
export async function aggregateStreamChunks(stream: ReadableStream) {
  const reader = stream.getReader();
  const chunks: Uint8Array[] = [];

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    if (value) chunks.push(value);
  }

  const aggregated = new Uint8Array(
    chunks.reduce((acc, chunk) => acc + chunk.length, 0)
  );
  let offset = 0;
  for (const chunk of chunks) {
    aggregated.set(chunk, offset);
    offset += chunk.length;
  }

  const textContent = new TextDecoder().decode(aggregated);
  const jsonObjects = textContent
    .split("\n")
    .filter((line) => line.trim())
    .map((line) => JSON.parse(line));
  return jsonObjects;
}


### File: ./openai/src/functions/openai/utils/client.ts ###
import OpenAI from "openai/index";
import "dotenv/config";

let openaiInstance: OpenAI | null = null;

export const openaiClient = ({
  apiKey = process.env.OPENAI_API_KEY,
}: {
  apiKey?: string;
}): OpenAI => {
  if (!apiKey) {
    throw new Error("API key is required to create OpenAI client.");
  }

  if (!openaiInstance) {
    openaiInstance = new OpenAI({
      apiKey,
    });
  }
  return openaiInstance;
};


### File: ./openai/src/functions/openai/utils/index.ts ###
export * from "./aggregateStream";
export * from "./client";
export * from "./cost";
export * from "./mergeToolCalls";


### File: ./openai/src/functions/openai/utils/mergeToolCalls.ts ###
import OpenAI from "openai/index";
import { ChatCompletionChunk } from "openai/resources/chat/completions.mjs";

export function mergeToolCalls(aggregatedStream: ChatCompletionChunk[]) {
  const toolCalls: OpenAI.Chat.Completions.ChatCompletionChunk.Choice.Delta.ToolCall[] =
    [];

  aggregatedStream.forEach((chunk) => {
    chunk.choices.forEach((choice) => {
      if (choice.delta.tool_calls) {
        choice.delta.tool_calls.forEach((toolCall) => {
          const lastToolCall = toolCalls[toolCalls.length - 1];
          if (toolCall.id) {
            toolCalls.push({
              ...toolCall,
              function: { ...toolCall.function, arguments: "" },
            });
          } else if (
            lastToolCall &&
            lastToolCall.function &&
            toolCall.function?.arguments
          ) {
            lastToolCall.function.arguments += toolCall.function.arguments;
          }
        });
      }
    });
  });

  return { toolCalls };
}


### File: ./openai/src/functions/openai/index.ts ###
export * from "./chat";
export * from "./thread";


### File: ./openai/src/functions/openai/thread/createMessageOnThread.ts ###
import OpenAI from "openai/index";
import { FunctionFailure } from "@restackio/ai/function";

import { openaiClient } from "../utils/client";

export async function createMessageOnThread({
  apiKey,
  threadId,
  content,
  role,
}: {
  apiKey: string;
  threadId: string;
  content: string;
  role: OpenAI.Beta.Threads.MessageCreateParams["role"];
}) {
  try {
    const openai = openaiClient({ apiKey });
    await openai.beta.threads.messages.create(threadId, {
      role,
      content,
    });
  } catch (error) {
    throw FunctionFailure.nonRetryable(
      `Error creating message thread: ${error}`
    );
  }
}


### File: ./openai/src/functions/openai/thread/runThread.ts ###
import { FunctionFailure } from "@restackio/ai/function";
import { Stream } from "openai/streaming";
import { AssistantStreamEvent } from "openai/resources/beta/index";
import { Run } from "openai/resources/beta/threads/runs/index";

import { openaiClient } from "../utils/client";

export async function runThread({
  apiKey,
  threadId,
  assistantId,
  stream = false,
}: {
  apiKey: string;
  threadId: string;
  assistantId: string;
  stream: boolean;
}): Promise<Stream<AssistantStreamEvent> | Run> {
  try {
    const openai = openaiClient({ apiKey });

    const run = await openai.beta.threads.runs.create(threadId, {
      assistant_id: assistantId,
      ...(stream && { stream }),
    });

    return run;
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error running thread: ${error}`);
  }
}

### File: ./openai/src/functions/openai/thread/createAssistant.ts ###
import { ChatModel } from "openai/resources/index";
import { FunctionFailure } from "@restackio/ai/function";
import { Assistant, AssistantTool } from "openai/resources/beta/index";

import { openaiClient } from "../utils/client";

export async function createAssistant({
  apiKey,
  name,
  instructions,
  model = "gpt-4o-mini",
  tools = [],
}: {
  apiKey: string;
  name: string;
  instructions: string;
  tools?: AssistantTool[];
  model: ChatModel;
}): Promise<Assistant> {
  try {
    const openai = openaiClient({ apiKey });

    const assistant = await openai.beta.assistants.create({
      name,
      instructions,
      model,
      tools,
    });

    return assistant;
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error creating assistant: ${error}`);
  }
}


### File: ./openai/src/functions/openai/thread/index.ts ###
export * from "./createAssistant";
export * from "./createMessageOnThread";
export * from "./createThread";
export * from "./runThread";


### File: ./openai/src/functions/openai/thread/createThread.ts ###
import { FunctionFailure } from "@restackio/ai/function";
import { Thread } from "openai/resources/beta/index";

import { openaiClient } from "../utils/client";

export async function createThread({
  apiKey,
}: {
  apiKey: string;
}): Promise<Thread> {
  try {
    const openai = openaiClient({ apiKey });
    const thread = await openai.beta.threads.create();

    return thread;
  } catch (error) {
    throw FunctionFailure.nonRetryable(`Error creating thread: ${error}`);
  }
}


### File: ./openai/src/services.ts ###
import {
  goodbye,
  openaiChatCompletionsBase,
  openaiChatCompletionsStream,
} from "./functions";
import { client } from "./client";

async function services() {
  const workflowsPath = require.resolve("./workflows");
  try {
    await Promise.all([
      // Start service with current workflows and functions
      client.startService({
        workflowsPath,
        functions: { goodbye },
      }),
      // Start the openai service
      client.startService({
        taskQueue: "openai",
        functions: { openaiChatCompletionsBase, openaiChatCompletionsStream },
      }),
    ]);

    console.log("Services running successfully.");
  } catch (e) {
    console.error("Failed to run services", e);
  }
}

services().catch((err) => {
  console.error("Error running services:", err);
});


